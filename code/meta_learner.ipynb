{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import importlib\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "\n",
    "# Pycox and PyTorch tuples for survival analysis\n",
    "import torchtuples as tt\n",
    "import pycox\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "from pycox.models import CoxPH, DeepHit\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "# Ray for hyperparameter tuning and distributed processing\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.air import session\n",
    "import ray.cloudpickle as pickle\n",
    "\n",
    "# Custom modules for data handling, balancing, training, evaluation, and model architectures\n",
    "import dataloader2\n",
    "import databalancer2\n",
    "import datatrainer2\n",
    "import modeleval\n",
    "import netweaver2\n",
    "\n",
    "# Reload custom modules to ensure latest changes are available\n",
    "importlib.reload(dataloader2)\n",
    "importlib.reload(databalancer2)\n",
    "importlib.reload(datatrainer2)\n",
    "importlib.reload(modeleval)\n",
    "importlib.reload(netweaver2)\n",
    "\n",
    "# Import specific functions from custom modules to keep code clean and readable\n",
    "from netweaver2 import (\n",
    "    lstm_net_init, DHANNWrapper, LSTMWrapper, generalized_ann_net_init\n",
    ")\n",
    "from dataloader2 import (\n",
    "    load_and_transform_data, preprocess_data #stack_sequences, dh_dataset_loader\n",
    ")\n",
    "from databalancer2 import (\n",
    "    define_medoid_general, df_event_focus, rebalance_data, underbalance_data_general, medoid_cluster, \n",
    "    dh_rebalance_data\n",
    ")\n",
    "from datatrainer2 import (\n",
    "    recursive_clustering, prepare_training_data, \n",
    "    prepare_validation_data, lstm_training\n",
    ")\n",
    "from modeleval import (\n",
    "    dh_test_model, nam_dagostino_chi2, get_baseline_hazard_at_timepoints, combined_test_model\n",
    ")\n",
    "\n",
    "import psutil\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define constants, load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:37:44,680 - INFO - Transforming training data...\n",
      "2024-11-15 23:37:57,708 - INFO - Transforming test data...\n"
     ]
    }
   ],
   "source": [
    "# Define Constants and Load Datasets\n",
    "RANDOM_SEED = 12345\n",
    "N_SPLIT = 2\n",
    "FEATURE_COLS = ['gender', 'dm', 'ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3']\n",
    "DURATION_COL = 'date_from_sub_60'\n",
    "EVENT_COL = 'endpoint'\n",
    "CLUSTER_COL = 'key'\n",
    "TIME_GRID = np.array([i * 365 for i in range(6)])\n",
    "\n",
    "# Define Feature Groups\n",
    "CAT_FEATURES = ['gender', 'dm', 'ht', 'sprint']\n",
    "LOG_FEATURES = ['a1c', 'po4', 'UACR_mg_g', 'Cr']\n",
    "STANDARD_FEATURES = ['age', 'alb', 'ca', 'hb', 'hco3']\n",
    "PASSTHROUGH_FEATURES = ['key', 'date_from_sub_60', 'endpoint']\n",
    "\n",
    "# Load and Transform Data\n",
    "BASE_FILENAME = '/mnt/d/pydatascience/g3_regress/data/X/X_20240628'\n",
    "X_train_transformed, X_test_transformed = load_and_transform_data(\n",
    "    BASE_FILENAME, CAT_FEATURES, LOG_FEATURES, STANDARD_FEATURES, PASSTHROUGH_FEATURES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and save models\n",
    "- model naming: {deepsurv/deephit}\\_{nn}\\_{resample method}_{outcome}\n",
    "- for deepsurv model, only the result in time_grid will be retrieved so the result of deepsurv and deephit models are compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(config, num_risk = len(X_train_transformed[EVENT_COL].unique()) - 1, num_time_bins=len(TIME_GRID)):\n",
    "    \"\"\"\n",
    "    Function to create a neural network based on the given configuration.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing model type, network type, and hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Created neural network model.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if config['model'] == 'deepsurv':\n",
    "        num_risk = None\n",
    "        num_time_bins=None\n",
    "    elif config['model'] == 'deephit':\n",
    "        num_risk = num_risk\n",
    "        num_time_bins = num_time_bins\n",
    "    # Create the Neural Network\n",
    "    if config['net'] == 'ann':\n",
    "        net = generalized_ann_net_init(\n",
    "            input_size=len(config['features']),\n",
    "            num_nodes=config[\"num_nodes\"],\n",
    "            batch_norm=config[\"batch_norm\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            output_size=1, # Default output size for DeepSurv\n",
    "            num_risks = num_risk,\n",
    "            num_time_bins = num_time_bins\n",
    "        )\n",
    "    elif config['net'] == 'lstm':\n",
    "        net = lstm_net_init(\n",
    "            input_size=len(config['features']),\n",
    "            num_nodes=config[\"num_nodes\"],\n",
    "            batch_norm=config[\"batch_norm\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            num_risks = num_risk,\n",
    "            num_time_bins = num_time_bins\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown network type: {}\".format(config['net']))\n",
    "\n",
    "    optimizer = tt.optim.AdamWR(decoupled_weight_decay=1e-6, cycle_eta_multiplier=0.8)\n",
    "    if config['model'] == 'deepsurv':\n",
    "        model = CoxPH(net, optimizer)\n",
    "    elif config['model'] == 'deephit':\n",
    "        model = DeepHit(net, optimizer)\n",
    "    model.optimizer.set_lr(config[\"lr\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_neural_network(model, config, X_train, X_val, duration_col, event_col, cluster_col, callbacks, time_grid=None):\n",
    "    \"\"\"\n",
    "    Function to train a given neural network using the provided datasets.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): Neural network to be trained.\n",
    "        config (dict): Configuration dictionary containing model hyperparameters.\n",
    "        X_train (pd.DataFrame): Training dataset with features.\n",
    "        X_val (pd.DataFrame): Validation dataset with features.\n",
    "        duration_col (str): Column representing event durations.\n",
    "        event_col (str): Column representing event occurrences.\n",
    "        cluster_col (str): Column for grouping during cross-validation.\n",
    "        callbacks (list): List of callbacks for training.\n",
    "        time_grid (np.array, optional): Time grid for evaluation if required. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained PyCox model.\n",
    "        logs: Training logs.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Train the model\n",
    "    if config['model'] == 'deepsurv':\n",
    "        print('Initiate training of deepsurv neural network')\n",
    "        X_val = df_event_focus(X_val, event_col, config['endpoint'])\n",
    "        X_val_processed, y_val = preprocess_data(X_val, config['features'], duration_col, event_col)\n",
    "        val_data = (X_val_processed, y_val)\n",
    "        if config['net'] == 'ann':\n",
    "            print('model structure: ANN')\n",
    "            if config['balance_method'] == 'clustering':\n",
    "                print('data balancing method: clustering')\n",
    "                model, logs = recursive_clustering(model, X_train, duration_col, event_col, config, val_data, callbacks, max_repeats=30)\n",
    "            elif config['balance_method'] == 'enn':\n",
    "                print('data balancing method: smoteenn')\n",
    "                X_train = rebalance_data(X_train, event_col, config['endpoint'], CAT_FEATURES, config, RANDOM_SEED, method='ENN')\n",
    "                X_train, y_train = preprocess_data(X_train, config['features'], duration_col, event_col)\n",
    "                logs = model.fit(X_train, y_train, config['batch_size'], int(config['max_epochs']), callbacks, verbose=True, val_data=val_data, num_workers=10)\n",
    "            elif config['balance_method'] == 'tomek':\n",
    "                print('data balancing method: smotetomek')\n",
    "                X_train = rebalance_data(X_train, event_col, config['endpoint'], CAT_FEATURES, config, RANDOM_SEED, method='Tomek')\n",
    "                X_train, y_train = preprocess_data(X_train, config['features'], duration_col, event_col)\n",
    "                logs = model.fit(X_train, y_train, config['batch_size'], int(config['max_epochs']), callbacks, verbose=True, val_data=val_data, num_workers=10)\n",
    "        elif config['net'] == 'lstm':\n",
    "            print('model structure: LSTM')\n",
    "            if config['balance_method'] == 'clustering':\n",
    "                print('data balancing method: clustering')\n",
    "                model, logs = lstm_training(model, X_train, X_val, duration_col, event_col, cluster_col, config, callbacks, time_grid)\n",
    "            elif config['balance_method'] == 'NearMiss':\n",
    "                print('data balancing method: NearMiss')\n",
    "                model, logs = lstm_training(model, X_train, X_val, duration_col, event_col, cluster_col, config, callbacks, time_grid)\n",
    "    elif config['model'] == 'deephit':\n",
    "        print('Initiate training of deephit neural network')\n",
    "        X_val_processed, y_val = preprocess_data(X_val, config['features'], duration_col, event_col, TIME_GRID, discretize=True)\n",
    "        val_data = (X_val_processed, y_val)\n",
    "        if config['net'] == 'ann':\n",
    "            print('model structure: ANN')\n",
    "            if config['balance_method'] == 'clustering':\n",
    "                print('data balancing method: clustering')\n",
    "                model, logs = recursive_clustering(model, X_train, duration_col, event_col, config, val_data, callbacks, max_repeats=30, time_grid=TIME_GRID)\n",
    "            elif config['balance_method'] == 'NearMiss':\n",
    "                print('data balancing method: NearMiss')\n",
    "                X_train = underbalance_data_general(X_train, EVENT_COL, CLUSTER_COL, config, version=config['version'])\n",
    "                X_train, y_train = preprocess_data(X_train, config['features'], duration_col, event_col, TIME_GRID, discretize=True)\n",
    "                logs = model.fit(X_train, y_train, config['batch_size'], int(config['max_epochs']), callbacks, verbose=True, val_data=val_data)\n",
    "        elif config['net'] == 'lstm':\n",
    "            print('model structure: LSTM')\n",
    "            if config['balance_method'] == 'clustering':\n",
    "                print('data balancing method: clustering')\n",
    "                model, logs = lstm_training(model, X_train, X_val, duration_col, event_col, cluster_col, config, callbacks, time_grid)\n",
    "            elif config['balance_method'] == 'NearMiss':\n",
    "                print('data balancing method: NearMiss')\n",
    "                model, logs = lstm_training(model, X_train, X_val, duration_col, event_col, cluster_col, config, callbacks, time_grid)        \n",
    "\n",
    "    # Free memory after training\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, logs\n",
    "\n",
    "def save_model(params, model, model_path, baseline_hazard_path):\n",
    "    \"\"\"\n",
    "    Save model weights and baseline hazard data.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model to save.\n",
    "    - model_path: Path to save the model weights (.pt file).\n",
    "    - baseline_hazard_path: Path to save the baseline hazards (.pkl file).\n",
    "    \"\"\"\n",
    "    # Compute baseline hazards and save\n",
    "    if params['model'] == 'deepsurv':\n",
    "        baseline_hazard = model.compute_baseline_hazards()\n",
    "        baseline_hazard.to_pickle(baseline_hazard_path)\n",
    "    \n",
    "    # Save model weights\n",
    "    model.save_model_weights(model_path)\n",
    "    print(f\"Model and baseline hazards saved to {model_path} and {baseline_hazard_path}.\")\n",
    "\n",
    "def training_wrapper(df, config, spliter, model_path, hazard_path, feature_col=FEATURE_COLS, duration_col=DURATION_COL, event_col=EVENT_COL, cluster_col=CLUSTER_COL, time_grid=TIME_GRID):\n",
    "    \"\"\"\n",
    "    Train and save a survival analysis model with grouped cross-validation splits.\n",
    "\n",
    "    This function performs training on grouped cross-validation splits of the input DataFrame and saves each trained model\n",
    "    along with its baseline hazards. Memory management is handled to ensure efficient GPU usage.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing training data.\n",
    "    - config (dict): Configuration dictionary for initializing the neural network.\n",
    "    - spliter (object): Splitter object (e.g., GroupShuffleSplit or StratifiedKFold) used for creating train-validation splits.\n",
    "    - model_path (str): File path to save the trained model weights (.pt file).\n",
    "    - hazard_path (str): File path to save the baseline hazards (.pkl file).\n",
    "    - feature_col (list): List of feature column names in `df` used for model training.\n",
    "    - duration_col (str): Name of the column representing duration/time-to-event.\n",
    "    - event_col (str): Name of the column representing the event indicator (0 = censored, 1 = event).\n",
    "    - cluster_col (str): Name of the column used for grouping (clusters for cross-validation).\n",
    "    - time_grid (list): List or array defining the time grid for training.\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves the model weights and baseline hazard data for each cross-validation split.\n",
    "    \"\"\"\n",
    "    for train_idx, val_idx in spliter.split(X=df[feature_col], y=df[event_col], groups=df[cluster_col]):\n",
    "        # Clear GPU memory for each split\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Define early stopping callback\n",
    "        callbacks = [tt.cb.EarlyStopping()]\n",
    "        \n",
    "        # Create training and validation sets\n",
    "        train_df = df.iloc[train_idx]\n",
    "        val_df = df.iloc[val_idx]\n",
    "        \n",
    "        # Initialize and train the model\n",
    "        model = create_neural_network(config)\n",
    "        model, logs = train_neural_network(\n",
    "            model, config,\n",
    "            X_train=train_df, X_val=val_df,\n",
    "            duration_col=duration_col, event_col=event_col,\n",
    "            cluster_col=cluster_col, callbacks=callbacks, time_grid=time_grid\n",
    "        )\n",
    "        \n",
    "        # Save the trained model and its baseline hazards\n",
    "        save_model(config, model, model_path, hazard_path)\n",
    "        \n",
    "        # Free memory for the next iteration\n",
    "        del model, logs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Training and saving completed for all cross-validation splits.\")\n",
    "\n",
    "    print(\"All models have been trained and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 deepsurv_ann_clustering_1\n",
    "- features: ['gender', 'dm', 'ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3']\n",
    "- sampling strategy: 0.05\n",
    "- 2 hidden layers with 8 and 4 nodes\n",
    "- no batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.1144793446270997\n",
    "- learning rate: 0.1\n",
    "- max epochs: 9\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "deepsurv_ann_clustering_1_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'ann',\n",
    "    'balance_method': 'clustering',\n",
    "    'features': ['gender', 'dm', 'ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3'],\n",
    "    'endpoint': 1,\n",
    "    'num_nodes': [8, 4],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0.1144793446270997,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 9,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 deepsurv_ann_smoteenn_1\n",
    "- features: ['ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'hb', 'hco3']\n",
    "- sampling strategy: 0.3\n",
    "- 4 hidden layers with 64, 32, 16 and 8 nodes\n",
    "- batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.09555033386059111\n",
    "- learning rate: 0.1\n",
    "- max epochs: 16\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsurv_ann_smoteenn_1_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'ann',\n",
    "    'balance_method': 'enn',\n",
    "    'features': ['ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'hb', 'hco3'],\n",
    "    'endpoint': 1,\n",
    "    'num_nodes': [64, 32, 16, 8],\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.09555033386059111,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 16,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.3,\n",
    "    'seq_length': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 deepsurv_ann_smotetomek_1\n",
    "- features:  ['gender', 'dm', 'ht', 'sprint', 'po4', 'UACR_mg_g', 'Cr', 'age', 'hb', 'hco3']\n",
    "- sampling strategy: 0.2\n",
    "- 3 hidden layers with 32, 16 and 8 nodes\n",
    "- batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.23872991564684112\n",
    "- learning rate: 0.1\n",
    "- max epochs: 14\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsurv_ann_smotetomek_1_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'ann',\n",
    "    'balance_method': 'tomek',\n",
    "    'features': ['gender', 'dm', 'ht', 'sprint', 'po4', 'UACR_mg_g', 'Cr', 'age', 'hb', 'hco3'],\n",
    "    'endpoint': 1,\n",
    "    'num_nodes': [32, 16, 8],\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.23872991564684112,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 14,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.2,\n",
    "    'seq_length': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 deepsurv_ann_clustering_2\n",
    "- features: [\"gender\", \"a1c\", \"po4\", \"UACR_mg_g\", \"Cr\"]\n",
    "- sampling_strategy: 0.05\n",
    "- 3 hidden layers with 32, 16, 8 nodes\n",
    "- no batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.3058921011568742\n",
    "- learning rate: 0.1\n",
    "- max epochs: 14\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsurv_ann_clustering_2_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'ann',\n",
    "    'balance_method': 'clustering',\n",
    "    'features': [\"gender\", \"a1c\", \"po4\", \"UACR_mg_g\", \"Cr\"],\n",
    "    'endpoint': 2,\n",
    "    'num_nodes': [32, 16, 8],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0.3058921011568742,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 14,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 deepsurv_ann_smoteenn_2\n",
    "- features: [\"gender\", \"dm\", \"ht\", \"sprint\", \"a1c\", \"po4\", \"UACR_mg_g\", \"Cr\", \"age\", \"alb\", \"ca\", \"hb\", \"hco3\"]\n",
    "- sampling_strategy: 0.1, \n",
    "- 2 hidden layers with 8, 4 nodes\n",
    "- no batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.38878203553667456\n",
    "- learning rate: 0.01\n",
    "- max epochs: 10\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsurv_ann_smoteenn_2_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'ann',\n",
    "    'balance_method': 'enn',\n",
    "    'features': [\"gender\", \"dm\", \"ht\", \"sprint\", \"a1c\", \"po4\", \"UACR_mg_g\", \"Cr\", \"age\", \"alb\", \"ca\", \"hb\", \"hco3\"],\n",
    "    'endpoint': 2,\n",
    "    'num_nodes': [8, 4],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0.38878203553667456,\n",
    "    'lr': 0.01,\n",
    "    'max_epochs': 10,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.1,\n",
    "    'seq_length': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 deepsurv_ann_smotetomek_2\n",
    "- features: ['ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3']\n",
    "- sampling_strategy: 0.05\n",
    "- 2 hidden layers with 64, 32 nodes\n",
    "- batch normalization in each hidden layer \n",
    "- dropout ratio in each layer: 0.3162398297390827\n",
    "- learning rate: 0.1\n",
    "- max epochs: 11\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsurv_ann_smotetomek_2_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'ann',\n",
    "    'balance_method': 'tomek',\n",
    "    'features': ['ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3'],\n",
    "    'endpoint': 2,\n",
    "    'num_nodes': [64, 32],\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.3162398297390827,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 11,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 deepsurv_lstm_clustering_1\n",
    "- features: ['gender', 'a1c', 'po4', 'UACR_mg_g', 'Cr']\n",
    "- sampling_strategy: 0.05\n",
    "- 3 hidden layers with 8, 4, 2 nodes\n",
    "- sequence length 7\n",
    "- no batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.2772567071863989\n",
    "- learning rate: 0.1\n",
    "- max epochs: 13\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsurv_lstm_clustering_1_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'lstm',\n",
    "    'balance_method': 'clustering',\n",
    "    'features': ['gender', 'a1c', 'po4', 'UACR_mg_g', 'Cr'],\n",
    "    'endpoint': 1,\n",
    "    'num_nodes': [8, 4, 2],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0.2772567071863989,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 13,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 deepsurv_lstm_nearmiss_1\n",
    "- features: ['gender', 'a1c', 'po4', 'UACR_mg_g', 'Cr']\n",
    "- sampling_strategy: 0.05\n",
    "- 3 hidden layers with 8, 4, 2 nodes\n",
    "- seq_length': 8\n",
    "- no batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.3397308077824205\n",
    "- learning rate: 0.001\n",
    "- max epochs: 9\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsurv_lstm_nearmiss_1_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'lstm',\n",
    "    'balance_method': 'NearMiss',\n",
    "    'features': ['gender', 'a1c', 'po4', 'UACR_mg_g', 'Cr'],\n",
    "    'endpoint': 1,\n",
    "    'num_nodes': [8, 4, 2],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0.3397308077824205,\n",
    "    'lr': 0.001,\n",
    "    'max_epochs': 9,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 deepsurv_lstm_clustering_2\n",
    "- features: ['gender', 'a1c', 'po4', 'UACR_mg_g', 'Cr']\n",
    "- sampling_strategy: 0.05\n",
    "- 3 hidden layers with 8, 4, 2 nodes\n",
    "- seq_length': 8\n",
    "- no batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.3397308077824205\n",
    "- learning rate: 0.001\n",
    "- max epochs: 9\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsurv_lstm_clustering_2_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'lstm',\n",
    "    'balance_method': 'clustering',\n",
    "    'features': ['gender', 'a1c', 'po4', 'UACR_mg_g', 'Cr'],\n",
    "    'endpoint': 2,\n",
    "    'num_nodes': [8, 4, 2],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0.3397308077824205,\n",
    "    'lr': 0.001,\n",
    "    'max_epochs': 9,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10 deepsurv_lstm_nearmiss_2\n",
    "- features: ['sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3']\n",
    "- sampling_strategy: 0.05\n",
    "- 2 hidden layers with 32, 16 nodes\n",
    "- seq_length': 2\n",
    "- no batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.35763396978044143\n",
    "- learning rate: 0.1\n",
    "- max epochs: 10\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsurv_lstm_nearmiss_2_config = {\n",
    "    'model': 'deepsurv',\n",
    "    'net': 'lstm',\n",
    "    'balance_method': 'NearMiss',\n",
    "    'features': ['sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3'],\n",
    "    'endpoint': 2,\n",
    "    'num_nodes': [32, 16],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0.35763396978044143,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 10,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.11 deephit_ann_clustering_all\n",
    "- features: ['gender', 'dm', 'ht', 'sprint', 'po4', 'UACR_mg_g', 'Cr', 'age', 'hb', 'hco3']\n",
    "- sampling strategy: 0.05\n",
    "- 2 hidden layers with 64 and 32 nodes\n",
    "- batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.26400151710698067\n",
    "- learning rate: 0.1\n",
    "- max epochs: 8\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deephit_ann_clustering_all_config = {\n",
    "    'model': 'deephit',\n",
    "    'net': 'ann',\n",
    "    'balance_method': 'clustering',\n",
    "    'features': ['gender', 'dm', 'ht', 'sprint', 'po4', 'UACR_mg_g', 'Cr', 'age', 'hb', 'hco3'],\n",
    "    'endpoint': 'all',\n",
    "    'num_nodes': [64, 32],\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.26400151710698067,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 8,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.12 deephit_ann_nearmiss2_all\n",
    "- features: ['sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3']\n",
    "- sampling strategy: 0.05\n",
    "- 2 hidden layers with 8, 4 and 2 nodes\n",
    "- batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.7346754269827496\n",
    "- learning rate: 0.01\n",
    "- max epochs: 7\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "deephit_ann_nearmiss2_all_config = {\n",
    "    'model': 'deephit',\n",
    "    'net': 'ann',\n",
    "    'balance_method': 'NearMiss',\n",
    "    'version': 2,\n",
    "    'features': ['sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3'],\n",
    "    'endpoint': 'all',\n",
    "    'num_nodes': [8, 4, 2],\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.7346754269827496,\n",
    "    'lr': 0.01,\n",
    "    'max_epochs': 7,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.13 deephit_lstm_clustering_all\n",
    "- features: ['ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3']\n",
    "- sampling strategy: 0.05\n",
    "- seq_length: 6\n",
    "- 3 hidden layers with 64, 32 and 16 nodes\n",
    "- batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.46132889488306583\n",
    "- learning rate: 0.1\n",
    "- max epochs: 5\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "deephit_lstm_clustering_all_config = {\n",
    "    'model': 'deephit',\n",
    "    'net': 'lstm',\n",
    "    'balance_method': 'clustering',\n",
    "    'version': 2,\n",
    "    'features': ['ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3'],\n",
    "    'endpoint': 'all',\n",
    "    'num_nodes': [64, 32, 16],\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.46132889488306583,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 5,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.14 deephit_lstm_nearmiss1_all\n",
    "- features: ['gender', 'a1c', 'po4', 'UACR_mg_g', 'Cr']\n",
    "- sampling strategy: 0.05\n",
    "- seq_length: 9\n",
    "- 3 hidden layers with 32, 16 and 8 nodes\n",
    "- batch normalization in each hidden layer\n",
    "- dropout ratio in each layer: 0.18001924589390816\n",
    "- learning rate: 0.1\n",
    "- max epochs: 9\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deephit_lstm_nearmiss1_all_config = {\n",
    "    'model': 'deephit',\n",
    "    'net': 'lstm',\n",
    "    'balance_method': 'NearMiss',\n",
    "    'version': 1,\n",
    "    'features': ['gender', 'a1c', 'po4', 'UACR_mg_g', 'Cr'],\n",
    "    'endpoint': 'all',\n",
    "    'num_nodes': [32, 16, 8],\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.18001924589390816,\n",
    "    'lr': 0.1,\n",
    "    'max_epochs': 9,\n",
    "    'batch_size': 512,\n",
    "    'sampling_strategy': 0.05,\n",
    "    'seq_length': 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ls = ['deepsurv_ann_clustering_1', 'deepsurv_ann_smoteenn_1', 'deepsurv_ann_smotetomek_1',\n",
    "            'deepsurv_ann_clustering_2', 'deepsurv_ann_smoteenn_2', 'deepsurv_ann_smotetomek_2',\n",
    "            'deepsurv_lstm_clustering_1', 'deepsurv_lstm_nearmiss_1', 'deepsurv_lstm_clustering_2', 'deepsurv_lstm_nearmiss_2',\n",
    "            'deephit_ann_clustering_all', 'deephit_ann_nearmiss2_all', 'deephit_lstm_clustering_all', 'deephit_lstm_nearmiss1_all']\n",
    "model_path = '/mnt/d/PYDataScience/g3_regress/code/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:37:59,243 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:37:59,249 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:37:59,260 - INFO - Performing clustering iteration 1 / 20\n",
      "2024-11-15 23:37:59,260 - INFO - init\n",
      "2024-11-15 23:37:59,263 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:37:59,267 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: ANN\n",
      "data balancing method: clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:37:59,934 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/callbacks.py:607: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
      "  p.data = p.data.add(-weight_decay * eta, p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 5.1222,\tval_loss: 7.8605\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 5.0338,\tval_loss: 7.5699\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 4.9697,\tval_loss: 7.4738\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 4.9293,\tval_loss: 7.3146\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 4.8518,\tval_loss: 6.9994\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 4.7889,\tval_loss: 6.8219\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 4.8050,\tval_loss: 6.8218\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 4.7501,\tval_loss: 6.7547\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 4.7655,\tval_loss: 6.7621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:00,813 - INFO - Performing clustering iteration 2 / 20\n",
      "2024-11-15 23:38:00,813 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:00,818 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:38:01,253 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:\t[0s / 0s],\t\ttrain_loss: 4.6969,\tval_loss: 6.7727\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 4.6655,\tval_loss: 6.6474\n",
      "11:\t[0s / 0s],\t\ttrain_loss: 4.6524,\tval_loss: 6.8231\n",
      "12:\t[0s / 0s],\t\ttrain_loss: 4.6561,\tval_loss: 6.7562\n",
      "13:\t[0s / 0s],\t\ttrain_loss: 4.6518,\tval_loss: 6.7023\n",
      "14:\t[0s / 0s],\t\ttrain_loss: 4.6509,\tval_loss: 6.5587\n",
      "15:\t[0s / 0s],\t\ttrain_loss: 4.6507,\tval_loss: 6.6676\n",
      "16:\t[0s / 0s],\t\ttrain_loss: 4.6268,\tval_loss: 6.6052\n",
      "17:\t[0s / 0s],\t\ttrain_loss: 4.6377,\tval_loss: 6.5342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:01,790 - INFO - Performing clustering iteration 3 / 20\n",
      "2024-11-15 23:38:01,791 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:01,794 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:38:02,231 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:\t[0s / 0s],\t\ttrain_loss: 4.6094,\tval_loss: 6.6564\n",
      "19:\t[0s / 0s],\t\ttrain_loss: 4.6289,\tval_loss: 6.5130\n",
      "20:\t[0s / 0s],\t\ttrain_loss: 4.5973,\tval_loss: 6.5200\n",
      "21:\t[0s / 0s],\t\ttrain_loss: 4.6182,\tval_loss: 6.4772\n",
      "22:\t[0s / 0s],\t\ttrain_loss: 4.6057,\tval_loss: 6.4865\n",
      "23:\t[0s / 0s],\t\ttrain_loss: 4.6040,\tval_loss: 6.3840\n",
      "24:\t[0s / 0s],\t\ttrain_loss: 4.5978,\tval_loss: 6.3814\n",
      "25:\t[0s / 0s],\t\ttrain_loss: 4.6132,\tval_loss: 6.4298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:02,824 - INFO - Performing clustering iteration 4 / 20\n",
      "2024-11-15 23:38:02,824 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:02,827 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26:\t[0s / 0s],\t\ttrain_loss: 4.6217,\tval_loss: 6.4693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:03,230 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27:\t[0s / 0s],\t\ttrain_loss: 4.6167,\tval_loss: 6.3916\n",
      "28:\t[0s / 0s],\t\ttrain_loss: 4.6326,\tval_loss: 6.3972\n",
      "29:\t[0s / 0s],\t\ttrain_loss: 4.6202,\tval_loss: 6.3943\n",
      "30:\t[0s / 0s],\t\ttrain_loss: 4.6303,\tval_loss: 6.3755\n",
      "31:\t[0s / 0s],\t\ttrain_loss: 4.6168,\tval_loss: 6.4505\n",
      "32:\t[0s / 0s],\t\ttrain_loss: 4.6078,\tval_loss: 6.4495\n",
      "33:\t[0s / 0s],\t\ttrain_loss: 4.6297,\tval_loss: 6.3474\n",
      "34:\t[0s / 0s],\t\ttrain_loss: 4.6127,\tval_loss: 6.3939\n",
      "35:\t[0s / 0s],\t\ttrain_loss: 4.5996,\tval_loss: 6.3698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:03,761 - INFO - Performing clustering iteration 5 / 20\n",
      "2024-11-15 23:38:03,762 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:03,764 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:38:04,170 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36:\t[0s / 0s],\t\ttrain_loss: 4.6199,\tval_loss: 6.3322\n",
      "37:\t[0s / 0s],\t\ttrain_loss: 4.6204,\tval_loss: 6.4536\n",
      "38:\t[0s / 0s],\t\ttrain_loss: 4.5920,\tval_loss: 6.3989\n",
      "39:\t[0s / 0s],\t\ttrain_loss: 4.6021,\tval_loss: 6.2783\n",
      "40:\t[0s / 0s],\t\ttrain_loss: 4.5824,\tval_loss: 6.3993\n",
      "41:\t[0s / 0s],\t\ttrain_loss: 4.5764,\tval_loss: 6.3986\n",
      "42:\t[0s / 0s],\t\ttrain_loss: 4.5946,\tval_loss: 6.3371\n",
      "43:\t[0s / 0s],\t\ttrain_loss: 4.6034,\tval_loss: 6.4016\n",
      "44:\t[0s / 0s],\t\ttrain_loss: 4.6128,\tval_loss: 6.4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:04,682 - INFO - Performing clustering iteration 6 / 20\n",
      "2024-11-15 23:38:04,682 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:04,686 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:38:05,091 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45:\t[0s / 0s],\t\ttrain_loss: 4.6003,\tval_loss: 6.4377\n",
      "46:\t[0s / 0s],\t\ttrain_loss: 4.5957,\tval_loss: 6.4163\n",
      "47:\t[0s / 0s],\t\ttrain_loss: 4.6103,\tval_loss: 6.3416\n",
      "48:\t[0s / 0s],\t\ttrain_loss: 4.6085,\tval_loss: 6.3274\n",
      "49:\t[0s / 0s],\t\ttrain_loss: 4.6087,\tval_loss: 6.3596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:05,402 - INFO - Performing clustering iteration 7 / 20\n",
      "2024-11-15 23:38:05,403 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:05,407 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:38:05,792 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:05,943 - INFO - Performing clustering iteration 8 / 20\n",
      "2024-11-15 23:38:05,943 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:05,947 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50:\t[0s / 0s],\t\ttrain_loss: 4.6048,\tval_loss: 6.4077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:06,331 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:06,490 - INFO - Performing clustering iteration 9 / 20\n",
      "2024-11-15 23:38:06,491 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:06,494 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51:\t[0s / 0s],\t\ttrain_loss: 4.6086,\tval_loss: 6.4236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:06,871 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:07,018 - INFO - Performing clustering iteration 10 / 20\n",
      "2024-11-15 23:38:07,019 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:07,021 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52:\t[0s / 0s],\t\ttrain_loss: 4.6170,\tval_loss: 6.3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:07,418 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:07,568 - INFO - Performing clustering iteration 11 / 20\n",
      "2024-11-15 23:38:07,569 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:07,572 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53:\t[0s / 0s],\t\ttrain_loss: 4.5973,\tval_loss: 6.3334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:07,959 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:08,123 - INFO - Performing clustering iteration 12 / 20\n",
      "2024-11-15 23:38:08,124 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:08,127 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54:\t[0s / 0s],\t\ttrain_loss: 4.6139,\tval_loss: 6.2936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:08,506 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:08,667 - INFO - Performing clustering iteration 13 / 20\n",
      "2024-11-15 23:38:08,668 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:08,671 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55:\t[0s / 0s],\t\ttrain_loss: 4.6199,\tval_loss: 6.3020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:09,041 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:09,190 - INFO - Performing clustering iteration 14 / 20\n",
      "2024-11-15 23:38:09,190 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:09,193 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56:\t[0s / 0s],\t\ttrain_loss: 4.6315,\tval_loss: 6.2808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:09,570 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57:\t[3s / 3s],\t\ttrain_loss: 4.6205,\tval_loss: 6.2778\n",
      "58:\t[0s / 3s],\t\ttrain_loss: 4.6115,\tval_loss: 6.2782\n",
      "59:\t[0s / 3s],\t\ttrain_loss: 4.6213,\tval_loss: 6.2788\n",
      "60:\t[0s / 3s],\t\ttrain_loss: 4.6268,\tval_loss: 6.5559\n",
      "61:\t[0s / 3s],\t\ttrain_loss: 4.6183,\tval_loss: 6.2105\n",
      "62:\t[0s / 3s],\t\ttrain_loss: 4.6344,\tval_loss: 6.2469\n",
      "63:\t[0s / 3s],\t\ttrain_loss: 4.6127,\tval_loss: 6.4725\n",
      "64:\t[0s / 3s],\t\ttrain_loss: 4.6021,\tval_loss: 6.2031\n",
      "65:\t[0s / 3s],\t\ttrain_loss: 4.6055,\tval_loss: 6.1676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:13,678 - INFO - Performing clustering iteration 15 / 20\n",
      "2024-11-15 23:38:13,679 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:13,683 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:38:14,087 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66:\t[0s / 0s],\t\ttrain_loss: 4.6355,\tval_loss: 6.2681\n",
      "67:\t[0s / 0s],\t\ttrain_loss: 4.6207,\tval_loss: 6.3029\n",
      "68:\t[0s / 0s],\t\ttrain_loss: 4.5872,\tval_loss: 6.1525\n",
      "69:\t[0s / 0s],\t\ttrain_loss: 4.6243,\tval_loss: 6.1809\n",
      "70:\t[0s / 0s],\t\ttrain_loss: 4.6310,\tval_loss: 6.1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71:\t[0s / 0s],\t\ttrain_loss: 4.5923,\tval_loss: 6.2492\n",
      "72:\t[0s / 0s],\t\ttrain_loss: 4.5884,\tval_loss: 6.1203\n",
      "73:\t[0s / 0s],\t\ttrain_loss: 4.6063,\tval_loss: 6.1447\n",
      "74:\t[0s / 0s],\t\ttrain_loss: 4.6177,\tval_loss: 6.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:14,660 - INFO - Performing clustering iteration 16 / 20\n",
      "2024-11-15 23:38:14,661 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:14,664 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:38:15,081 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75:\t[0s / 0s],\t\ttrain_loss: 4.6029,\tval_loss: 6.0245\n",
      "76:\t[0s / 0s],\t\ttrain_loss: 4.5860,\tval_loss: 6.1040\n",
      "77:\t[0s / 0s],\t\ttrain_loss: 4.6219,\tval_loss: 6.2977\n",
      "78:\t[0s / 0s],\t\ttrain_loss: 4.6078,\tval_loss: 6.1980\n",
      "79:\t[0s / 0s],\t\ttrain_loss: 4.6001,\tval_loss: 6.2019\n",
      "80:\t[0s / 0s],\t\ttrain_loss: 4.6007,\tval_loss: 6.1669\n",
      "81:\t[0s / 0s],\t\ttrain_loss: 4.6201,\tval_loss: 6.0048\n",
      "82:\t[0s / 0s],\t\ttrain_loss: 4.6083,\tval_loss: 6.0783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:15,724 - INFO - Performing clustering iteration 17 / 20\n",
      "2024-11-15 23:38:15,724 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:15,727 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83:\t[0s / 0s],\t\ttrain_loss: 4.6184,\tval_loss: 6.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:16,143 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84:\t[0s / 0s],\t\ttrain_loss: 4.6088,\tval_loss: 6.3463\n",
      "85:\t[0s / 0s],\t\ttrain_loss: 4.6013,\tval_loss: 6.1670\n",
      "86:\t[0s / 0s],\t\ttrain_loss: 4.6261,\tval_loss: 6.0214\n",
      "87:\t[0s / 0s],\t\ttrain_loss: 4.6163,\tval_loss: 6.1141\n",
      "88:\t[0s / 0s],\t\ttrain_loss: 4.6053,\tval_loss: 6.2017\n",
      "89:\t[0s / 0s],\t\ttrain_loss: 4.6292,\tval_loss: 6.1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:16,542 - INFO - Performing clustering iteration 18 / 20\n",
      "2024-11-15 23:38:16,542 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:16,545 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90:\t[0s / 0s],\t\ttrain_loss: 4.6066,\tval_loss: 6.1575\n",
      "91:\t[0s / 0s],\t\ttrain_loss: 4.6072,\tval_loss: 6.1915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:16,912 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:17,067 - INFO - Performing clustering iteration 19 / 20\n",
      "2024-11-15 23:38:17,068 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:17,070 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92:\t[0s / 0s],\t\ttrain_loss: 4.5918,\tval_loss: 6.1420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:17,428 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:38:17,584 - INFO - Performing clustering iteration 20 / 20\n",
      "2024-11-15 23:38:17,585 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:38:17,588 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93:\t[0s / 0s],\t\ttrain_loss: 4.6236,\tval_loss: 6.1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:17,958 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94:\t[0s / 0s],\t\ttrain_loss: 4.6207,\tval_loss: 6.1573\n",
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_clustering_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_clustering_1_hazard.pkl.\n",
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:38:18,782 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:38:18,788 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: ANN\n",
      "data balancing method: smoteenn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/imblearn/over_sampling/_smote/base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n",
      "2024-11-15 23:38:22,924 - INFO - Missing values imputed using IterativeImputer.\n",
      "2024-11-15 23:38:22,934 - INFO - Dataframe rebalanced with SMOTE and ENN.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[2s / 2s],\t\ttrain_loss: 3.7049,\tval_loss: 5.0127\n",
      "1:\t[2s / 5s],\t\ttrain_loss: 3.6708,\tval_loss: 5.0493\n",
      "2:\t[2s / 8s],\t\ttrain_loss: 3.6362,\tval_loss: 5.0243\n",
      "3:\t[2s / 10s],\t\ttrain_loss: 3.6525,\tval_loss: 5.2003\n",
      "4:\t[2s / 13s],\t\ttrain_loss: 3.6420,\tval_loss: 5.0970\n",
      "5:\t[2s / 16s],\t\ttrain_loss: 3.6308,\tval_loss: 5.0213\n",
      "6:\t[2s / 18s],\t\ttrain_loss: 3.6192,\tval_loss: 5.0222\n",
      "7:\t[2s / 21s],\t\ttrain_loss: 3.6382,\tval_loss: 5.0518\n",
      "8:\t[6s / 27s],\t\ttrain_loss: 3.6333,\tval_loss: 5.0301\n",
      "9:\t[2s / 29s],\t\ttrain_loss: 3.6285,\tval_loss: 5.0079\n",
      "10:\t[2s / 32s],\t\ttrain_loss: 3.6230,\tval_loss: 5.0445\n",
      "11:\t[2s / 34s],\t\ttrain_loss: 3.6155,\tval_loss: 5.0227\n",
      "12:\t[2s / 37s],\t\ttrain_loss: 3.6104,\tval_loss: 5.0244\n",
      "13:\t[2s / 39s],\t\ttrain_loss: 3.6048,\tval_loss: 5.0159\n",
      "14:\t[2s / 42s],\t\ttrain_loss: 3.6038,\tval_loss: 5.0234\n",
      "15:\t[2s / 44s],\t\ttrain_loss: 3.6190,\tval_loss: 5.1283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smoteenn_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smoteenn_1_hazard.pkl.\n",
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:39:08,627 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:39:08,634 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: ANN\n",
      "data balancing method: smotetomek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/imblearn/over_sampling/_smote/base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n",
      "2024-11-15 23:39:12,832 - INFO - Missing values imputed using IterativeImputer.\n",
      "2024-11-15 23:39:12,837 - INFO - Dataframe rebalanced with SMOTE and Tomek.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[2s / 2s],\t\ttrain_loss: 3.6834,\tval_loss: 4.9195\n",
      "1:\t[2s / 5s],\t\ttrain_loss: 3.6443,\tval_loss: 4.9716\n",
      "2:\t[2s / 8s],\t\ttrain_loss: 3.6111,\tval_loss: 4.9026\n",
      "3:\t[6s / 14s],\t\ttrain_loss: 3.6332,\tval_loss: 4.8952\n",
      "4:\t[2s / 17s],\t\ttrain_loss: 3.6232,\tval_loss: 4.9265\n",
      "5:\t[2s / 19s],\t\ttrain_loss: 3.6095,\tval_loss: 4.9254\n",
      "6:\t[2s / 22s],\t\ttrain_loss: 3.5997,\tval_loss: 4.9153\n",
      "7:\t[2s / 25s],\t\ttrain_loss: 3.6251,\tval_loss: 5.0457\n",
      "8:\t[2s / 27s],\t\ttrain_loss: 3.6175,\tval_loss: 4.9683\n",
      "9:\t[2s / 30s],\t\ttrain_loss: 3.6121,\tval_loss: 4.9377\n",
      "10:\t[2s / 33s],\t\ttrain_loss: 3.6123,\tval_loss: 4.9208\n",
      "11:\t[2s / 35s],\t\ttrain_loss: 3.6026,\tval_loss: 4.8810\n",
      "12:\t[2s / 38s],\t\ttrain_loss: 3.5956,\tval_loss: 4.9226\n",
      "13:\t[2s / 41s],\t\ttrain_loss: 3.5942,\tval_loss: 4.9058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smotetomek_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smotetomek_1_hazard.pkl.\n",
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:39:54,822 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:39:54,828 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:39:54,833 - INFO - Performing clustering iteration 1 / 20\n",
      "2024-11-15 23:39:54,834 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:39:54,836 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: ANN\n",
      "data balancing method: clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:39:55,394 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 4.9144,\tval_loss: 7.7658\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 4.8010,\tval_loss: 7.7832\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 4.7803,\tval_loss: 7.7904\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 4.7784,\tval_loss: 7.7887\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 4.7645,\tval_loss: 7.7980\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 4.7691,\tval_loss: 7.7994\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 4.7514,\tval_loss: 7.7963\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 4.7597,\tval_loss: 7.8178\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 4.7688,\tval_loss: 7.8023\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 4.7658,\tval_loss: 7.8010\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 4.7633,\tval_loss: 7.7893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:39:56,143 - INFO - Performing clustering iteration 2 / 20\n",
      "2024-11-15 23:39:56,143 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:39:56,146 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:39:56,539 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:39:56,717 - INFO - Performing clustering iteration 3 / 20\n",
      "2024-11-15 23:39:56,717 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:39:56,720 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:\t[0s / 0s],\t\ttrain_loss: 4.8377,\tval_loss: 7.7744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:39:57,096 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:39:57,292 - INFO - Performing clustering iteration 4 / 20\n",
      "2024-11-15 23:39:57,292 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:39:57,296 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:\t[0s / 0s],\t\ttrain_loss: 4.9166,\tval_loss: 7.7662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:39:57,664 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:\t[0s / 0s],\t\ttrain_loss: 5.0643,\tval_loss: 7.7514\n",
      "14:\t[0s / 0s],\t\ttrain_loss: 5.0404,\tval_loss: 7.7497\n",
      "15:\t[3s / 3s],\t\ttrain_loss: 5.0334,\tval_loss: 7.7524\n",
      "16:\t[0s / 3s],\t\ttrain_loss: 5.0277,\tval_loss: 7.7549\n",
      "17:\t[0s / 3s],\t\ttrain_loss: 5.0254,\tval_loss: 7.7560\n",
      "18:\t[0s / 3s],\t\ttrain_loss: 5.0119,\tval_loss: 7.7649\n",
      "19:\t[0s / 3s],\t\ttrain_loss: 5.0121,\tval_loss: 7.7579\n",
      "20:\t[0s / 3s],\t\ttrain_loss: 5.0099,\tval_loss: 7.7723\n",
      "21:\t[0s / 4s],\t\ttrain_loss: 5.0099,\tval_loss: 7.7752\n",
      "22:\t[0s / 4s],\t\ttrain_loss: 5.0002,\tval_loss: 7.7663\n",
      "23:\t[0s / 4s],\t\ttrain_loss: 5.0047,\tval_loss: 7.7761\n",
      "24:\t[0s / 4s],\t\ttrain_loss: 5.0055,\tval_loss: 7.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:02,000 - INFO - Performing clustering iteration 5 / 20\n",
      "2024-11-15 23:40:02,001 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:02,004 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:40:02,373 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:02,547 - INFO - Performing clustering iteration 6 / 20\n",
      "2024-11-15 23:40:02,547 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:02,550 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25:\t[0s / 0s],\t\ttrain_loss: 5.0442,\tval_loss: 7.7525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:02,905 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:03,086 - INFO - Performing clustering iteration 7 / 20\n",
      "2024-11-15 23:40:03,087 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:03,090 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26:\t[0s / 0s],\t\ttrain_loss: 5.0522,\tval_loss: 7.7498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:03,427 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:03,606 - INFO - Performing clustering iteration 8 / 20\n",
      "2024-11-15 23:40:03,607 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:03,609 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27:\t[0s / 0s],\t\ttrain_loss: 5.0629,\tval_loss: 7.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:03,935 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:04,115 - INFO - Performing clustering iteration 9 / 20\n",
      "2024-11-15 23:40:04,115 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:04,119 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28:\t[0s / 0s],\t\ttrain_loss: 5.0598,\tval_loss: 7.7498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:04,455 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:04,638 - INFO - Performing clustering iteration 10 / 20\n",
      "2024-11-15 23:40:04,639 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:04,642 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29:\t[0s / 0s],\t\ttrain_loss: 5.0760,\tval_loss: 7.7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:04,980 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:05,176 - INFO - Performing clustering iteration 11 / 20\n",
      "2024-11-15 23:40:05,177 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:05,180 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30:\t[0s / 0s],\t\ttrain_loss: 5.0863,\tval_loss: 7.7541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:05,497 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:05,691 - INFO - Performing clustering iteration 12 / 20\n",
      "2024-11-15 23:40:05,691 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:05,694 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31:\t[0s / 0s],\t\ttrain_loss: 5.0876,\tval_loss: 7.7519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:05,994 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:06,169 - INFO - Performing clustering iteration 13 / 20\n",
      "2024-11-15 23:40:06,169 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:06,172 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32:\t[0s / 0s],\t\ttrain_loss: 5.0849,\tval_loss: 7.7538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:06,465 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:06,639 - INFO - Performing clustering iteration 14 / 20\n",
      "2024-11-15 23:40:06,640 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:06,644 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33:\t[0s / 0s],\t\ttrain_loss: 5.1088,\tval_loss: 7.7525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:06,930 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:07,107 - INFO - Performing clustering iteration 15 / 20\n",
      "2024-11-15 23:40:07,108 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:07,111 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34:\t[0s / 0s],\t\ttrain_loss: 5.1025,\tval_loss: 7.7537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:07,394 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:07,588 - INFO - Performing clustering iteration 16 / 20\n",
      "2024-11-15 23:40:07,589 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:07,592 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35:\t[0s / 0s],\t\ttrain_loss: 5.0835,\tval_loss: 7.7556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:07,858 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:08,037 - INFO - Performing clustering iteration 17 / 20\n",
      "2024-11-15 23:40:08,038 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:08,041 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36:\t[0s / 0s],\t\ttrain_loss: 5.1060,\tval_loss: 7.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:08,304 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:08,483 - INFO - Performing clustering iteration 18 / 20\n",
      "2024-11-15 23:40:08,483 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:08,486 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37:\t[0s / 0s],\t\ttrain_loss: 5.1001,\tval_loss: 7.7504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:08,749 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:08,936 - INFO - Performing clustering iteration 19 / 20\n",
      "2024-11-15 23:40:08,937 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:08,940 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38:\t[0s / 0s],\t\ttrain_loss: 5.1054,\tval_loss: 7.7547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:09,186 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:40:09,380 - INFO - Performing clustering iteration 20 / 20\n",
      "2024-11-15 23:40:09,381 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:40:09,384 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39:\t[0s / 0s],\t\ttrain_loss: 5.0835,\tval_loss: 7.7549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:09,623 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40:\t[0s / 0s],\t\ttrain_loss: 5.1034,\tval_loss: 7.7561\n",
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_clustering_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_clustering_2_hazard.pkl.\n",
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:10,430 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:40:10,437 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: ANN\n",
      "data balancing method: smoteenn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/imblearn/over_sampling/_smote/base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n",
      "2024-11-15 23:40:14,202 - INFO - Missing values imputed using IterativeImputer.\n",
      "2024-11-15 23:40:14,210 - INFO - Dataframe rebalanced with SMOTE and ENN.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[1s / 1s],\t\ttrain_loss: 4.8324,\tval_loss: 7.6427\n",
      "1:\t[1s / 3s],\t\ttrain_loss: 4.6138,\tval_loss: 7.5167\n",
      "2:\t[1s / 5s],\t\ttrain_loss: 4.5290,\tval_loss: 7.4838\n",
      "3:\t[1s / 7s],\t\ttrain_loss: 4.5476,\tval_loss: 7.4889\n",
      "4:\t[1s / 9s],\t\ttrain_loss: 4.5359,\tval_loss: 7.5161\n",
      "5:\t[1s / 10s],\t\ttrain_loss: 4.5164,\tval_loss: 7.5008\n",
      "6:\t[1s / 12s],\t\ttrain_loss: 4.4945,\tval_loss: 7.4864\n",
      "7:\t[1s / 14s],\t\ttrain_loss: 4.5217,\tval_loss: 7.4929\n",
      "8:\t[1s / 16s],\t\ttrain_loss: 4.5093,\tval_loss: 7.4916\n",
      "9:\t[1s / 18s],\t\ttrain_loss: 4.5003,\tval_loss: 7.5183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smoteenn_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smoteenn_2_hazard.pkl.\n",
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:40:33,235 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:40:33,240 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: ANN\n",
      "data balancing method: smotetomek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/imblearn/over_sampling/_smote/base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n",
      "2024-11-15 23:40:40,623 - INFO - Missing values imputed using IterativeImputer.\n",
      "2024-11-15 23:40:40,631 - INFO - Dataframe rebalanced with SMOTE and Tomek.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[2s / 2s],\t\ttrain_loss: 4.6938,\tval_loss: 7.4255\n",
      "1:\t[2s / 4s],\t\ttrain_loss: 4.6640,\tval_loss: 7.3824\n",
      "2:\t[2s / 6s],\t\ttrain_loss: 4.5899,\tval_loss: 7.3939\n",
      "3:\t[2s / 8s],\t\ttrain_loss: 4.6403,\tval_loss: 7.4294\n",
      "4:\t[2s / 11s],\t\ttrain_loss: 4.6187,\tval_loss: 7.4090\n",
      "5:\t[2s / 13s],\t\ttrain_loss: 4.5856,\tval_loss: 7.3997\n",
      "6:\t[2s / 15s],\t\ttrain_loss: 4.5693,\tval_loss: 7.4068\n",
      "7:\t[2s / 17s],\t\ttrain_loss: 4.6241,\tval_loss: 7.4783\n",
      "8:\t[2s / 20s],\t\ttrain_loss: 4.6051,\tval_loss: 7.4627\n",
      "9:\t[2s / 22s],\t\ttrain_loss: 4.5962,\tval_loss: 7.5095\n",
      "10:\t[2s / 24s],\t\ttrain_loss: 4.5839,\tval_loss: 7.4644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smotetomek_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smotetomek_2_hazard.pkl.\n",
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:41:05,901 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:05,998 - INFO - Performing clustering iteration 1 / 20\n",
      "2024-11-15 23:41:05,998 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:06,002 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: LSTM\n",
      "data balancing method: clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:41:06,605 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:06,606 - INFO - Performing clustering iteration 2 / 20\n",
      "2024-11-15 23:41:06,607 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:06,609 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:07,006 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:07,007 - INFO - Performing clustering iteration 3 / 20\n",
      "2024-11-15 23:41:07,008 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:07,010 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:07,412 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:07,413 - INFO - Performing clustering iteration 4 / 20\n",
      "2024-11-15 23:41:07,414 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:07,416 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:07,803 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:07,804 - INFO - Performing clustering iteration 5 / 20\n",
      "2024-11-15 23:41:07,804 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:07,807 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:08,193 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:08,195 - INFO - Performing clustering iteration 6 / 20\n",
      "2024-11-15 23:41:08,195 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:08,198 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:08,578 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:08,579 - INFO - Performing clustering iteration 7 / 20\n",
      "2024-11-15 23:41:08,580 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:08,582 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:08,959 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:08,960 - INFO - Performing clustering iteration 8 / 20\n",
      "2024-11-15 23:41:08,961 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:08,963 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:09,366 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:09,367 - INFO - Performing clustering iteration 9 / 20\n",
      "2024-11-15 23:41:09,367 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:09,370 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:09,753 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:09,755 - INFO - Performing clustering iteration 10 / 20\n",
      "2024-11-15 23:41:09,755 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:09,758 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:13,719 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:13,721 - INFO - Performing clustering iteration 11 / 20\n",
      "2024-11-15 23:41:13,722 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:13,725 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:14,096 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:14,098 - INFO - Performing clustering iteration 12 / 20\n",
      "2024-11-15 23:41:14,098 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:14,100 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:14,464 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:14,464 - INFO - Performing clustering iteration 13 / 20\n",
      "2024-11-15 23:41:14,465 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:14,467 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:14,825 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:14,826 - INFO - Performing clustering iteration 14 / 20\n",
      "2024-11-15 23:41:14,826 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:14,828 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:15,193 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:15,194 - INFO - Performing clustering iteration 15 / 20\n",
      "2024-11-15 23:41:15,196 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:15,199 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:15,549 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:15,551 - INFO - Performing clustering iteration 16 / 20\n",
      "2024-11-15 23:41:15,551 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:15,553 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:15,903 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:15,904 - INFO - Performing clustering iteration 17 / 20\n",
      "2024-11-15 23:41:15,904 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:15,907 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:16,252 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:16,253 - INFO - Performing clustering iteration 18 / 20\n",
      "2024-11-15 23:41:16,254 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:16,256 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:16,601 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:16,602 - INFO - Performing clustering iteration 19 / 20\n",
      "2024-11-15 23:41:16,602 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:16,605 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:16,946 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:16,947 - INFO - Performing clustering iteration 20 / 20\n",
      "2024-11-15 23:41:16,948 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:41:16,950 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:41:17,298 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:41:17,309 - INFO - Cluster data retrieved\n",
      "2024-11-15 23:41:34,676 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:42:00,669 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 3.2189,\tval_loss: 5.3828\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 3.0697,\tval_loss: 5.8590\n",
      "2:\t[0s / 2s],\t\ttrain_loss: 2.2388,\tval_loss: 5.5454\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 2.2063,\tval_loss: 5.1473\n",
      "4:\t[0s / 3s],\t\ttrain_loss: 2.1677,\tval_loss: 5.6474\n",
      "5:\t[0s / 4s],\t\ttrain_loss: 2.0999,\tval_loss: 5.1721\n",
      "6:\t[0s / 4s],\t\ttrain_loss: 2.0678,\tval_loss: 5.3049\n",
      "7:\t[0s / 5s],\t\ttrain_loss: 2.1510,\tval_loss: 5.6118\n",
      "8:\t[0s / 6s],\t\ttrain_loss: 2.1197,\tval_loss: 5.7434\n",
      "9:\t[0s / 6s],\t\ttrain_loss: 2.0947,\tval_loss: 5.2586\n",
      "10:\t[0s / 7s],\t\ttrain_loss: 2.1024,\tval_loss: 5.2827\n",
      "11:\t[0s / 8s],\t\ttrain_loss: 2.0948,\tval_loss: 5.0368\n",
      "12:\t[0s / 8s],\t\ttrain_loss: 2.0674,\tval_loss: 5.1209\n",
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_clustering_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_clustering_1_hazard.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:42:10,387 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "/mnt/d/PYDataScience/g3_regress/code/databalancer2.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['_original_index'] = df.index\n",
      "2024-11-15 23:42:10,488 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: LSTM\n",
      "data balancing method: NearMiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n",
      "2024-11-15 23:42:10,788 - INFO - Dataset for deepsurv model undersampled using method 'NearMiss' with sampling strategy 0.05.\n",
      "2024-11-15 23:42:15,668 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:42:42,697 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 5.1360,\tval_loss: 7.8974\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 5.1355,\tval_loss: 7.8970\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 5.1179,\tval_loss: 7.8969\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 5.1235,\tval_loss: 7.8965\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 5.1454,\tval_loss: 7.8961\n",
      "5:\t[0s / 3s],\t\ttrain_loss: 5.1308,\tval_loss: 7.8958\n",
      "6:\t[0s / 3s],\t\ttrain_loss: 5.1366,\tval_loss: 7.8958\n",
      "7:\t[0s / 4s],\t\ttrain_loss: 5.1391,\tval_loss: 7.8951\n",
      "8:\t[0s / 4s],\t\ttrain_loss: 5.1251,\tval_loss: 7.8943\n",
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_nearmiss_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_nearmiss_1_hazard.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:42:48,139 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:48,233 - INFO - Performing clustering iteration 1 / 20\n",
      "2024-11-15 23:42:48,233 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:48,236 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: LSTM\n",
      "data balancing method: clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:42:48,772 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:48,773 - INFO - Performing clustering iteration 2 / 20\n",
      "2024-11-15 23:42:48,773 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:48,775 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:49,163 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:49,164 - INFO - Performing clustering iteration 3 / 20\n",
      "2024-11-15 23:42:49,164 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:49,166 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:49,537 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:49,538 - INFO - Performing clustering iteration 4 / 20\n",
      "2024-11-15 23:42:49,538 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:49,541 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:49,905 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:49,906 - INFO - Performing clustering iteration 5 / 20\n",
      "2024-11-15 23:42:49,906 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:49,910 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:50,287 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:50,288 - INFO - Performing clustering iteration 6 / 20\n",
      "2024-11-15 23:42:50,288 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:50,292 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:50,657 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:50,658 - INFO - Performing clustering iteration 7 / 20\n",
      "2024-11-15 23:42:50,659 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:50,662 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:51,024 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:51,025 - INFO - Performing clustering iteration 8 / 20\n",
      "2024-11-15 23:42:51,025 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:51,028 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:51,365 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:51,365 - INFO - Performing clustering iteration 9 / 20\n",
      "2024-11-15 23:42:51,366 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:51,370 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:51,714 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:51,716 - INFO - Performing clustering iteration 10 / 20\n",
      "2024-11-15 23:42:51,717 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:51,720 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:52,040 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:52,041 - INFO - Performing clustering iteration 11 / 20\n",
      "2024-11-15 23:42:52,041 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:52,043 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:52,357 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:52,358 - INFO - Performing clustering iteration 12 / 20\n",
      "2024-11-15 23:42:52,359 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:52,361 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:52,664 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:52,665 - INFO - Performing clustering iteration 13 / 20\n",
      "2024-11-15 23:42:52,666 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:52,668 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:52,972 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:52,973 - INFO - Performing clustering iteration 14 / 20\n",
      "2024-11-15 23:42:52,974 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:52,976 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:53,273 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:53,274 - INFO - Performing clustering iteration 15 / 20\n",
      "2024-11-15 23:42:53,274 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:53,276 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:53,562 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:53,563 - INFO - Performing clustering iteration 16 / 20\n",
      "2024-11-15 23:42:53,563 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:53,566 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:53,844 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:53,845 - INFO - Performing clustering iteration 17 / 20\n",
      "2024-11-15 23:42:53,845 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:53,847 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:54,146 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:54,147 - INFO - Performing clustering iteration 18 / 20\n",
      "2024-11-15 23:42:54,148 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:54,150 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:54,406 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:54,406 - INFO - Performing clustering iteration 19 / 20\n",
      "2024-11-15 23:42:54,407 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:54,409 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:54,665 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:54,666 - INFO - Performing clustering iteration 20 / 20\n",
      "2024-11-15 23:42:54,666 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:42:54,669 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:42:54,920 - INFO - Defined medoid for deepsurv model with 3725 clusters.\n",
      "2024-11-15 23:42:54,948 - INFO - Cluster data retrieved\n",
      "2024-11-15 23:43:51,311 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:44:18,039 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[1s / 1s],\t\ttrain_loss: 4.9840\n",
      "1:\t[1s / 2s],\t\ttrain_loss: 4.9825\n",
      "2:\t[1s / 3s],\t\ttrain_loss: 4.9566\n",
      "3:\t[1s / 5s],\t\ttrain_loss: 4.9380\n",
      "4:\t[1s / 6s],\t\ttrain_loss: 4.8915\n",
      "5:\t[1s / 7s],\t\ttrain_loss: 4.8752\n",
      "6:\t[1s / 8s],\t\ttrain_loss: 4.8627\n",
      "7:\t[1s / 10s],\t\ttrain_loss: 4.8587\n",
      "8:\t[1s / 11s],\t\ttrain_loss: 4.8425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_clustering_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_clustering_2_hazard.pkl.\n",
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:44:30,366 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "/mnt/d/PYDataScience/g3_regress/code/databalancer2.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['_original_index'] = df.index\n",
      "2024-11-15 23:44:30,474 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deepsurv neural network\n",
      "model structure: LSTM\n",
      "data balancing method: NearMiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n",
      "2024-11-15 23:44:30,800 - INFO - Dataset for deepsurv model undersampled using method 'NearMiss' with sampling strategy 0.05.\n",
      "2024-11-15 23:44:58,994 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-15 23:45:32,421 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 5.0672\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 5.0364\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 4.9895\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 4.9804\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 4.9804\n",
      "5:\t[0s / 3s],\t\ttrain_loss: 4.9720\n",
      "6:\t[0s / 4s],\t\ttrain_loss: 4.9666\n",
      "7:\t[0s / 4s],\t\ttrain_loss: 4.9780\n",
      "8:\t[0s / 5s],\t\ttrain_loss: 4.9683\n",
      "9:\t[0s / 5s],\t\ttrain_loss: 4.9642\n",
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_nearmiss_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_nearmiss_2_hazard.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:45:38,855 - INFO - Performing clustering iteration 1 / 20\n",
      "2024-11-15 23:45:38,855 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deephit neural network\n",
      "model structure: ANN\n",
      "data balancing method: clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:45:39,393 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 1.7559,\tval_loss: 0.0801\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 0.5162,\tval_loss: 0.0713\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 0.4664,\tval_loss: 0.0699\n",
      "3:\t[0s / 1s],\t\ttrain_loss: 0.4431,\tval_loss: 0.0731\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 0.4144,\tval_loss: 0.0728\n",
      "5:\t[0s / 2s],\t\ttrain_loss: 0.4015,\tval_loss: 0.0743\n",
      "6:\t[0s / 2s],\t\ttrain_loss: 0.3978,\tval_loss: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:45:42,832 - INFO - Performing clustering iteration 2 / 20\n",
      "2024-11-15 23:45:42,832 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7:\t[0s / 3s],\t\ttrain_loss: 0.3859,\tval_loss: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:45:43,222 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8:\t[0s / 0s],\t\ttrain_loss: 0.4676,\tval_loss: 0.0701\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 0.4190,\tval_loss: 0.0683\n",
      "10:\t[0s / 1s],\t\ttrain_loss: 0.3914,\tval_loss: 0.0624\n",
      "11:\t[0s / 1s],\t\ttrain_loss: 0.3826,\tval_loss: 0.0642\n",
      "12:\t[0s / 1s],\t\ttrain_loss: 0.3750,\tval_loss: 0.0659\n",
      "13:\t[0s / 2s],\t\ttrain_loss: 0.3747,\tval_loss: 0.0652\n",
      "14:\t[0s / 2s],\t\ttrain_loss: 0.3738,\tval_loss: 0.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:45:46,488 - INFO - Performing clustering iteration 3 / 20\n",
      "2024-11-15 23:45:46,489 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:\t[0s / 3s],\t\ttrain_loss: 0.3679,\tval_loss: 0.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:45:46,879 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:\t[0s / 0s],\t\ttrain_loss: 0.3614,\tval_loss: 0.0636\n",
      "17:\t[0s / 0s],\t\ttrain_loss: 0.3533,\tval_loss: 0.0619\n",
      "18:\t[0s / 1s],\t\ttrain_loss: 0.3489,\tval_loss: 0.0647\n",
      "19:\t[0s / 1s],\t\ttrain_loss: 0.3464,\tval_loss: 0.0656\n",
      "20:\t[0s / 1s],\t\ttrain_loss: 0.3459,\tval_loss: 0.0605\n",
      "21:\t[0s / 2s],\t\ttrain_loss: 0.3399,\tval_loss: 0.0602\n",
      "22:\t[0s / 2s],\t\ttrain_loss: 0.3382,\tval_loss: 0.0615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:45:50,086 - INFO - Performing clustering iteration 4 / 20\n",
      "2024-11-15 23:45:50,086 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:\t[0s / 3s],\t\ttrain_loss: 0.3376,\tval_loss: 0.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:45:50,439 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24:\t[0s / 0s],\t\ttrain_loss: 0.3393,\tval_loss: 0.0615\n",
      "25:\t[0s / 0s],\t\ttrain_loss: 0.3376,\tval_loss: 0.0599\n",
      "26:\t[0s / 1s],\t\ttrain_loss: 0.3395,\tval_loss: 0.0612\n",
      "27:\t[0s / 1s],\t\ttrain_loss: 0.3344,\tval_loss: 0.0582\n",
      "28:\t[0s / 1s],\t\ttrain_loss: 0.3326,\tval_loss: 0.0601\n",
      "29:\t[0s / 2s],\t\ttrain_loss: 0.3362,\tval_loss: 0.0599\n",
      "30:\t[0s / 2s],\t\ttrain_loss: 0.3355,\tval_loss: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:45:53,510 - INFO - Performing clustering iteration 5 / 20\n",
      "2024-11-15 23:45:53,510 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31:\t[0s / 2s],\t\ttrain_loss: 0.3382,\tval_loss: 0.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:45:53,872 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32:\t[0s / 0s],\t\ttrain_loss: 0.3389,\tval_loss: 0.0639\n",
      "33:\t[0s / 0s],\t\ttrain_loss: 0.3331,\tval_loss: 0.0656\n",
      "34:\t[0s / 1s],\t\ttrain_loss: 0.3354,\tval_loss: 0.0642\n",
      "35:\t[0s / 1s],\t\ttrain_loss: 0.3468,\tval_loss: 0.0528\n",
      "36:\t[0s / 1s],\t\ttrain_loss: 0.3347,\tval_loss: 0.0602\n",
      "37:\t[0s / 2s],\t\ttrain_loss: 0.3327,\tval_loss: 0.0595\n",
      "38:\t[0s / 2s],\t\ttrain_loss: 0.3287,\tval_loss: 0.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:45:56,927 - INFO - Performing clustering iteration 6 / 20\n",
      "2024-11-15 23:45:56,927 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39:\t[0s / 2s],\t\ttrain_loss: 0.3317,\tval_loss: 0.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:00,768 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40:\t[0s / 0s],\t\ttrain_loss: 0.3375,\tval_loss: 0.0605\n",
      "41:\t[0s / 0s],\t\ttrain_loss: 0.3329,\tval_loss: 0.0565\n",
      "42:\t[0s / 1s],\t\ttrain_loss: 0.3314,\tval_loss: 0.0566\n",
      "43:\t[0s / 1s],\t\ttrain_loss: 0.3311,\tval_loss: 0.0508\n",
      "44:\t[0s / 1s],\t\ttrain_loss: 0.3341,\tval_loss: 0.0654\n",
      "45:\t[0s / 2s],\t\ttrain_loss: 0.3323,\tval_loss: 0.0533\n",
      "46:\t[0s / 2s],\t\ttrain_loss: 0.3336,\tval_loss: 0.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:03,866 - INFO - Performing clustering iteration 7 / 20\n",
      "2024-11-15 23:46:03,866 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47:\t[0s / 2s],\t\ttrain_loss: 0.3322,\tval_loss: 0.0598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:04,200 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48:\t[0s / 0s],\t\ttrain_loss: 0.3351,\tval_loss: 0.0613\n",
      "49:\t[0s / 0s],\t\ttrain_loss: 0.3300,\tval_loss: 0.0587\n",
      "50:\t[0s / 1s],\t\ttrain_loss: 0.3328,\tval_loss: 0.0589\n",
      "51:\t[0s / 1s],\t\ttrain_loss: 0.3262,\tval_loss: 0.0555\n",
      "52:\t[0s / 1s],\t\ttrain_loss: 0.3276,\tval_loss: 0.0589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:06,541 - INFO - Performing clustering iteration 8 / 20\n",
      "2024-11-15 23:46:06,542 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53:\t[0s / 2s],\t\ttrain_loss: 0.3307,\tval_loss: 0.0575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:06,858 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:07,387 - INFO - Performing clustering iteration 9 / 20\n",
      "2024-11-15 23:46:07,388 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54:\t[0s / 0s],\t\ttrain_loss: 0.3305,\tval_loss: 0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:07,708 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:08,257 - INFO - Performing clustering iteration 10 / 20\n",
      "2024-11-15 23:46:08,257 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55:\t[0s / 0s],\t\ttrain_loss: 0.3324,\tval_loss: 0.0583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:08,547 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:09,036 - INFO - Performing clustering iteration 11 / 20\n",
      "2024-11-15 23:46:09,036 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56:\t[0s / 0s],\t\ttrain_loss: 0.3339,\tval_loss: 0.0590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:09,320 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:09,869 - INFO - Performing clustering iteration 12 / 20\n",
      "2024-11-15 23:46:09,869 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57:\t[0s / 0s],\t\ttrain_loss: 0.3350,\tval_loss: 0.0584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:10,154 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:10,644 - INFO - Performing clustering iteration 13 / 20\n",
      "2024-11-15 23:46:10,644 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58:\t[0s / 0s],\t\ttrain_loss: 0.3346,\tval_loss: 0.0564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:10,905 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:11,385 - INFO - Performing clustering iteration 14 / 20\n",
      "2024-11-15 23:46:11,385 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59:\t[0s / 0s],\t\ttrain_loss: 0.3386,\tval_loss: 0.0546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:11,649 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:12,159 - INFO - Performing clustering iteration 15 / 20\n",
      "2024-11-15 23:46:12,159 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60:\t[0s / 0s],\t\ttrain_loss: 0.3403,\tval_loss: 0.0530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:12,418 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:12,912 - INFO - Performing clustering iteration 16 / 20\n",
      "2024-11-15 23:46:12,912 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61:\t[0s / 0s],\t\ttrain_loss: 0.3453,\tval_loss: 0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:13,144 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62:\t[0s / 0s],\t\ttrain_loss: 0.3780,\tval_loss: 0.0493\n",
      "63:\t[0s / 0s],\t\ttrain_loss: 0.3552,\tval_loss: 0.0593\n",
      "64:\t[0s / 1s],\t\ttrain_loss: 0.3454,\tval_loss: 0.0539\n",
      "65:\t[0s / 1s],\t\ttrain_loss: 0.3388,\tval_loss: 0.0555\n",
      "66:\t[0s / 1s],\t\ttrain_loss: 0.3392,\tval_loss: 0.0561\n",
      "67:\t[0s / 2s],\t\ttrain_loss: 0.3357,\tval_loss: 0.0584\n",
      "68:\t[0s / 2s],\t\ttrain_loss: 0.3364,\tval_loss: 0.0540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:16,239 - INFO - Performing clustering iteration 17 / 20\n",
      "2024-11-15 23:46:16,239 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69:\t[0s / 2s],\t\ttrain_loss: 0.3329,\tval_loss: 0.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:16,463 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70:\t[0s / 0s],\t\ttrain_loss: 0.3514,\tval_loss: 0.0510\n",
      "71:\t[0s / 0s],\t\ttrain_loss: 0.3403,\tval_loss: 0.0568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:17,692 - INFO - Performing clustering iteration 18 / 20\n",
      "2024-11-15 23:46:17,693 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72:\t[0s / 1s],\t\ttrain_loss: 0.3387,\tval_loss: 0.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:17,921 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:18,419 - INFO - Performing clustering iteration 19 / 20\n",
      "2024-11-15 23:46:18,420 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73:\t[0s / 0s],\t\ttrain_loss: 0.3518,\tval_loss: 0.0585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:18,621 - INFO - Defined medoid for deephit model with 4932 clusters.\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "2024-11-15 23:46:19,157 - INFO - Performing clustering iteration 20 / 20\n",
      "2024-11-15 23:46:19,157 - INFO - CUDA environment set up and GPU memory cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74:\t[0s / 0s],\t\ttrain_loss: 0.3447,\tval_loss: 0.0556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:19,356 - INFO - Defined medoid for deephit model with 4932 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75:\t[0s / 0s],\t\ttrain_loss: 0.3463,\tval_loss: 0.0570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deephit_ann_clustering_all.pt and /mnt/d/PYDataScience/g3_regress/code/models/deephit_ann_clustering_all_hazard.pkl.\n",
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n",
      "Initiate training of deephit neural network\n",
      "model structure: ANN\n",
      "data balancing method: NearMiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/PYDataScience/g3_regress/code/databalancer2.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['_original_index'] = df.index\n",
      "2024-11-15 23:46:30,859 - INFO - Dataset for deephit model undersampled using method 'NearMiss' with sampling strategy 0.05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[2s / 2s],\t\ttrain_loss: 0.1141,\tval_loss: 0.0613\n",
      "1:\t[4s / 6s],\t\ttrain_loss: 0.0735,\tval_loss: 0.0353\n",
      "2:\t[1s / 8s],\t\ttrain_loss: 0.0613,\tval_loss: 0.0324\n",
      "3:\t[1s / 9s],\t\ttrain_loss: 0.0587,\tval_loss: 0.0281\n",
      "4:\t[1s / 11s],\t\ttrain_loss: 0.0556,\tval_loss: 0.0271\n",
      "5:\t[1s / 12s],\t\ttrain_loss: 0.0550,\tval_loss: 0.0269\n",
      "6:\t[1s / 14s],\t\ttrain_loss: 0.0549,\tval_loss: 0.0270\n",
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deephit_ann_nearmiss2_all.pt and /mnt/d/PYDataScience/g3_regress/code/models/deephit_ann_nearmiss2_all_hazard.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:45,959 - INFO - Performing clustering iteration 1 / 20\n",
      "2024-11-15 23:46:45,959 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:45,965 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate training of deephit neural network\n",
      "model structure: LSTM\n",
      "data balancing method: clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:46:46,391 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:46,392 - INFO - Performing clustering iteration 2 / 20\n",
      "2024-11-15 23:46:46,393 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:46,396 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:46,813 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:46,814 - INFO - Performing clustering iteration 3 / 20\n",
      "2024-11-15 23:46:46,814 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:46,818 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:47,222 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:47,223 - INFO - Performing clustering iteration 4 / 20\n",
      "2024-11-15 23:46:47,224 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:47,226 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:47,626 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:47,627 - INFO - Performing clustering iteration 5 / 20\n",
      "2024-11-15 23:46:47,628 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:47,630 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:48,026 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:48,027 - INFO - Performing clustering iteration 6 / 20\n",
      "2024-11-15 23:46:48,028 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:48,031 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:48,427 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:48,428 - INFO - Performing clustering iteration 7 / 20\n",
      "2024-11-15 23:46:48,428 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:48,431 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:48,827 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:48,828 - INFO - Performing clustering iteration 8 / 20\n",
      "2024-11-15 23:46:48,829 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:48,831 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:49,220 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:49,221 - INFO - Performing clustering iteration 9 / 20\n",
      "2024-11-15 23:46:49,221 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:49,224 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:49,608 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:49,610 - INFO - Performing clustering iteration 10 / 20\n",
      "2024-11-15 23:46:49,610 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:49,613 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:49,988 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:49,989 - INFO - Performing clustering iteration 11 / 20\n",
      "2024-11-15 23:46:49,989 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:49,991 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:50,360 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:50,362 - INFO - Performing clustering iteration 12 / 20\n",
      "2024-11-15 23:46:50,363 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:50,365 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:50,749 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:50,749 - INFO - Performing clustering iteration 13 / 20\n",
      "2024-11-15 23:46:50,750 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:50,752 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:51,118 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:51,119 - INFO - Performing clustering iteration 14 / 20\n",
      "2024-11-15 23:46:51,119 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:51,122 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:51,478 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:51,478 - INFO - Performing clustering iteration 15 / 20\n",
      "2024-11-15 23:46:51,479 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:51,481 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:51,846 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:51,847 - INFO - Performing clustering iteration 16 / 20\n",
      "2024-11-15 23:46:51,847 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:51,850 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:52,206 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:52,207 - INFO - Performing clustering iteration 17 / 20\n",
      "2024-11-15 23:46:52,207 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:52,209 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:52,567 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:52,568 - INFO - Performing clustering iteration 18 / 20\n",
      "2024-11-15 23:46:52,568 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:52,571 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:52,931 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:52,932 - INFO - Performing clustering iteration 19 / 20\n",
      "2024-11-15 23:46:52,932 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:52,935 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:53,280 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:53,281 - INFO - Performing clustering iteration 20 / 20\n",
      "2024-11-15 23:46:53,281 - INFO - CUDA environment set up and GPU memory cleared.\n",
      "2024-11-15 23:46:53,283 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-15 23:46:53,623 - INFO - Defined medoid for deepsurv model with 1207 clusters.\n",
      "2024-11-15 23:46:53,634 - INFO - Cluster data retrieved\n",
      "2024-11-15 23:48:13,076 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[1s / 1s],\t\ttrain_loss: 0.0581,\tval_loss: 0.0450\n",
      "1:\t[1s / 2s],\t\ttrain_loss: 0.0494,\tval_loss: 0.0408\n",
      "2:\t[1s / 4s],\t\ttrain_loss: 0.0387,\tval_loss: 0.0465\n",
      "3:\t[1s / 5s],\t\ttrain_loss: 0.0410,\tval_loss: 0.0508\n",
      "4:\t[1s / 6s],\t\ttrain_loss: 0.0381,\tval_loss: 0.0451\n",
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_clustering_all.pt and /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_clustering_all_hazard.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n",
      "Initiate training of deephit neural network\n",
      "model structure: LSTM\n",
      "data balancing method: NearMiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/PYDataScience/g3_regress/code/databalancer2.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['_original_index'] = df.index\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n",
      "2024-11-15 23:48:20,934 - INFO - Dataset for deephit model undersampled using method 'NearMiss' with sampling strategy 0.05.\n",
      "2024-11-15 23:48:53,799 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[1s / 1s],\t\ttrain_loss: 0.4743,\tval_loss: 0.0951\n",
      "1:\t[1s / 2s],\t\ttrain_loss: 0.4158,\tval_loss: 0.0645\n",
      "2:\t[4s / 6s],\t\ttrain_loss: 0.3919,\tval_loss: 0.0709\n",
      "3:\t[0s / 7s],\t\ttrain_loss: 0.3916,\tval_loss: 0.0732\n",
      "4:\t[0s / 8s],\t\ttrain_loss: 0.3853,\tval_loss: 0.0715\n",
      "5:\t[1s / 9s],\t\ttrain_loss: 0.3825,\tval_loss: 0.0693\n",
      "6:\t[0s / 10s],\t\ttrain_loss: 0.3815,\tval_loss: 0.0725\n",
      "7:\t[0s / 11s],\t\ttrain_loss: 0.3855,\tval_loss: 0.0761\n",
      "8:\t[0s / 12s],\t\ttrain_loss: 0.3829,\tval_loss: 0.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards saved to /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_nearmiss1_all.pt and /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_nearmiss1_all_hazard.pkl.\n",
      "Training and saving completed for all cross-validation splits.\n",
      "All models have been trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "gss1 = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_SEED)\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_SEED)\n",
    "for train_idx_1, fin_val_idx in gss1.split(X=X_train_transformed[FEATURE_COLS], y=X_train_transformed[EVENT_COL], groups=X_train_transformed[CLUSTER_COL]):\n",
    "    X_train_transformed_2, X_fin_val = X_train_transformed.iloc[train_idx_1, :], X_train_transformed.iloc[fin_val_idx, :]\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    for model in model_ls:\n",
    "        config_var_name = model + \"_config\"\n",
    "        model_config = globals().get(config_var_name)\n",
    "        if model_config is None:\n",
    "            print(f\"Configuration for {config_var_name} not found.\")\n",
    "            continue\n",
    "\n",
    "        model_weights_path = f'{model_path}{model}.pt'\n",
    "        model_hazard_path = f'{model_path}{model}_hazard.pkl'\n",
    "        \n",
    "        training_wrapper(X_train_transformed_2, model_config, gss2, model_weights_path, \n",
    "                        model_hazard_path, \n",
    "                        feature_col=FEATURE_COLS, duration_col=DURATION_COL, event_col=EVENT_COL, cluster_col=CLUSTER_COL, time_grid=TIME_GRID)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load models amd hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_config, model_path, baseline_hazard_path):\n",
    "    \"\"\"\n",
    "    Load model weights and baseline hazard data.\n",
    "\n",
    "    Parameters:\n",
    "    - create_model_func: Function to create the model architecture (e.g., create_neural_network).\n",
    "    - model_path: Path to load the model weights (.pt file).\n",
    "    - baseline_hazard_path: Path to load the baseline hazards (.pkl file).\n",
    "\n",
    "    Returns:\n",
    "    - model: The loaded model with weights and baseline hazards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_model_weights(model_path)\n",
    "    \n",
    "    # Load baseline hazards and assign to model\n",
    "    if model_config['model'] == 'deepsurv':\n",
    "        baseline_hazard = pd.read_pickle(baseline_hazard_path)\n",
    "        model.baseline_hazards_ = baseline_hazard\n",
    "        model.baseline_cumulative_hazards_ = baseline_hazard.cumsum()\n",
    "    \n",
    "    print(f\"Model and baseline hazards loaded from {model_path} and {baseline_hazard_path}.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_clustering_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_clustering_1_hazard.pkl.\n",
      "Loaded model deepsurv_ann_clustering_1\n",
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smoteenn_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smoteenn_1_hazard.pkl.\n",
      "Loaded model deepsurv_ann_smoteenn_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smotetomek_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smotetomek_1_hazard.pkl.\n",
      "Loaded model deepsurv_ann_smotetomek_1\n",
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_clustering_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_clustering_2_hazard.pkl.\n",
      "Loaded model deepsurv_ann_clustering_2\n",
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smoteenn_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smoteenn_2_hazard.pkl.\n",
      "Loaded model deepsurv_ann_smoteenn_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smotetomek_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_ann_smotetomek_2_hazard.pkl.\n",
      "Loaded model deepsurv_ann_smotetomek_2\n",
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_clustering_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_clustering_1_hazard.pkl.\n",
      "Loaded model deepsurv_lstm_clustering_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_nearmiss_1.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_nearmiss_1_hazard.pkl.\n",
      "Loaded model deepsurv_lstm_nearmiss_1\n",
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_clustering_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_clustering_2_hazard.pkl.\n",
      "Loaded model deepsurv_lstm_clustering_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_nearmiss_2.pt and /mnt/d/PYDataScience/g3_regress/code/models/deepsurv_lstm_nearmiss_2_hazard.pkl.\n",
      "Loaded model deepsurv_lstm_nearmiss_2\n",
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deephit_ann_clustering_all.pt and /mnt/d/PYDataScience/g3_regress/code/models/deephit_ann_clustering_all_hazard.pkl.\n",
      "Loaded model deephit_ann_clustering_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deephit_ann_nearmiss2_all.pt and /mnt/d/PYDataScience/g3_regress/code/models/deephit_ann_nearmiss2_all_hazard.pkl.\n",
      "Loaded model deephit_ann_nearmiss2_all\n",
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_clustering_all.pt and /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_clustering_all_hazard.pkl.\n",
      "Loaded model deephit_lstm_clustering_all\n",
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_nearmiss1_all.pt and /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_nearmiss1_all_hazard.pkl.\n",
      "Loaded model deephit_lstm_nearmiss1_all\n",
      "Model and baseline hazards loaded from /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_nearmiss1_all.pt and /mnt/d/PYDataScience/g3_regress/code/models/deephit_lstm_nearmiss1_all_hazard.pkl.\n",
      "Loaded model deephit_lstm_nearmiss1_all\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store loaded models\n",
    "loaded_models = {}\n",
    "\n",
    "for model_name in model_ls:\n",
    "    # Retrieve configuration by dynamically constructing the variable name\n",
    "    config_var_name = model_name + \"_config\"\n",
    "    model_config = globals().get(config_var_name)\n",
    "    \n",
    "    if model_config is None:\n",
    "        print(f\"Configuration for {config_var_name} not found.\")\n",
    "        continue\n",
    "\n",
    "    model_weights_path = f'{model_path}{model_name}.pt'\n",
    "    model_hazard_path = f'{model_path}{model_name}_hazard.pkl'\n",
    "    \n",
    "    # Define the model creation function as a lambda to pass the config\n",
    "    create_model_func = lambda: create_neural_network(\n",
    "        config=model_config,\n",
    "        num_risk=len(X_train_transformed[EVENT_COL].unique()) - 1,\n",
    "        num_time_bins=len(TIME_GRID)\n",
    "    )\n",
    "    model = create_model_func()\n",
    "    \n",
    "    # Load the model and store it in the dictionary\n",
    "    loaded_models[model_name] = load_model(model, model_config, model_weights_path, model_hazard_path)\n",
    "    print(f'Loaded model {model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prepare and make prediction on the training set, combine the prediction arrays for training of super learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_neural_network(model, config, X_test, duration_col, event_col, cluster_col, time_grid=None):\n",
    "    \"\"\"\n",
    "    Function to train a given neural network using the provided datasets.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): Neural network to be trained.\n",
    "        config (dict): Configuration dictionary containing model hyperparameters.\n",
    "        X_train (pd.DataFrame): Training dataset with features.\n",
    "        X_val (pd.DataFrame): Validation dataset with features.\n",
    "        duration_col (str): Column representing event durations.\n",
    "        event_col (str): Column representing event occurrences.\n",
    "        cluster_col (str): Column for grouping during cross-validation.\n",
    "        callbacks (list): List of callbacks for training.\n",
    "        time_grid (np.array, optional): Time grid for evaluation if required. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained PyCox model.\n",
    "        logs: Training logs.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Train the model\n",
    "    if config['model'] == 'deepsurv':\n",
    "        print('Initiate testing of deepsurv neural network')\n",
    "        X_test = df_event_focus(X_test, event_col, config['endpoint'])\n",
    "        if config['net'] == 'ann':\n",
    "            print('model structure: ANN')\n",
    "            X_test_processed, y_test = preprocess_data(X_test, config['features'], duration_col, event_col)\n",
    "            surv = model.predict_surv_df(X_test_processed, batch_size=512)\n",
    "        elif config['net'] == 'lstm':\n",
    "            print('model structure: LSTM')\n",
    "            X_test_processed, y_test = prepare_validation_data(X_test, config['features'], duration_col, event_col, config, cluster_col, config['model'], time_grid)\n",
    "            X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
    "            y_test_tensor = (torch.tensor(y_test[0], dtype=torch.float32), torch.tensor(y_test[1], dtype=torch.float32))\n",
    "            surv = model.predict_surv_df(X_test_tensor, batch_size=512)\n",
    "    elif config['model'] == 'deephit':\n",
    "        print('Initiate testing of deephit neural network')\n",
    "        if config['net'] == 'ann':\n",
    "            print('model structure: ANN')\n",
    "            X_test_processed, y_test = preprocess_data(X_test, config['features'], duration_col, event_col, time_grid, discretize=True)\n",
    "            surv = model.predict_cif(X_test_processed, batch_size=512)\n",
    "            print('prediction complete, please note that prediction of deephit models are CIF.')\n",
    "        elif config['net'] == 'lstm':\n",
    "            print('model structure: LSTM')\n",
    "            X_test_processed, y_test = prepare_validation_data(X_test, config['features'], duration_col, event_col, config, cluster_col, config['model'], time_grid)\n",
    "            surv = model.predict_cif(X_test_processed, batch_size=512)\n",
    "            print('prediction complete, please note that prediction of deephit models are CIF.')\n",
    "\n",
    "    # Free memory after training\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return surv, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_to_time_grid(surv, time_grid):\n",
    "    \"\"\"\n",
    "    Align the survival DataFrame to the closest indices of the time grid.\n",
    "\n",
    "    Parameters:\n",
    "        surv (pd.DataFrame): Survival probabilities DataFrame.\n",
    "        time_grid (np.array): Array of target time points to align.\n",
    "\n",
    "    Returns:\n",
    "        aligned_surv (pd.DataFrame): Aligned survival probabilities.\n",
    "    \"\"\"\n",
    "    # Convert the DataFrame's index to a NumPy array for fast computation\n",
    "    surv_times = np.array(surv.index)\n",
    "    \n",
    "    # Find the closest time in the survival DataFrame for each time in the grid\n",
    "    closest_indices = [np.argmin(np.abs(surv_times - t)) for t in time_grid]\n",
    "    \n",
    "    # Extract the rows corresponding to the closest times\n",
    "    aligned_surv = surv.iloc[closest_indices].copy()\n",
    "    \n",
    "    # Reindex the DataFrame to match the time grid\n",
    "    aligned_surv.index = range(len(time_grid))  # Standardize indices to 0, 1, 2, ...\n",
    "    return aligned_surv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Use lifelines and CoxPH Fitter to get the CIF of both outcomes as the 'ground truth' of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.359. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 3 total observations, 2 right-censored observations>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "from lifelines import AalenJohansenFitter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Convert all non-target events to 0 (censored)\n",
    "X_train_transformed[\"event1\"] = X_train_transformed[EVENT_COL].apply(lambda x: 1 if x == 1 else 0)\n",
    "X_train_transformed[\"event2\"] = X_train_transformed[EVENT_COL].apply(lambda x: 1 if x == 2 else 0)\n",
    "\n",
    "class_counts = X_train_transformed[EVENT_COL].value_counts()\n",
    "X_train_transformed['weights'] = X_train_transformed[EVENT_COL].map(lambda e: 1 / class_counts[e]).values\n",
    "\n",
    "# Step 1: Fit a Cox model for each event type\n",
    "cox_model_event_1 = CoxPHFitter()\n",
    "cox_model_event_1.fit(X_train_transformed[FEATURE_COLS + [DURATION_COL, 'event1', CLUSTER_COL, 'weights']], duration_col=DURATION_COL, event_col=\"event1\", cluster_col=CLUSTER_COL, weights_col=\"weights\", robust=True)\n",
    "\n",
    "cox_model_event_2 = CoxPHFitter()\n",
    "cox_model_event_2.fit(X_train_transformed[FEATURE_COLS + [DURATION_COL, 'event2', CLUSTER_COL, 'weights']], duration_col=DURATION_COL, event_col=\"event2\", cluster_col=CLUSTER_COL, weights_col=\"weights\", robust=True)\n",
    "\n",
    "# Step 2: Predict individual cumulative hazards for each event type\n",
    "cumulative_hazard_event_1 = cox_model_event_1.predict_cumulative_hazard(X_train_transformed)\n",
    "cumulative_hazard_event_2 = cox_model_event_2.predict_cumulative_hazard(X_train_transformed)\n",
    "\n",
    "# Step 3: Compute overall survival for each individual\n",
    "# Overall survival: S(t) = exp(- (H1(t) + H2(t)))\n",
    "overall_survival = np.exp(-(cumulative_hazard_event_1 + cumulative_hazard_event_2))\n",
    "\n",
    "# Step 4: Calculate CIF for each event type\n",
    "# CIF_k(t) =  h_k(u) * S(u) du (approximated as cumulative sum)\n",
    "cif_event_1 = (cumulative_hazard_event_1 * overall_survival).cumsum(axis=0)\n",
    "cif_event_2 = (cumulative_hazard_event_2 * overall_survival).cumsum(axis=0)\n",
    "\n",
    "# Step 5: Format and display the CIF predictions\n",
    "cif_event_1_normalized = cif_event_1.div(cif_event_1.iloc[-1].max(), axis=1)\n",
    "cif_event_2_normalized = cif_event_2.div(cif_event_2.iloc[-1].max(), axis=1)\n",
    "\n",
    "# Compute the CIF ground truth\n",
    "cif_ground_truth = np.zeros((2,6,396424))\n",
    "cif_ground_truth[0] = align_to_time_grid(cif_event_1_normalized, TIME_GRID).values\n",
    "cif_ground_truth[1] = align_to_time_grid(cif_event_2_normalized, TIME_GRID).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.9818982092210652, 'Event_2': 0.7033707118427875}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0       0.000000\n",
       " 365     0.038203\n",
       " 730     0.038203\n",
       " 1095    0.038203\n",
       " 1460    0.038203\n",
       " 1825    0.038203\n",
       " Name: brier_score, dtype: float64,\n",
       " 'Event_2': 0       0.000000\n",
       " 365     0.085103\n",
       " 730     0.085103\n",
       " 1095    0.085103\n",
       " 1460    0.085103\n",
       " 1825    0.085103\n",
       " Name: brier_score, dtype: float64}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.03565589439044665, 'Event_2': 0.07942951892202983}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.17732477425591778, 'Event_2': 0.27434970836708117}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, y_ground_truth = preprocess_data(X_train_transformed, FEATURE_COLS, DURATION_COL, EVENT_COL, TIME_GRID, discretize=True)\n",
    "concordance_indices = {}\n",
    "integrated_brier_scores = {}\n",
    "neg_log_likelihoods = {}\n",
    "brier_series = {}\n",
    "for i in range(0, 2):\n",
    "    event_interest = i + 1\n",
    "    cif = pd.DataFrame(cif_ground_truth[i], [0, 1, 2, 3, 4, 5])\n",
    "    ev = EvalSurv(1-cif, y_ground_truth[0], y_ground_truth[1] == event_interest, censor_surv='km')\n",
    "    concordance_indices[f\"Event_{event_interest}\"] = ev.concordance_td()\n",
    "    brier_series[f\"Event_{event_interest}\"] = ev.brier_score(TIME_GRID)\n",
    "    integrated_brier_scores[f\"Event_{event_interest}\"] = ev.integrated_brier_score(TIME_GRID)\n",
    "    neg_log_likelihoods[f\"Event_{event_interest}\"] = ev.integrated_nbll(TIME_GRID)\n",
    "\n",
    "    # # Nam and D'Agostino Chi2 statistic for calibration\n",
    "    # for time in time_grid:\n",
    "    #     chi2_stat, p_value, observed_events, expected_events, n, prob_df = nam_dagostino_chi2(\n",
    "    #         df=df_test, \n",
    "    #         duration_col=duration_col, \n",
    "    #         event_col=event_col,\n",
    "    #         surv=(1-cif), \n",
    "    #         time=time, \n",
    "    #         event_focus=event_interest\n",
    "    #     )\n",
    "    #     nam_dagostino_results.append({\n",
    "    #         'Event': event_interest,\n",
    "    #         'Year': round(time / 365),\n",
    "    #         'Chi2_Stat': chi2_stat,\n",
    "    #         'P_Value': p_value,\n",
    "    #         'Observed_Events': observed_events.tolist(),\n",
    "    #         'Expected_Events': expected_events.tolist(),\n",
    "    #         'Sample_Size': n.tolist()\n",
    "    #     })\n",
    "display(concordance_indices)\n",
    "display(brier_series)\n",
    "display(integrated_brier_scores)\n",
    "display(neg_log_likelihoods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Get the prediction for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:46:02,084 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:46:02,731 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:46:39,846 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:47:00,502 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:47:01,105 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:47:02,518 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:47:04,464 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:47:04,469 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 1.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:47:18,564 - INFO - Validation data retrieved\n",
      "2024-11-16 17:47:19,230 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:47:19,231 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:47:30,069 - INFO - Validation data retrieved\n",
      "2024-11-16 17:47:30,668 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:47:30,669 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:47:40,669 - INFO - Validation data retrieved\n",
      "2024-11-16 17:47:41,221 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:47:41,222 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 1.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:47:56,588 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 1.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 1.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 1.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:48:10,963 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 1.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:48:24,909 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 1.\n",
      "Processing fold 2...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:48:25,333 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:48:25,949 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:48:36,146 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:48:44,552 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:48:45,195 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:48:46,661 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:48:48,485 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:48:48,486 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 2.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:49:02,091 - INFO - Validation data retrieved\n",
      "2024-11-16 17:49:02,675 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:49:02,677 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:49:12,471 - INFO - Validation data retrieved\n",
      "2024-11-16 17:49:13,047 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:49:13,048 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:49:22,714 - INFO - Validation data retrieved\n",
      "2024-11-16 17:49:23,338 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:49:23,339 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 2.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:49:37,609 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 2.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 2.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 2.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:49:50,922 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 2.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:00,640 - INFO - Validation data retrieved\n",
      "2024-11-16 17:50:01,006 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 2.\n",
      "Processing fold 3...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:01,632 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:13,955 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:22,137 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:22,788 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:24,218 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:25,979 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:50:25,986 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 3.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:36,077 - INFO - Validation data retrieved\n",
      "2024-11-16 17:50:36,686 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:50:36,687 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:50,176 - INFO - Validation data retrieved\n",
      "2024-11-16 17:50:50,766 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:50:50,767 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:51:00,605 - INFO - Validation data retrieved\n",
      "2024-11-16 17:51:01,169 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:51:01,170 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 3.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:51:11,815 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 3.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 3.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 3.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:51:28,952 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 3.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:51:38,627 - INFO - Validation data retrieved\n",
      "2024-11-16 17:51:39,018 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 3.\n",
      "Processing fold 4...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:51:39,621 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:51:48,104 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:52:03,013 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:52:03,618 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:52:05,057 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:52:06,879 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:52:06,880 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 4.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:52:17,252 - INFO - Validation data retrieved\n",
      "2024-11-16 17:52:17,766 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:52:17,767 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:52:28,089 - INFO - Validation data retrieved\n",
      "2024-11-16 17:52:28,777 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:52:28,778 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:52:42,607 - INFO - Validation data retrieved\n",
      "2024-11-16 17:52:43,146 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:52:43,148 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 4.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:52:54,588 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 4.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 4.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 4.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:12,367 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 4.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:22,664 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 4.\n",
      "Processing fold 5...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:23,064 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:23,719 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:31,984 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:40,317 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:40,961 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:42,452 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:47,992 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:53:47,994 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 5.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:53:57,991 - INFO - Validation data retrieved\n",
      "2024-11-16 17:53:58,636 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:53:58,637 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:54:08,647 - INFO - Validation data retrieved\n",
      "2024-11-16 17:54:09,254 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:54:09,256 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:54:19,030 - INFO - Validation data retrieved\n",
      "2024-11-16 17:54:19,581 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:54:19,582 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 5.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:54:34,464 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 5.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 5.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 5.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:54:48,357 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 5.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:02,064 - INFO - Validation data retrieved\n",
      "2024-11-16 17:55:02,426 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 5.\n",
      "Processing fold 6...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:03,016 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:11,117 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:18,768 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:19,390 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:20,920 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:22,641 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:55:22,642 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 6.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:36,117 - INFO - Validation data retrieved\n",
      "2024-11-16 17:55:36,746 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:55:36,747 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:46,521 - INFO - Validation data retrieved\n",
      "2024-11-16 17:55:47,061 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:55:47,062 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:55:56,501 - INFO - Validation data retrieved\n",
      "2024-11-16 17:55:57,026 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:55:57,027 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 6.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:56:08,162 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 6.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 6.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 6.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:56:25,105 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 6.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:56:34,708 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 6.\n",
      "Processing fold 7...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:56:35,084 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:56:35,702 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:56:44,058 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:56:57,918 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:56:58,545 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:57:00,272 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:57:02,118 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:57:02,119 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 7.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:57:12,718 - INFO - Validation data retrieved\n",
      "2024-11-16 17:57:13,361 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:57:13,362 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:57:27,345 - INFO - Validation data retrieved\n",
      "2024-11-16 17:57:27,924 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:57:27,926 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:57:38,190 - INFO - Validation data retrieved\n",
      "2024-11-16 17:57:38,788 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:57:38,789 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 7.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:57:50,606 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 7.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 7.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 7.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:58:08,729 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 7.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:58:19,278 - INFO - Validation data retrieved\n",
      "2024-11-16 17:58:19,707 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 7.\n",
      "Processing fold 8...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:58:20,323 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:58:28,339 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:58:39,859 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:58:40,447 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:58:41,925 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:58:43,702 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:58:43,703 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 8.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:58:53,523 - INFO - Validation data retrieved\n",
      "2024-11-16 17:58:54,178 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 17:58:54,179 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:59:03,912 - INFO - Validation data retrieved\n",
      "2024-11-16 17:59:04,426 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:59:04,427 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:59:17,923 - INFO - Validation data retrieved\n",
      "2024-11-16 17:59:18,458 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 17:59:18,459 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 8.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:59:29,346 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 8.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 8.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 8.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:59:42,942 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 8.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:59:56,272 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 8.\n",
      "Processing fold 9...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:59:56,620 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:59:57,189 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:00:05,006 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:00:12,763 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:00:13,323 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:00:14,705 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:00:16,566 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 18:00:16,567 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 9.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:00:30,281 - INFO - Validation data retrieved\n",
      "2024-11-16 18:00:30,867 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 18:00:30,868 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:00:40,773 - INFO - Validation data retrieved\n",
      "2024-11-16 18:00:41,311 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 18:00:41,312 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:00:51,213 - INFO - Validation data retrieved\n",
      "2024-11-16 18:00:51,736 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 18:00:51,737 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 9.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:01:06,101 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 9.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 9.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 9.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:01:19,804 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 9.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:01:29,709 - INFO - Validation data retrieved\n",
      "2024-11-16 18:01:30,125 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 9.\n",
      "Processing fold 10...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:01:30,719 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:01:42,441 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:01:50,452 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:01:51,038 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:01:52,542 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:01:54,395 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 18:01:54,396 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 10.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:02:04,127 - INFO - Validation data retrieved\n",
      "2024-11-16 18:02:04,742 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-16 18:02:04,743 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:02:18,385 - INFO - Validation data retrieved\n",
      "2024-11-16 18:02:18,996 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 18:02:18,997 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:02:28,895 - INFO - Validation data retrieved\n",
      "2024-11-16 18:02:29,440 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-16 18:02:29,441 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 10.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:02:40,509 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 10.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 10.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 10.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:02:57,629 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 10.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 18:03:07,443 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 10.\n",
      "Final predictions and y_test combined.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize dictionary to store combined predictions and y_test\n",
    "combined_predictions = []\n",
    "combined_y_test = []\n",
    "cif_ground_truth_test = []\n",
    "\n",
    "# Get unique keys and split them into 10 groups\n",
    "unique_keys = X_train_transformed_2['key'].unique()\n",
    "np.random.shuffle(unique_keys)  # Shuffle keys to ensure randomness\n",
    "key_folds = np.array_split(unique_keys, 10)\n",
    "\n",
    "for fold_idx, test_keys in enumerate(key_folds):\n",
    "    print(f\"Processing fold {fold_idx + 1}...\")\n",
    "    # Split the dataset into train and test based on keys\n",
    "    X_test_fold = X_train_transformed_2[X_train_transformed_2['key'].isin(test_keys)]\n",
    "    \n",
    "    # Get the indices of X_test_fold relative to the original dataset\n",
    "    test_indices = X_test_fold.index.to_numpy()\n",
    "\n",
    "    # Extract CIF ground truth for these indices\n",
    "    fold_cif_ground_truth = cif_ground_truth[:, :, test_indices]\n",
    "\n",
    "    # Stack this fold's CIF ground truth\n",
    "    cif_ground_truth_test.append(fold_cif_ground_truth)\n",
    "    \n",
    "    # Store predictions for this fold\n",
    "    fold_predictions = {}\n",
    "    \n",
    "    X_test, y_test = preprocess_data(X_test_fold, FEATURE_COLS, DURATION_COL, EVENT_COL, TIME_GRID, discretize=True)\n",
    "    combined_y_test.append(y_test)\n",
    "    \n",
    "    for model_name in model_ls:\n",
    "        # Retrieve configuration by dynamically constructing the variable name\n",
    "        config_var_name = model_name + \"_config\"\n",
    "        model_config = globals().get(config_var_name)\n",
    "        if model_config is None:\n",
    "            print(f\"Configuration for {config_var_name} not found.\")\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"Initiating prediction for model: {model_name}\")\n",
    "            \n",
    "            # Retrieve the loaded model\n",
    "            model = loaded_models.get(model_name)\n",
    "            if model is None:\n",
    "                print(f\"Model {model_name} is not loaded.\")\n",
    "                continue\n",
    "            \n",
    "            # Predict using the loaded model and configuration\n",
    "            surv, _ = predict_neural_network(\n",
    "                model=model,\n",
    "                config=model_config,\n",
    "                X_test=X_test_fold,\n",
    "                duration_col=DURATION_COL,\n",
    "                event_col=EVENT_COL,\n",
    "                cluster_col=CLUSTER_COL,\n",
    "                time_grid=TIME_GRID\n",
    "            )\n",
    "            \n",
    "            # Align survival probabilities (if DeepSurv)\n",
    "            if model_config['model'] == 'deepsurv':\n",
    "                surv = align_to_time_grid(surv, TIME_GRID).values  # 2D array\n",
    "                \n",
    "                # Structure key dynamically\n",
    "                key = f\"deepsurv_{model_config['net']}_{model_config['balance_method']}\"\n",
    "                \n",
    "                # Store predictions\n",
    "                if key not in fold_predictions:\n",
    "                    fold_predictions[key] = []\n",
    "                fold_predictions[key].append(np.expand_dims(1 - surv, axis=0))\n",
    "            \n",
    "            # Handle DeepHit predictions\n",
    "            elif model_config['model'] == 'deephit':\n",
    "                surv = np.array(surv)  # Convert to numpy array\n",
    "                \n",
    "                # Structure key dynamically\n",
    "                key = f\"deephit_{model_config['net']}_{model_config['balance_method']}\"\n",
    "                \n",
    "                # Store predictions\n",
    "                if key not in fold_predictions:\n",
    "                    fold_predictions[key] = []\n",
    "                fold_predictions[key].append(surv)\n",
    "            \n",
    "            print(f\"Prediction completed for {model_name} on fold {fold_idx + 1}.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction for {model_name} on fold {fold_idx + 1}: {e}\")\n",
    "            \n",
    "    combined_predictions.append(fold_predictions)\n",
    "\n",
    "meta_learner_X_train = {}\n",
    "\n",
    "for fold_predictions in combined_predictions:\n",
    "    for model_key, fold_data in fold_predictions.items():\n",
    "        if model_key not in meta_learner_X_train:\n",
    "            meta_learner_X_train[model_key] = []\n",
    "        \n",
    "        # Align DeepSurv predictions to match DeepHit format\n",
    "        if \"deepsurv\" in model_key:\n",
    "            # Stack competing outcomes and remove the redundant dimension\n",
    "            meta_learner_X_train[model_key].extend(\n",
    "                [np.squeeze(np.stack(fold_data, axis=0), axis=1)]  # Squeeze out extra axis\n",
    "            )\n",
    "        else:\n",
    "            # Keep DeepHit predictions as-is\n",
    "            meta_learner_X_train[model_key].extend(fold_data)\n",
    "\n",
    "# Combine y_test\n",
    "meta_learner_y_train = (\n",
    "    np.concatenate([fold[0] for fold in combined_y_test]),  # Concatenate all first elements\n",
    "    np.concatenate([fold[1] for fold in combined_y_test])   # Concatenate all second elements\n",
    ")\n",
    "\n",
    "# Combine CIF ground truth for all folds\n",
    "cif_ground_truth_test_stacked = np.concatenate(cif_ground_truth_test, axis=2)\n",
    "\n",
    "print(\"Final predictions and y_test combined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsurv_ann_clustering: final shape (2, 6, 316242)\n",
      "deepsurv_ann_enn: final shape (2, 6, 316242)\n",
      "deepsurv_ann_tomek: final shape (2, 6, 316242)\n",
      "deepsurv_lstm_clustering: final shape (2, 6, 316242)\n",
      "deepsurv_lstm_NearMiss: final shape (2, 6, 316242)\n",
      "deephit_ann_clustering: final shape (2, 6, 316242)\n",
      "deephit_ann_NearMiss: final shape (2, 6, 316242)\n",
      "deephit_lstm_clustering: final shape (2, 6, 316242)\n",
      "deephit_lstm_NearMiss: final shape (2, 6, 316242)\n"
     ]
    }
   ],
   "source": [
    "# Combine all 10 items for each model's predictions\n",
    "final_meta_learner_X_train = {}\n",
    "\n",
    "for key, predictions in meta_learner_X_train.items():\n",
    "    # Concatenate the predictions along the last axis\n",
    "    final_meta_learner_X_train[key] = np.concatenate(predictions, axis=2)  # Combine along the feature axis\n",
    "\n",
    "# Validate the shapes\n",
    "for key, combined_prediction in final_meta_learner_X_train.items():\n",
    "    print(f\"{key}: final shape {combined_prediction.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 316242)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316242,)\n",
      "(316242,)\n",
      "(array([0, 1, 2]), array([310074,   1509,   4659]))\n"
     ]
    }
   ],
   "source": [
    "display(np.shape(cif_ground_truth_test_stacked))\n",
    "\n",
    "# Use the duration and event data as targets\n",
    "durations = meta_learner_y_train[0]  # Time to event or censoring\n",
    "events = meta_learner_y_train[1]  # Event type (competing risks)\n",
    "\n",
    "print(durations.shape)\n",
    "print(events.shape)\n",
    "print(np.unique(events, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Prepare each model's prediction on validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:18,328 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:18,790 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:21,687 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:27,567 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:27,998 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:28,662 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 1.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:29,458 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:36:29,459 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 1.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:31,992 - INFO - Validation data retrieved\n",
      "2024-11-18 02:36:32,387 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:36:32,388 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:34,942 - INFO - Validation data retrieved\n",
      "2024-11-18 02:36:35,372 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:36:35,373 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 1.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:37,833 - INFO - Validation data retrieved\n",
      "2024-11-18 02:36:38,245 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:36:38,246 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 1.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:40,799 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 1.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 1.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 1.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:44,934 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 1.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:47,620 - INFO - Validation data retrieved\n",
      "2024-11-18 02:36:47,961 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 1.\n",
      "Processing fold 2...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:48,371 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:51,012 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:53,399 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:53,842 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:54,512 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 2.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:55,291 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:36:55,292 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 2.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:36:57,667 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:01,743 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:37:01,744 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:04,139 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:04,528 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:37:04,529 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 2.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:06,958 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:07,360 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:37:07,360 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 2.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:09,810 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 2.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 2.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 2.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:13,634 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 2.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:16,269 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:16,617 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 2.\n",
      "Processing fold 3...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:17,034 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:19,607 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:21,916 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:22,363 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:23,101 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 3.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:23,872 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:37:23,873 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 3.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:26,267 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:26,706 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:37:26,707 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:29,071 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:29,497 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:37:29,498 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 3.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:31,982 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:32,389 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:37:32,390 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 3.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:38,556 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 3.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 3.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 3.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:42,391 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 3.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:45,029 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:45,407 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 3.\n",
      "Processing fold 4...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:45,825 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:48,101 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:50,258 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:50,687 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:51,304 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 4.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:52,010 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:37:52,011 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 4.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:54,407 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:54,851 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:37:54,852 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:57,178 - INFO - Validation data retrieved\n",
      "2024-11-18 02:37:57,578 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:37:57,579 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 4.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:37:59,919 - INFO - Validation data retrieved\n",
      "2024-11-18 02:38:00,305 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:38:00,306 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 4.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:02,687 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 4.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 4.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 4.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:06,488 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 4.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:09,140 - INFO - Validation data retrieved\n",
      "2024-11-18 02:38:09,516 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 4.\n",
      "Processing fold 5...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:09,951 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:15,961 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:18,181 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:18,603 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:19,256 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 5.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:19,947 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:38:19,948 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 5.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:22,394 - INFO - Validation data retrieved\n",
      "2024-11-18 02:38:22,793 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:38:22,794 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:25,199 - INFO - Validation data retrieved\n",
      "2024-11-18 02:38:25,574 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:38:25,575 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 5.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:28,038 - INFO - Validation data retrieved\n",
      "2024-11-18 02:38:28,440 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:38:28,441 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 5.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:30,908 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 5.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 5.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 5.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:34,838 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 5.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:37,624 - INFO - Validation data retrieved\n",
      "2024-11-18 02:38:37,952 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 5.\n",
      "Processing fold 6...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:38,424 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:40,902 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:43,296 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:43,720 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:44,363 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 6.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:45,109 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:38:45,109 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 6.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:51,255 - INFO - Validation data retrieved\n",
      "2024-11-18 02:38:51,666 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:38:51,666 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:54,223 - INFO - Validation data retrieved\n",
      "2024-11-18 02:38:54,624 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:38:54,624 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 6.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:38:57,151 - INFO - Validation data retrieved\n",
      "2024-11-18 02:38:57,563 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:38:57,563 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 6.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:00,168 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 6.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 6.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 6.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:04,262 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 6.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:07,120 - INFO - Validation data retrieved\n",
      "2024-11-18 02:39:07,481 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 6.\n",
      "Processing fold 7...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:08,025 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:10,413 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:12,658 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:13,067 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:13,688 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 7.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:14,377 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:39:14,378 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 7.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:16,909 - INFO - Validation data retrieved\n",
      "2024-11-18 02:39:17,318 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:39:17,319 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:19,806 - INFO - Validation data retrieved\n",
      "2024-11-18 02:39:20,242 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:39:20,242 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 7.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:26,324 - INFO - Validation data retrieved\n",
      "2024-11-18 02:39:26,699 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:39:26,701 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 7.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:29,274 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 7.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 7.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 7.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:33,254 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 7.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:35,957 - INFO - Validation data retrieved\n",
      "2024-11-18 02:39:36,277 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 7.\n",
      "Processing fold 8...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:36,738 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:39,288 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:41,482 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:41,880 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:42,509 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 8.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:43,179 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:39:43,179 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 8.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:45,504 - INFO - Validation data retrieved\n",
      "2024-11-18 02:39:45,923 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:39:45,924 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:48,201 - INFO - Validation data retrieved\n",
      "2024-11-18 02:39:48,580 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:39:48,581 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 8.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:50,853 - INFO - Validation data retrieved\n",
      "2024-11-18 02:39:51,284 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:39:51,284 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 8.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:53,633 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 8.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 8.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 8.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:39:57,328 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 8.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:03,476 - INFO - Validation data retrieved\n",
      "2024-11-18 02:40:03,794 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 8.\n",
      "Processing fold 9...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:04,227 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:06,538 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:08,682 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:09,087 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:09,710 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 9.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:10,397 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:40:10,397 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 9.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:12,827 - INFO - Validation data retrieved\n",
      "2024-11-18 02:40:13,206 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:40:13,207 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:15,513 - INFO - Validation data retrieved\n",
      "2024-11-18 02:40:15,927 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:40:15,929 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 9.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:18,251 - INFO - Validation data retrieved\n",
      "2024-11-18 02:40:18,635 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:40:18,636 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 9.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:21,004 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 9.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 9.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 9.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:24,794 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 9.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:27,348 - INFO - Validation data retrieved\n",
      "2024-11-18 02:40:27,686 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 9.\n",
      "Processing fold 10...\n",
      "Initiating prediction for model: deepsurv_ann_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:28,151 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:30,765 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:33,115 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:33,550 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_clustering_2 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_smoteenn_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:34,179 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smoteenn_2 on fold 10.\n",
      "Initiating prediction for model: deepsurv_ann_smotetomek_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:38,629 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:40:38,630 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_ann_smotetomek_2 on fold 10.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:40,977 - INFO - Validation data retrieved\n",
      "2024-11-18 02:40:41,422 - INFO - Event column 'endpoint' updated with focus on event value 1.\n",
      "2024-11-18 02:40:41,423 - INFO - Event column 'endpoint' updated with focus on event value 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_1\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:43,838 - INFO - Validation data retrieved\n",
      "2024-11-18 02:40:44,313 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:40:44,315 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_1 on fold 10.\n",
      "Initiating prediction for model: deepsurv_lstm_clustering_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:46,699 - INFO - Validation data retrieved\n",
      "2024-11-18 02:40:47,115 - INFO - Event column 'endpoint' updated with focus on event value 2.\n",
      "2024-11-18 02:40:47,116 - INFO - Event column 'endpoint' updated with focus on event value 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_clustering_2 on fold 10.\n",
      "Initiating prediction for model: deepsurv_lstm_nearmiss_2\n",
      "Initiate testing of deepsurv neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:49,554 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed for deepsurv_lstm_nearmiss_2 on fold 10.\n",
      "Initiating prediction for model: deephit_ann_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_clustering_all on fold 10.\n",
      "Initiating prediction for model: deephit_ann_nearmiss2_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: ANN\n",
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_ann_nearmiss2_all on fold 10.\n",
      "Initiating prediction for model: deephit_lstm_clustering_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:53,374 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_clustering_all on fold 10.\n",
      "Initiating prediction for model: deephit_lstm_nearmiss1_all\n",
      "Initiate testing of deephit neural network\n",
      "model structure: LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:40:55,953 - INFO - Validation data retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction complete, please note that prediction of deephit models are CIF.\n",
      "Prediction completed for deephit_lstm_nearmiss1_all on fold 10.\n",
      "Final predictions and y_test combined.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize dictionary to store combined predictions and y_test\n",
    "combined_predictions = []\n",
    "combined_y_val = []\n",
    "cif_ground_truth_val = []\n",
    "\n",
    "# Get unique keys and split them into 10 groups\n",
    "unique_keys = X_fin_val['key'].unique()\n",
    "np.random.shuffle(unique_keys)  # Shuffle keys to ensure randomness\n",
    "key_folds = np.array_split(unique_keys, 10)\n",
    "\n",
    "for fold_idx, test_keys in enumerate(key_folds):\n",
    "    print(f\"Processing fold {fold_idx + 1}...\")\n",
    "    # Split the dataset into train and test based on keys\n",
    "    X_val_fold = X_fin_val[X_fin_val['key'].isin(test_keys)]\n",
    "    \n",
    "    # Get the indices of X_test_fold relative to the original dataset\n",
    "    test_indices = X_val_fold.index.to_numpy()\n",
    "\n",
    "    # Extract CIF ground truth for these indices\n",
    "    cif_ground_truth_val_fold = cif_ground_truth[:, :, test_indices]\n",
    "\n",
    "    # Stack this fold's CIF ground truth\n",
    "    cif_ground_truth_val.append(cif_ground_truth_val_fold)\n",
    "    \n",
    "    # Store predictions for this fold\n",
    "    fold_predictions = {}\n",
    "    \n",
    "    X_val, y_val = preprocess_data(X_val_fold, FEATURE_COLS, DURATION_COL, EVENT_COL, TIME_GRID, discretize=True)\n",
    "    combined_y_val.append(y_val)\n",
    "    \n",
    "    for model_name in model_ls:\n",
    "        # Retrieve configuration by dynamically constructing the variable name\n",
    "        config_var_name = model_name + \"_config\"\n",
    "        model_config = globals().get(config_var_name)\n",
    "        if model_config is None:\n",
    "            print(f\"Configuration for {config_var_name} not found.\")\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"Initiating prediction for model: {model_name}\")\n",
    "            \n",
    "            # Retrieve the loaded model\n",
    "            model = loaded_models.get(model_name)\n",
    "            if model is None:\n",
    "                print(f\"Model {model_name} is not loaded.\")\n",
    "                continue\n",
    "            \n",
    "            # Predict using the loaded model and configuration\n",
    "            surv, _ = predict_neural_network(\n",
    "                model=model,\n",
    "                config=model_config,\n",
    "                X_test=X_val_fold,\n",
    "                duration_col=DURATION_COL,\n",
    "                event_col=EVENT_COL,\n",
    "                cluster_col=CLUSTER_COL,\n",
    "                time_grid=TIME_GRID\n",
    "            )\n",
    "            \n",
    "            # Align survival probabilities (if DeepSurv)\n",
    "            if model_config['model'] == 'deepsurv':\n",
    "                surv = align_to_time_grid(surv, TIME_GRID).values  # 2D array\n",
    "                \n",
    "                # Structure key dynamically\n",
    "                key = f\"deepsurv_{model_config['net']}_{model_config['balance_method']}\"\n",
    "                \n",
    "                # Store predictions\n",
    "                if key not in fold_predictions:\n",
    "                    fold_predictions[key] = []\n",
    "                fold_predictions[key].append(np.expand_dims(1 - surv, axis=0))\n",
    "            \n",
    "            # Handle DeepHit predictions\n",
    "            elif model_config['model'] == 'deephit':\n",
    "                surv = np.array(surv)  # Convert to numpy array\n",
    "                \n",
    "                # Structure key dynamically\n",
    "                key = f\"deephit_{model_config['net']}_{model_config['balance_method']}\"\n",
    "                \n",
    "                # Store predictions\n",
    "                if key not in fold_predictions:\n",
    "                    fold_predictions[key] = []\n",
    "                fold_predictions[key].append(surv)\n",
    "            \n",
    "            print(f\"Prediction completed for {model_name} on fold {fold_idx + 1}.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction for {model_name} on fold {fold_idx + 1}: {e}\")\n",
    "            \n",
    "    combined_predictions.append(fold_predictions)\n",
    "\n",
    "meta_learner_X_val = {}\n",
    "\n",
    "for fold_predictions in combined_predictions:\n",
    "    for model_key, fold_data in fold_predictions.items():\n",
    "        if model_key not in meta_learner_X_val:\n",
    "            meta_learner_X_val[model_key] = []\n",
    "        \n",
    "        # Align DeepSurv predictions to match DeepHit format\n",
    "        if \"deepsurv\" in model_key:\n",
    "            # Stack competing outcomes and remove the redundant dimension\n",
    "            meta_learner_X_val[model_key].extend(\n",
    "                [np.squeeze(np.stack(fold_data, axis=0), axis=1)]  # Squeeze out extra axis\n",
    "            )\n",
    "        else:\n",
    "            # Keep DeepHit predictions as-is\n",
    "            meta_learner_X_val[model_key].extend(fold_data)\n",
    "\n",
    "# Combine y_test\n",
    "meta_learner_y_val = (\n",
    "    np.concatenate([fold[0] for fold in combined_y_val]),  # Concatenate all first elements\n",
    "    np.concatenate([fold[1] for fold in combined_y_val])   # Concatenate all second elements\n",
    ")\n",
    "\n",
    "# Combine CIF ground truth for all folds\n",
    "cif_ground_truth_val_stacked = np.concatenate(cif_ground_truth_val, axis=2)\n",
    "\n",
    "print(\"Final predictions and y_test combined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsurv_ann_clustering: final shape (2, 6, 80182)\n",
      "deepsurv_ann_enn: final shape (2, 6, 80182)\n",
      "deepsurv_ann_tomek: final shape (2, 6, 80182)\n",
      "deepsurv_lstm_clustering: final shape (2, 6, 80182)\n",
      "deepsurv_lstm_NearMiss: final shape (2, 6, 80182)\n",
      "deephit_ann_clustering: final shape (2, 6, 80182)\n",
      "deephit_ann_NearMiss: final shape (2, 6, 80182)\n",
      "deephit_lstm_clustering: final shape (2, 6, 80182)\n",
      "deephit_lstm_NearMiss: final shape (2, 6, 80182)\n"
     ]
    }
   ],
   "source": [
    "# Combine all 10 items for each model's predictions\n",
    "final_meta_learner_X_val = {}\n",
    "\n",
    "for key, predictions in meta_learner_X_val.items():\n",
    "    # Concatenate the predictions along the last axis\n",
    "    final_meta_learner_X_val[key] = np.concatenate(predictions, axis=2)  # Combine along the feature axis\n",
    "\n",
    "# Validate the shapes\n",
    "for key, combined_prediction in final_meta_learner_X_val.items():\n",
    "    print(f\"{key}: final shape {combined_prediction.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80182,)\n",
      "(80182,)\n",
      "(array([0, 1, 2]), array([78593,   416,  1173]))\n"
     ]
    }
   ],
   "source": [
    "# Use the duration and event data as targets\n",
    "durations_val = meta_learner_y_val[0]  # Time to event or censoring\n",
    "events_val = meta_learner_y_val[1]  # Event type (competing risks)\n",
    "\n",
    "print(durations_val.shape)\n",
    "print(events_val.shape)\n",
    "print(np.unique(events_val, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Voting Predictions shape: (2, 6, 80182)\n"
     ]
    }
   ],
   "source": [
    "# Predictions from each model\n",
    "predictions = [final_meta_learner_X_val[key] for key in final_meta_learner_X_val.keys()]\n",
    "\n",
    "# Aggregate by majority voting for each time point and event type\n",
    "average_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "print(\"Average Voting Predictions shape:\", average_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.9871318950066342, 'Event_2': 0.8090145647829391}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0       3.694347e-07\n",
       " 365     5.130334e-02\n",
       " 730     5.130334e-02\n",
       " 1095    5.130334e-02\n",
       " 1460    5.130334e-02\n",
       " 1825    5.130334e-02\n",
       " Name: brier_score, dtype: float64,\n",
       " 'Event_2': 0       5.540600e-07\n",
       " 365     2.079270e-01\n",
       " 730     2.079270e-01\n",
       " 1095    2.079270e-01\n",
       " 1460    2.079270e-01\n",
       " 1825    2.079270e-01\n",
       " Name: brier_score, dtype: float64}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.047883145941353206, 'Event_2': 0.19406523364327338}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.23892322617407574, 'Event_2': 0.5680026580581663}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concordance_indices = {}\n",
    "integrated_brier_scores = {}\n",
    "neg_log_likelihoods = {}\n",
    "brier_series = {}\n",
    "for i in range(0, 2):\n",
    "    event_interest = i + 1\n",
    "    cif = pd.DataFrame(average_predictions[i], [0, 1, 2, 3, 4, 5])\n",
    "    ev = EvalSurv(1-cif, durations_val, events_val == event_interest, censor_surv='km')\n",
    "    concordance_indices[f\"Event_{event_interest}\"] = ev.concordance_td()\n",
    "    brier_series[f\"Event_{event_interest}\"] = ev.brier_score(TIME_GRID)\n",
    "    integrated_brier_scores[f\"Event_{event_interest}\"] = ev.integrated_brier_score(TIME_GRID)\n",
    "    neg_log_likelihoods[f\"Event_{event_interest}\"] = ev.integrated_nbll(TIME_GRID)\n",
    "\n",
    "    # # Nam and D'Agostino Chi2 statistic for calibration\n",
    "    # for time in time_grid:\n",
    "    #     chi2_stat, p_value, observed_events, expected_events, n, prob_df = nam_dagostino_chi2(\n",
    "    #         df=df_test, \n",
    "    #         duration_col=duration_col, \n",
    "    #         event_col=event_col,\n",
    "    #         surv=(1-cif), \n",
    "    #         time=time, \n",
    "    #         event_focus=event_interest\n",
    "    #     )\n",
    "    #     nam_dagostino_results.append({\n",
    "    #         'Event': event_interest,\n",
    "    #         'Year': round(time / 365),\n",
    "    #         'Chi2_Stat': chi2_stat,\n",
    "    #         'P_Value': p_value,\n",
    "    #         'Observed_Events': observed_events.tolist(),\n",
    "    #         'Expected_Events': expected_events.tolist(),\n",
    "    #         'Sample_Size': n.tolist()\n",
    "    #     })\n",
    "display(concordance_indices)\n",
    "display(brier_series)\n",
    "display(integrated_brier_scores)\n",
    "display(neg_log_likelihoods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:27] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/goma/miniconda3/envs/ai_dev/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:24:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Stacking Predictions Shape: (2, 6, 316242)\n"
     ]
    }
   ],
   "source": [
    "def train_and_predict_with_xgboost(outcome_idx, time_idx):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model using CIF targets for a specific outcome and time point.\n",
    "    \"\"\"\n",
    "    stacking_inputs = np.array([\n",
    "        final_meta_learner_X_train[key][outcome_idx, time_idx]\n",
    "        for key in final_meta_learner_X_train.keys()\n",
    "    ]).T  # Shape: (316242, 9)\n",
    "    \n",
    "    target = cif_ground_truth_test_stacked[outcome_idx, time_idx, :]  # Correct indexing\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(stacking_inputs, target, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "    # Prepare DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # Configure GPU training\n",
    "    # params = {\n",
    "    #     \"objective\": \"reg:squarederror\",\n",
    "    #     \"tree_method\": \"gpu_hist\",\n",
    "    #     \"max_depth\": 6,\n",
    "    #     \"learning_rate\": 0.1,\n",
    "    #     \"subsample\": 0.8,\n",
    "    #     \"colsample_bytree\": 0.8,\n",
    "    # }\n",
    "    params = {'objective': 'reg:squarederror',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'max_depth': 3,\n",
    "        'learning_rate': 0.014837295326564928,\n",
    "        'subsample': 0.8168005866659258,\n",
    "        'colsample_bytree': 0.8290129403377126,\n",
    "        'lambda': 6.7539372305286465,\n",
    "        'alpha': 0.5018353832953043}\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    booster = xgb.train(\n",
    "        params, dtrain, num_boost_round=500,\n",
    "        evals=[(dtrain, \"train\"), (dval, \"validation\")],\n",
    "        early_stopping_rounds=20,  # Stop if no improvement for 20 rounds\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Predict CIF for each patient\n",
    "    dtest = xgb.DMatrix(stacking_inputs)\n",
    "    cif_predictions = booster.predict(dtest)\n",
    "\n",
    "    return outcome_idx, time_idx, cif_predictions\n",
    "\n",
    "# Parallel processing for each outcome and time point\n",
    "results = Parallel(n_jobs=-1)(delayed(train_and_predict_with_xgboost)(outcome_idx, time_idx)\n",
    "                                for outcome_idx in range(2)\n",
    "                                for time_idx in range(6))\n",
    "\n",
    "# Assign results to the xgboost_predictions array\n",
    "for outcome_idx, time_idx, cif_predictions in results:\n",
    "    xgboost_predictions[outcome_idx, time_idx] = cif_predictions\n",
    "\n",
    "# Verify the shape of the stacked predictions\n",
    "print(\"XGBoost Stacking Predictions Shape:\", xgboost_predictions.shape)\n",
    "# Expected output: (2, 6, 316242)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.9807516215018126, 'Event_2': 0.7421550277486034}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0    0.000000\n",
       " 1    0.001827\n",
       " 2    0.005977\n",
       " 3    0.017441\n",
       " 4    0.121360\n",
       " 5    0.048115\n",
       " Name: brier_score, dtype: float64,\n",
       " 'Event_2': 0    0.000000\n",
       " 1    0.004290\n",
       " 2    0.022084\n",
       " 3    0.086680\n",
       " 4    0.280068\n",
       " 5    0.083950\n",
       " Name: brier_score, dtype: float64}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.03392605440147253, 'Event_2': 0.08876807544130376}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.12048712218642872, 'Event_2': 0.2885305607829278}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concordance_indices = {}\n",
    "integrated_brier_scores = {}\n",
    "neg_log_likelihoods = {}\n",
    "brier_series = {}\n",
    "for i in range(0, 2):\n",
    "    event_interest = i + 1\n",
    "    cif = pd.DataFrame(xgboost_predictions[i], [0, 1, 2, 3, 4, 5])\n",
    "    ev = EvalSurv(1-cif, durations, events == event_interest, censor_surv='km')\n",
    "    concordance_indices[f\"Event_{event_interest}\"] = ev.concordance_td()\n",
    "    brier_series[f\"Event_{event_interest}\"] = ev.brier_score(np.array([0, 1, 2, 3, 4, 5]))\n",
    "    integrated_brier_scores[f\"Event_{event_interest}\"] = ev.integrated_brier_score(np.array([0, 1, 2, 3, 4, 5]))\n",
    "    neg_log_likelihoods[f\"Event_{event_interest}\"] = ev.integrated_nbll(np.array([0, 1, 2, 3, 4, 5]))\n",
    "\n",
    "    # # Nam and D'Agostino Chi2 statistic for calibration\n",
    "    # for time in time_grid:\n",
    "    #     chi2_stat, p_value, observed_events, expected_events, n, prob_df = nam_dagostino_chi2(\n",
    "    #         df=df_test, \n",
    "    #         duration_col=duration_col, \n",
    "    #         event_col=event_col,\n",
    "    #         surv=(1-cif), \n",
    "    #         time=time, \n",
    "    #         event_focus=event_interest\n",
    "    #     )\n",
    "    #     nam_dagostino_results.append({\n",
    "    #         'Event': event_interest,\n",
    "    #         'Year': round(time / 365),\n",
    "    #         'Chi2_Stat': chi2_stat,\n",
    "    #         'P_Value': p_value,\n",
    "    #         'Observed_Events': observed_events.tolist(),\n",
    "    #         'Expected_Events': expected_events.tolist(),\n",
    "    #         'Sample_Size': n.tolist()\n",
    "    #     })\n",
    "display(concordance_indices)\n",
    "display(brier_series)\n",
    "display(integrated_brier_scores)\n",
    "display(neg_log_likelihoods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:31:06,984\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-17 20:15:33</td></tr>\n",
       "<tr><td>Running for: </td><td>01:44:26.88        </td></tr>\n",
       "<tr><td>Memory:      </td><td>100.8/117.9 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status    </th><th>loc                    </th><th style=\"text-align: right;\">      alpha</th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">     lambda</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  subsample</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  c-index stat</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00000</td><td>TERMINATED</td><td>192.168.236.234:1003253</td><td style=\"text-align: right;\">0.0141936  </td><td style=\"text-align: right;\">          0.872683</td><td style=\"text-align: right;\">0.00338673 </td><td style=\"text-align: right;\">      0.0666382</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">   0.614585</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         114.835</td><td style=\"text-align: right;\">      0.85425 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00001</td><td>TERMINATED</td><td>192.168.236.234:1003687</td><td style=\"text-align: right;\">0.0063679  </td><td style=\"text-align: right;\">          0.965151</td><td style=\"text-align: right;\">2.80002    </td><td style=\"text-align: right;\">      0.0217801</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.629694</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         130.689</td><td style=\"text-align: right;\">      0.852337</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00002</td><td>TERMINATED</td><td>192.168.236.234:1004147</td><td style=\"text-align: right;\">0.0244766  </td><td style=\"text-align: right;\">          0.600954</td><td style=\"text-align: right;\">0.00310441 </td><td style=\"text-align: right;\">      0.228248 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">   0.803935</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         112.6  </td><td style=\"text-align: right;\">      0.853303</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00003</td><td>TERMINATED</td><td>192.168.236.234:1004568</td><td style=\"text-align: right;\">1.89679    </td><td style=\"text-align: right;\">          0.520151</td><td style=\"text-align: right;\">7.01459    </td><td style=\"text-align: right;\">      0.0431147</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">   0.88264 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         114.363</td><td style=\"text-align: right;\">      0.856744</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00004</td><td>TERMINATED</td><td>192.168.236.234:1004988</td><td style=\"text-align: right;\">0.0766019  </td><td style=\"text-align: right;\">          0.689039</td><td style=\"text-align: right;\">0.0100393  </td><td style=\"text-align: right;\">      0.0121418</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.970722</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         132.607</td><td style=\"text-align: right;\">      0.855037</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00005</td><td>TERMINATED</td><td>192.168.236.234:1005454</td><td style=\"text-align: right;\">0.000204935</td><td style=\"text-align: right;\">          0.638219</td><td style=\"text-align: right;\">0.00213184 </td><td style=\"text-align: right;\">      0.0719649</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.769812</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         126.329</td><td style=\"text-align: right;\">      0.848903</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00006</td><td>TERMINATED</td><td>192.168.236.234:1005910</td><td style=\"text-align: right;\">0.00897389 </td><td style=\"text-align: right;\">          0.700659</td><td style=\"text-align: right;\">6.11243    </td><td style=\"text-align: right;\">      0.0141268</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.515068</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         115.955</td><td style=\"text-align: right;\">      0.858138</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00007</td><td>TERMINATED</td><td>192.168.236.234:1006335</td><td style=\"text-align: right;\">0.261425   </td><td style=\"text-align: right;\">          0.997296</td><td style=\"text-align: right;\">0.0508392  </td><td style=\"text-align: right;\">      0.140841 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.684653</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         125.169</td><td style=\"text-align: right;\">      0.848343</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00008</td><td>TERMINATED</td><td>192.168.236.234:1006794</td><td style=\"text-align: right;\">0.00456015 </td><td style=\"text-align: right;\">          0.958849</td><td style=\"text-align: right;\">0.000103928</td><td style=\"text-align: right;\">      0.0159644</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.863895</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         119.522</td><td style=\"text-align: right;\">      0.857573</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00009</td><td>TERMINATED</td><td>192.168.236.234:1007216</td><td style=\"text-align: right;\">0.00350792 </td><td style=\"text-align: right;\">          0.708339</td><td style=\"text-align: right;\">0.00135998 </td><td style=\"text-align: right;\">      0.0230098</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">   0.719504</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         112.981</td><td style=\"text-align: right;\">      0.859644</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00010</td><td>TERMINATED</td><td>192.168.236.234:1007636</td><td style=\"text-align: right;\">0.189726   </td><td style=\"text-align: right;\">          0.825057</td><td style=\"text-align: right;\">0.000213357</td><td style=\"text-align: right;\">      0.145456 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">   0.565484</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         113.172</td><td style=\"text-align: right;\">      0.854543</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00011</td><td>TERMINATED</td><td>192.168.236.234:1008046</td><td style=\"text-align: right;\">0.103832   </td><td style=\"text-align: right;\">          0.974492</td><td style=\"text-align: right;\">3.86932    </td><td style=\"text-align: right;\">      0.163204 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">   0.921749</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         118.862</td><td style=\"text-align: right;\">      0.848873</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00012</td><td>TERMINATED</td><td>192.168.236.234:1008484</td><td style=\"text-align: right;\">1.64066    </td><td style=\"text-align: right;\">          0.655229</td><td style=\"text-align: right;\">0.772558   </td><td style=\"text-align: right;\">      0.286105 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">   0.665612</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         129.112</td><td style=\"text-align: right;\">      0.849451</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00013</td><td>TERMINATED</td><td>192.168.236.234:1008937</td><td style=\"text-align: right;\">3.5125     </td><td style=\"text-align: right;\">          0.7575  </td><td style=\"text-align: right;\">0.000302093</td><td style=\"text-align: right;\">      0.15233  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">   0.746198</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         114.6  </td><td style=\"text-align: right;\">      0.853892</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00014</td><td>TERMINATED</td><td>192.168.236.234:1009359</td><td style=\"text-align: right;\">0.00130085 </td><td style=\"text-align: right;\">          0.886701</td><td style=\"text-align: right;\">1.83674    </td><td style=\"text-align: right;\">      0.0116391</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.701244</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         120.265</td><td style=\"text-align: right;\">      0.858873</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00015</td><td>TERMINATED</td><td>192.168.236.234:1009791</td><td style=\"text-align: right;\">0.000705284</td><td style=\"text-align: right;\">          0.681995</td><td style=\"text-align: right;\">0.599014   </td><td style=\"text-align: right;\">      0.106481 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">   0.577541</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         133.486</td><td style=\"text-align: right;\">      0.847732</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00016</td><td>TERMINATED</td><td>192.168.236.234:1010268</td><td style=\"text-align: right;\">0.032358   </td><td style=\"text-align: right;\">          0.638383</td><td style=\"text-align: right;\">1.25525    </td><td style=\"text-align: right;\">      0.0555081</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.80223 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         128.569</td><td style=\"text-align: right;\">      0.849706</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00017</td><td>TERMINATED</td><td>192.168.236.234:1010739</td><td style=\"text-align: right;\">0.0424569  </td><td style=\"text-align: right;\">          0.786463</td><td style=\"text-align: right;\">0.35989    </td><td style=\"text-align: right;\">      0.0452184</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">   0.680765</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         125.236</td><td style=\"text-align: right;\">      0.850989</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00018</td><td>TERMINATED</td><td>192.168.236.234:1011186</td><td style=\"text-align: right;\">2.02878    </td><td style=\"text-align: right;\">          0.591285</td><td style=\"text-align: right;\">0.113749   </td><td style=\"text-align: right;\">      0.0346388</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.543062</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         116.483</td><td style=\"text-align: right;\">      0.855771</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00019</td><td>TERMINATED</td><td>192.168.236.234:1011608</td><td style=\"text-align: right;\">0.00045425 </td><td style=\"text-align: right;\">          0.916554</td><td style=\"text-align: right;\">0.0182747  </td><td style=\"text-align: right;\">      0.0129238</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.770311</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         132.948</td><td style=\"text-align: right;\">      0.854299</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00020</td><td>TERMINATED</td><td>192.168.236.234:1012080</td><td style=\"text-align: right;\">0.00161531 </td><td style=\"text-align: right;\">          0.638477</td><td style=\"text-align: right;\">0.00189362 </td><td style=\"text-align: right;\">      0.118459 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">   0.998504</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         115.856</td><td style=\"text-align: right;\">      0.853294</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00021</td><td>TERMINATED</td><td>192.168.236.234:1012498</td><td style=\"text-align: right;\">8.11561    </td><td style=\"text-align: right;\">          0.894285</td><td style=\"text-align: right;\">1.95689    </td><td style=\"text-align: right;\">      0.0200139</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">   0.685129</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         119.755</td><td style=\"text-align: right;\">      0.85606 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00022</td><td>TERMINATED</td><td>192.168.236.234:1012942</td><td style=\"text-align: right;\">0.00103324 </td><td style=\"text-align: right;\">          0.775879</td><td style=\"text-align: right;\">0.0140383  </td><td style=\"text-align: right;\">      0.0429799</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.860492</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         128.121</td><td style=\"text-align: right;\">      0.85007 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00023</td><td>TERMINATED</td><td>192.168.236.234:1013410</td><td style=\"text-align: right;\">0.000158979</td><td style=\"text-align: right;\">          0.694164</td><td style=\"text-align: right;\">0.0656815  </td><td style=\"text-align: right;\">      0.097564 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">   0.739965</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         118.059</td><td style=\"text-align: right;\">      0.850055</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00024</td><td>TERMINATED</td><td>192.168.236.234:1013835</td><td style=\"text-align: right;\">0.212188   </td><td style=\"text-align: right;\">          0.820067</td><td style=\"text-align: right;\">2.64187    </td><td style=\"text-align: right;\">      0.0114646</td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">   0.94093 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         118.455</td><td style=\"text-align: right;\">      0.857685</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00025</td><td>TERMINATED</td><td>192.168.236.234:1014273</td><td style=\"text-align: right;\">0.688854   </td><td style=\"text-align: right;\">          0.695156</td><td style=\"text-align: right;\">0.000301549</td><td style=\"text-align: right;\">      0.0895866</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">   0.52563 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         121.01 </td><td style=\"text-align: right;\">      0.850118</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00026</td><td>TERMINATED</td><td>192.168.236.234:1014710</td><td style=\"text-align: right;\">0.00075681 </td><td style=\"text-align: right;\">          0.572355</td><td style=\"text-align: right;\">7.01653    </td><td style=\"text-align: right;\">      0.113241 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">   0.996577</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         139.665</td><td style=\"text-align: right;\">      0.848065</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00027</td><td>TERMINATED</td><td>192.168.236.234:1015201</td><td style=\"text-align: right;\">3.42695    </td><td style=\"text-align: right;\">          0.597623</td><td style=\"text-align: right;\">0.0195794  </td><td style=\"text-align: right;\">      0.145073 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.616231</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         125.378</td><td style=\"text-align: right;\">      0.851214</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00028</td><td>TERMINATED</td><td>192.168.236.234:1015645</td><td style=\"text-align: right;\">1.20526    </td><td style=\"text-align: right;\">          0.709657</td><td style=\"text-align: right;\">0.00229795 </td><td style=\"text-align: right;\">      0.0120392</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">   0.908045</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         121.61 </td><td style=\"text-align: right;\">      0.856722</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00029</td><td>TERMINATED</td><td>192.168.236.234:1016084</td><td style=\"text-align: right;\">0.0222691  </td><td style=\"text-align: right;\">          0.572108</td><td style=\"text-align: right;\">0.0920625  </td><td style=\"text-align: right;\">      0.0305251</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">   0.653229</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         114.437</td><td style=\"text-align: right;\">      0.858829</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00030</td><td>TERMINATED</td><td>192.168.236.234:1016508</td><td style=\"text-align: right;\">0.000822606</td><td style=\"text-align: right;\">          0.554367</td><td style=\"text-align: right;\">5.53132    </td><td style=\"text-align: right;\">      0.013192 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">   0.779922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         140.427</td><td style=\"text-align: right;\">      0.854119</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00031</td><td>TERMINATED</td><td>192.168.236.234:1017005</td><td style=\"text-align: right;\">0.000354071</td><td style=\"text-align: right;\">          0.894076</td><td style=\"text-align: right;\">0.0107043  </td><td style=\"text-align: right;\">      0.161475 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">   0.936782</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         118.801</td><td style=\"text-align: right;\">      0.848579</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00032</td><td>TERMINATED</td><td>192.168.236.234:1017444</td><td style=\"text-align: right;\">3.65044    </td><td style=\"text-align: right;\">          0.521565</td><td style=\"text-align: right;\">0.0308052  </td><td style=\"text-align: right;\">      0.0328091</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">   0.733853</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         116.023</td><td style=\"text-align: right;\">      0.857887</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00033</td><td>TERMINATED</td><td>192.168.236.234:1017875</td><td style=\"text-align: right;\">2.59293    </td><td style=\"text-align: right;\">          0.505528</td><td style=\"text-align: right;\">0.012097   </td><td style=\"text-align: right;\">      0.012767 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.518238</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         127.984</td><td style=\"text-align: right;\">      0.857112</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00034</td><td>TERMINATED</td><td>192.168.236.234:1018320</td><td style=\"text-align: right;\">0.000207843</td><td style=\"text-align: right;\">          0.852118</td><td style=\"text-align: right;\">0.00862701 </td><td style=\"text-align: right;\">      0.0195553</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">   0.572545</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         115.134</td><td style=\"text-align: right;\">      0.860227</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00035</td><td>TERMINATED</td><td>192.168.236.234:1018744</td><td style=\"text-align: right;\">0.156941   </td><td style=\"text-align: right;\">          0.696094</td><td style=\"text-align: right;\">0.0535538  </td><td style=\"text-align: right;\">      0.226177 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">   0.580455</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         120.55 </td><td style=\"text-align: right;\">      0.84798 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00036</td><td>TERMINATED</td><td>192.168.236.234:1019175</td><td style=\"text-align: right;\">0.0024709  </td><td style=\"text-align: right;\">          0.518967</td><td style=\"text-align: right;\">0.00112623 </td><td style=\"text-align: right;\">      0.0226958</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.916643</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         117.726</td><td style=\"text-align: right;\">      0.856514</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00037</td><td>TERMINATED</td><td>192.168.236.234:1019613</td><td style=\"text-align: right;\">1.29232    </td><td style=\"text-align: right;\">          0.545638</td><td style=\"text-align: right;\">2.18865    </td><td style=\"text-align: right;\">      0.115359 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">   0.589264</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         116.349</td><td style=\"text-align: right;\">      0.853989</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00038</td><td>TERMINATED</td><td>192.168.236.234:1020045</td><td style=\"text-align: right;\">0.0411813  </td><td style=\"text-align: right;\">          0.536011</td><td style=\"text-align: right;\">0.243726   </td><td style=\"text-align: right;\">      0.160642 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">   0.7973  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         125.44 </td><td style=\"text-align: right;\">      0.848956</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00039</td><td>TERMINATED</td><td>192.168.236.234:1020486</td><td style=\"text-align: right;\">3.86133    </td><td style=\"text-align: right;\">          0.963102</td><td style=\"text-align: right;\">4.0494     </td><td style=\"text-align: right;\">      0.106039 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">   0.856539</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         115.501</td><td style=\"text-align: right;\">      0.856262</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00040</td><td>TERMINATED</td><td>192.168.236.234:1020912</td><td style=\"text-align: right;\">0.157519   </td><td style=\"text-align: right;\">          0.51208 </td><td style=\"text-align: right;\">3.84408    </td><td style=\"text-align: right;\">      0.0224709</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">   0.722337</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         130.975</td><td style=\"text-align: right;\">      0.85297 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00041</td><td>TERMINATED</td><td>192.168.236.234:1021368</td><td style=\"text-align: right;\">1.62788    </td><td style=\"text-align: right;\">          0.830472</td><td style=\"text-align: right;\">8.10623    </td><td style=\"text-align: right;\">      0.0218567</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">   0.916826</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         113.034</td><td style=\"text-align: right;\">      0.86018 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00042</td><td>TERMINATED</td><td>192.168.236.234:1021788</td><td style=\"text-align: right;\">0.557428   </td><td style=\"text-align: right;\">          0.855299</td><td style=\"text-align: right;\">0.65906    </td><td style=\"text-align: right;\">      0.204949 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">   0.610634</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         134.559</td><td style=\"text-align: right;\">      0.84788 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00043</td><td>TERMINATED</td><td>192.168.236.234:1022265</td><td style=\"text-align: right;\">4.42143    </td><td style=\"text-align: right;\">          0.97294 </td><td style=\"text-align: right;\">5.39423    </td><td style=\"text-align: right;\">      0.0509519</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">   0.918353</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         119.285</td><td style=\"text-align: right;\">      0.852434</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00044</td><td>TERMINATED</td><td>192.168.236.234:1022695</td><td style=\"text-align: right;\">0.00136353 </td><td style=\"text-align: right;\">          0.555566</td><td style=\"text-align: right;\">0.122457   </td><td style=\"text-align: right;\">      0.0103321</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">   0.764392</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         144.934</td><td style=\"text-align: right;\">      0.854752</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00045</td><td>TERMINATED</td><td>192.168.236.234:1023207</td><td style=\"text-align: right;\">0.00181749 </td><td style=\"text-align: right;\">          0.910526</td><td style=\"text-align: right;\">4.43239    </td><td style=\"text-align: right;\">      0.0503477</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.887267</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         116.968</td><td style=\"text-align: right;\">      0.853878</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00046</td><td>TERMINATED</td><td>192.168.236.234:1023630</td><td style=\"text-align: right;\">0.119174   </td><td style=\"text-align: right;\">          0.83642 </td><td style=\"text-align: right;\">0.00012569 </td><td style=\"text-align: right;\">      0.0110687</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.732807</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         120.055</td><td style=\"text-align: right;\">      0.859234</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00047</td><td>TERMINATED</td><td>192.168.236.234:1024063</td><td style=\"text-align: right;\">1.71205    </td><td style=\"text-align: right;\">          0.823815</td><td style=\"text-align: right;\">0.072089   </td><td style=\"text-align: right;\">      0.142261 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">   0.878466</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         114.325</td><td style=\"text-align: right;\">      0.853106</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00048</td><td>TERMINATED</td><td>192.168.236.234:1024487</td><td style=\"text-align: right;\">0.00328282 </td><td style=\"text-align: right;\">          0.887186</td><td style=\"text-align: right;\">0.000410361</td><td style=\"text-align: right;\">      0.0542501</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">   0.551501</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         140.913</td><td style=\"text-align: right;\">      0.848814</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00049</td><td>TERMINATED</td><td>192.168.236.234:1024971</td><td style=\"text-align: right;\">0.501835   </td><td style=\"text-align: right;\">          0.829013</td><td style=\"text-align: right;\">6.75394    </td><td style=\"text-align: right;\">      0.0148373</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">   0.816801</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         114.063</td><td style=\"text-align: right;\">      0.861453</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th style=\"text-align: right;\">  c-index stat</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00000</td><td style=\"text-align: right;\">      0.85425 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00001</td><td style=\"text-align: right;\">      0.852337</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00002</td><td style=\"text-align: right;\">      0.853303</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00003</td><td style=\"text-align: right;\">      0.856744</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00004</td><td style=\"text-align: right;\">      0.855037</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00005</td><td style=\"text-align: right;\">      0.848903</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00006</td><td style=\"text-align: right;\">      0.858138</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00007</td><td style=\"text-align: right;\">      0.848343</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00008</td><td style=\"text-align: right;\">      0.857573</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00009</td><td style=\"text-align: right;\">      0.859644</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00010</td><td style=\"text-align: right;\">      0.854543</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00011</td><td style=\"text-align: right;\">      0.848873</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00012</td><td style=\"text-align: right;\">      0.849451</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00013</td><td style=\"text-align: right;\">      0.853892</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00014</td><td style=\"text-align: right;\">      0.858873</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00015</td><td style=\"text-align: right;\">      0.847732</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00016</td><td style=\"text-align: right;\">      0.849706</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00017</td><td style=\"text-align: right;\">      0.850989</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00018</td><td style=\"text-align: right;\">      0.855771</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00019</td><td style=\"text-align: right;\">      0.854299</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00020</td><td style=\"text-align: right;\">      0.853294</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00021</td><td style=\"text-align: right;\">      0.85606 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00022</td><td style=\"text-align: right;\">      0.85007 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00023</td><td style=\"text-align: right;\">      0.850055</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00024</td><td style=\"text-align: right;\">      0.857685</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00025</td><td style=\"text-align: right;\">      0.850118</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00026</td><td style=\"text-align: right;\">      0.848065</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00027</td><td style=\"text-align: right;\">      0.851214</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00028</td><td style=\"text-align: right;\">      0.856722</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00029</td><td style=\"text-align: right;\">      0.858829</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00030</td><td style=\"text-align: right;\">      0.854119</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00031</td><td style=\"text-align: right;\">      0.848579</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00032</td><td style=\"text-align: right;\">      0.857887</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00033</td><td style=\"text-align: right;\">      0.857112</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00034</td><td style=\"text-align: right;\">      0.860227</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00035</td><td style=\"text-align: right;\">      0.84798 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00036</td><td style=\"text-align: right;\">      0.856514</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00037</td><td style=\"text-align: right;\">      0.853989</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00038</td><td style=\"text-align: right;\">      0.848956</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00039</td><td style=\"text-align: right;\">      0.856262</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00040</td><td style=\"text-align: right;\">      0.85297 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00041</td><td style=\"text-align: right;\">      0.86018 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00042</td><td style=\"text-align: right;\">      0.84788 </td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00043</td><td style=\"text-align: right;\">      0.852434</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00044</td><td style=\"text-align: right;\">      0.854752</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00045</td><td style=\"text-align: right;\">      0.853878</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00046</td><td style=\"text-align: right;\">      0.859234</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00047</td><td style=\"text-align: right;\">      0.853106</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00048</td><td style=\"text-align: right;\">      0.848814</td></tr>\n",
       "<tr><td>xgboost_training_wrapper_0df1a_00049</td><td style=\"text-align: right;\">      0.861453</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:33:07,651 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:35:20,666 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:37:15,012 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:39:11,679 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:41:26,716 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:43:35,691 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:45:37,053 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:47:44,061 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:49:45,263 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:51:29,885\tWARNING util.py:201 -- The `choose_trial_to_run` operation took 3.612 s, which may be a performance bottleneck.\n",
      "2024-11-17 18:51:40,212 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:53:35,206 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:55:35,790 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:57:46,852 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 18:59:43,852 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:01:47,063 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:04:03,003 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:06:17,545 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:08:25,792 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:10:24,802 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:12:40,101 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:14:38,479 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:16:40,956 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:18:55,024 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:20:54,802 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:22:59,419 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:25:02,077 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:27:23,567 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:29:30,739 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:31:35,228 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:33:32,073 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:35:58,538 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:38:04,578 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:40:02,562 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:42:12,246 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:44:09,897 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:46:12,198 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:48:15,799 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:50:14,243 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:52:22,206 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:54:20,464 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:56:34,005 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 19:58:29,620 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 20:00:45,895 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 20:02:47,794 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 20:05:14,515 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 20:07:14,055 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 20:09:16,187 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 20:11:12,683 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 20:13:36,099 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 20:15:32,700 - INFO - Summary name ray/tune/c-index stat is illegal; using ray/tune/c-index_stat instead.\n",
      "2024-11-17 20:15:33,975\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/d/PYDataScience/g3_regress/data/results/xgboost_training_wrapper_2024-11-17_18-31-06' in 1.2675s.\n",
      "2024-11-17 20:15:33,981\tINFO tune.py:1041 -- Total run time: 6267.00 seconds (6265.62 seconds for the tuning loop).\n",
      "2024-11-17 20:15:34,258\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ray.train\n",
    "import xgboost as xgb\n",
    "import ray\n",
    "from ray import tune\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycox.evaluation import EvalSurv\n",
    "import pandas as pd\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def xgboost_training_wrapper(config, data):\n",
    "    \"\"\"\n",
    "    A Ray Tune-compatible wrapper to train XGBoost models for all outcomes and time points,\n",
    "    evaluate predictions using concordance index, and report mean concordance index.\n",
    "\n",
    "    Args:\n",
    "        config: Dictionary of hyperparameters provided by Ray Tune.\n",
    "        data: Dictionary containing final_meta_learner_X_train, cif_ground_truth_test_stacked,\n",
    "              durations, and events.\n",
    "    \"\"\"\n",
    "    final_meta_learner_X_train = data[\"final_meta_learner_X_train\"]\n",
    "    cif_ground_truth_test_stacked = data[\"cif_ground_truth_test_stacked\"]\n",
    "    durations = data[\"durations\"]\n",
    "    events = data[\"events\"]\n",
    "\n",
    "    # Initialize predictions array (2 outcomes, 6 time points, 316242 patients)\n",
    "    xgboost_predictions = np.zeros((2, 6, len(durations)))\n",
    "\n",
    "    # Train models and make predictions for each outcome and time point\n",
    "    for outcome_idx in range(2):\n",
    "        for time_idx in range(6):\n",
    "            # Prepare data\n",
    "            stacking_inputs = np.array([\n",
    "                final_meta_learner_X_train[key][outcome_idx, time_idx]\n",
    "                for key in final_meta_learner_X_train.keys()\n",
    "            ]).T  # Shape: (316242, 9)\n",
    "\n",
    "            target = cif_ground_truth_test_stacked[outcome_idx, time_idx, :]  # Shape: (316242,)\n",
    "\n",
    "            # Split data into training and validation sets\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                stacking_inputs, target, test_size=0.2, random_state=RANDOM_SEED\n",
    "            )\n",
    "\n",
    "            # Prepare DMatrix for XGBoost\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "            # Train XGBoost model\n",
    "            booster = xgb.train(\n",
    "                config,\n",
    "                dtrain,\n",
    "                num_boost_round=500,\n",
    "                evals=[(dtrain, \"train\"), (dval, \"validation\")],\n",
    "                early_stopping_rounds=20,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "\n",
    "            # Predict CIF for all patients\n",
    "            dtest = xgb.DMatrix(stacking_inputs)\n",
    "            xgboost_predictions[outcome_idx, time_idx] = booster.predict(dtest)\n",
    "\n",
    "    # Compute concordance index for each event\n",
    "    concordance_indices = {}\n",
    "    for i in range(2):\n",
    "        event_interest = i + 1\n",
    "        cif = pd.DataFrame(xgboost_predictions[i], index=[0, 1, 2, 3, 4, 5])\n",
    "        ev = EvalSurv(1 - cif, durations, events == event_interest, censor_surv=\"km\")\n",
    "        concordance_indices[f\"Event_{event_interest}\"] = ev.concordance_td()\n",
    "\n",
    "    # Calculate mean concordance index\n",
    "    mean_concordance_index = np.mean(list(concordance_indices.values()))\n",
    "\n",
    "    # Report mean concordance index to Ray Tune\n",
    "    ray.train.report({'c-index stat': mean_concordance_index})\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"tree_method\": \"gpu_hist\",  # Use GPU acceleration\n",
    "    \"max_depth\": tune.randint(3, 10),  # Maximum depth of trees\n",
    "    \"learning_rate\": tune.loguniform(0.01, 0.3),  # Learning rate\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),  # Subsampling rate\n",
    "    \"colsample_bytree\": tune.uniform(0.5, 1.0),  # Feature subsampling rate\n",
    "    \"lambda\": tune.loguniform(1e-4, 10.0),  # L2 regularization term\n",
    "    \"alpha\": tune.loguniform(1e-4, 10.0),  # L1 regularization term\n",
    "}\n",
    "\n",
    "# Prepare data\n",
    "data = {\n",
    "    \"final_meta_learner_X_train\": final_meta_learner_X_train,\n",
    "    \"cif_ground_truth_test_stacked\": cif_ground_truth_test_stacked,\n",
    "    \"durations\": durations,\n",
    "    \"events\": events,\n",
    "}\n",
    "\n",
    "# Run Ray Tune\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(xgboost_training_wrapper, data=data),\n",
    "    config=search_space,\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 1},  # Adjust based on your resources\n",
    "    num_samples=50,  # Number of hyperparameter configurations to try\n",
    "    metric='c-index stat',  # Metric to optimize\n",
    "    mode=\"max\",  # Maximize the concordance index\n",
    "    storage_path=\"/mnt/d/PYDataScience/g3_regress/data/results\",  # Directory to store results\n",
    ")\n",
    "\n",
    "# Get the best configuration\n",
    "best_config = analysis.get_best_config(metric=\"mean_concordance_index\", mode=\"max\")\n",
    "print(\"Best hyperparameters:\", best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8614533246252081,\n",
       " {'objective': 'reg:squarederror',\n",
       "  'tree_method': 'gpu_hist',\n",
       "  'max_depth': 3,\n",
       "  'learning_rate': 0.014837295326564928,\n",
       "  'subsample': 0.8168005866659258,\n",
       "  'colsample_bytree': 0.8290129403377126,\n",
       "  'lambda': 6.7539372305286465,\n",
       "  'alpha': 0.5018353832953043})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the base directory containing the trial folders\n",
    "base_dir = \"/mnt/d/PYDataScience/g3_regress/data/results/xgboost_training_wrapper_2024-11-17_18-31-06\"\n",
    "\n",
    "# Initialize variables to store the max c-index and corresponding config\n",
    "max_c_index = float(\"-inf\")\n",
    "best_config = None\n",
    "\n",
    "# Iterate through all folders and parse the result.json files\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file == \"result.json\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                if \"c-index stat\" in data:\n",
    "                    c_index = data[\"c-index stat\"]\n",
    "                    # Update max c-index and config if a new max is found\n",
    "                    if c_index > max_c_index:\n",
    "                        max_c_index = c_index\n",
    "                        best_config = data.get(\"config\", None)\n",
    "\n",
    "# Display the results\n",
    "max_c_index, best_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Ensemble Predictions shape: (63249, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare input for meta-learner\n",
    "stacked_X = np.hstack([pred.reshape(12, -1).T for pred in predictions])  # (316242, 108)\n",
    "stacked_y_durations = durations\n",
    "stacked_y_events = events\n",
    "\n",
    "# Split into train and validation\n",
    "X_train, X_val, y_train_durations, y_val_durations, y_train_events, y_val_events = train_test_split(\n",
    "    stacked_X, stacked_y_durations, stacked_y_events, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Logistic Regression as meta-learner\n",
    "meta_learner = LogisticRegressionCV(cv=5, max_iter=1000)\n",
    "meta_learner.fit(X_train, y_train_events)\n",
    "\n",
    "# Predict probabilities\n",
    "stacking_predictions = meta_learner.predict_proba(X_val)\n",
    "\n",
    "print(\"Stacking Ensemble Predictions shape:\", stacking_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Stacking Predictions Shape: (2, 6, 316242)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Initialize the final stacking predictions array\n",
    "lineregression_predictions = np.zeros((2, 6, 316242))  # (2 outcomes, 6 time points, 316242 patients)\n",
    "\n",
    "def train_and_predict_with_linear_regression(outcome_idx, time_idx):\n",
    "    \"\"\"\n",
    "    Train a Linear Regression model using CIF targets for a specific outcome and time point.\n",
    "    \"\"\"\n",
    "    # Prepare stacking inputs and targets\n",
    "    stacking_inputs = np.array([\n",
    "        final_meta_learner_X_train[key][outcome_idx, time_idx]\n",
    "        for key in final_meta_learner_X_train.keys()\n",
    "    ]).T  # Shape: (316242, 9)\n",
    "\n",
    "    target = cif_ground_truth_test_stacked[outcome_idx, time_idx, :]  # Correct indexing\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(stacking_inputs, target, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "    # Train the Linear Regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict CIF for all patients\n",
    "    cif_predictions = model.predict(stacking_inputs)\n",
    "\n",
    "    return outcome_idx, time_idx, cif_predictions\n",
    "\n",
    "# Parallel processing for each outcome and time point\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(train_and_predict_with_linear_regression)(outcome_idx, time_idx)\n",
    "    for outcome_idx in range(2)\n",
    "    for time_idx in range(6)\n",
    ")\n",
    "\n",
    "# Assign results to the xgboost_predictions array\n",
    "for outcome_idx, time_idx, cif_predictions in results:\n",
    "    lineregression_predictions[outcome_idx, time_idx] = cif_predictions\n",
    "\n",
    "# Verify the shape of the stacked predictions\n",
    "print(\"Linear Regression Stacking Predictions Shape:\", lineregression_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.9809462205590567, 'Event_2': 0.756546696256201}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0    0.000000\n",
       " 1    0.001830\n",
       " 2    0.005980\n",
       " 3    0.017156\n",
       " 4    0.117646\n",
       " 5    0.053186\n",
       " Name: brier_score, dtype: float64,\n",
       " 'Event_2': 0    0.000000\n",
       " 1    0.004280\n",
       " 2    0.021823\n",
       " 3    0.085563\n",
       " 4    0.295899\n",
       " 5    0.075195\n",
       " Name: brier_score, dtype: float64}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.033535969195957804, 'Event_2': 0.090887707288392}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Event_1': 0.11909481786053584, 'Event_2': 0.296799736499944}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concordance_indices = {}\n",
    "integrated_brier_scores = {}\n",
    "neg_log_likelihoods = {}\n",
    "brier_series = {}\n",
    "for i in range(0, 2):\n",
    "    event_interest = i + 1\n",
    "    cif = pd.DataFrame(lineregression_predictions[i], [0, 1, 2, 3, 4, 5])\n",
    "    ev = EvalSurv(1-cif, durations, events == event_interest, censor_surv='km')\n",
    "    concordance_indices[f\"Event_{event_interest}\"] = ev.concordance_td()\n",
    "    brier_series[f\"Event_{event_interest}\"] = ev.brier_score(np.array([0, 1, 2, 3, 4, 5]))\n",
    "    integrated_brier_scores[f\"Event_{event_interest}\"] = ev.integrated_brier_score(np.array([0, 1, 2, 3, 4, 5]))\n",
    "    neg_log_likelihoods[f\"Event_{event_interest}\"] = ev.integrated_nbll(np.array([0, 1, 2, 3, 4, 5]))\n",
    "\n",
    "    # # Nam and D'Agostino Chi2 statistic for calibration\n",
    "    # for time in time_grid:\n",
    "    #     chi2_stat, p_value, observed_events, expected_events, n, prob_df = nam_dagostino_chi2(\n",
    "    #         df=df_test, \n",
    "    #         duration_col=duration_col, \n",
    "    #         event_col=event_col,\n",
    "    #         surv=(1-cif), \n",
    "    #         time=time, \n",
    "    #         event_focus=event_interest\n",
    "    #     )\n",
    "    #     nam_dagostino_results.append({\n",
    "    #         'Event': event_interest,\n",
    "    #         'Year': round(time / 365),\n",
    "    #         'Chi2_Stat': chi2_stat,\n",
    "    #         'P_Value': p_value,\n",
    "    #         'Observed_Events': observed_events.tolist(),\n",
    "    #         'Expected_Events': expected_events.tolist(),\n",
    "    #         'Sample_Size': n.tolist()\n",
    "    #     })\n",
    "display(concordance_indices)\n",
    "display(brier_series)\n",
    "display(integrated_brier_scores)\n",
    "display(neg_log_likelihoods)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
