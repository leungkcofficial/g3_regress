{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necesssary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, GroupKFold, KFold\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import brier_score_loss, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "from pycox.models import CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.preprocessing.feature_transforms import OrderedCategoricalLong\n",
    "from pycox.models.loss import NLLMTLRLoss\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.model_selection import KFold, GroupShuffleSplit\n",
    "from sksurv.metrics import brier_score, integrated_brier_score, concordance_index_censored, cumulative_dynamic_auc\n",
    "from sksurv.util import Surv\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import Bunch\n",
    "import traceback\n",
    "import pickle\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the imputed training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gc.collect()\n",
    "# def load_imputed_datasets_hdf5(base_filename):\n",
    "#     imputed_datasets = {}\n",
    "#     with pd.HDFStore(f\"{base_filename}.h5\", 'r') as store:\n",
    "#         for key in store.keys():\n",
    "#             estimator_name, dataset_name = key.strip('/').split('/')\n",
    "#             if estimator_name not in imputed_datasets:\n",
    "#                 imputed_datasets[estimator_name] = []\n",
    "#             imputed_datasets[estimator_name].append(store[key])\n",
    "\n",
    "#     # Convert lists to arrays if necessary, or process them as needed\n",
    "#     return imputed_datasets\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_imputed_datasets_hdf5(base_filename):\n",
    "    datasets = {}\n",
    "    with pd.HDFStore(f\"{base_filename}.h5\", 'r') as store:\n",
    "        for key in store.keys():\n",
    "            # Split the key to get estimator_name and dataset identifier\n",
    "            parts = key.strip('/').split('/')\n",
    "            estimator_name = parts[0]\n",
    "            dataset_name = '/'.join(parts[1:])\n",
    "            if estimator_name not in datasets:\n",
    "                datasets[estimator_name] = []\n",
    "            datasets[estimator_name].append(store[key])\n",
    "    return datasets\n",
    "\n",
    "class RowFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column, condition):\n",
    "        self.column = column\n",
    "        self.condition = condition\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()  # To avoid modifying the original DataFrame\n",
    "        if self.column in X.columns:\n",
    "            X = X[self.condition(X[self.column])]\n",
    "        return X.reset_index(drop=True)\n",
    "\n",
    "# Custom transformer for log transformation\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, offset=1e-6):\n",
    "        self.offset = offset\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)  # Ensure X is a DataFrame\n",
    "        # Apply log transformation column-wise\n",
    "        X = X.apply(lambda col: np.log(col + self.offset) if np.issubdtype(col.dtype, np.number) else col)\n",
    "        return X\n",
    "\n",
    "class DataFrameTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_names, dtypes=None):\n",
    "        self.column_names = column_names\n",
    "        self.dtypes = dtypes or {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X, columns=self.column_names)\n",
    "        for column, dtype in self.dtypes.items():\n",
    "            X[column] = X[column].astype(dtype)\n",
    "        return X\n",
    "    \n",
    "class DataFrameShuffler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return shuffle(X).reset_index(drop=True)\n",
    "\n",
    "# Define feature groups\n",
    "cat_features = ['gender', 'dm', 'ht', 'sprint', 'endpoint']\n",
    "log_features = ['a1c', 'po4', 'UACR_mg_g', 'Cr']\n",
    "standard_features = ['age', 'alb', 'ca', 'hb', 'hco3']\n",
    "impute_features = cat_features + log_features + standard_features\n",
    "passthrough_features = ['key', 'date_from_sub_60']\n",
    "\n",
    "dtypes = {\n",
    "    **{name: 'float' for name in log_features + standard_features},\n",
    "    **{name: 'category' for name in cat_features},\n",
    "    'key': 'int'\n",
    "    # 'date': 'datetime64[ns]'\n",
    "}\n",
    "\n",
    "RANDOM_SEED=12345\n",
    "\n",
    "# Full pipeline with DataFrame preservation and shuffling\n",
    "pipeline = Pipeline([\n",
    "    ('impute', ColumnTransformer([\n",
    "        ('imputer', IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=RANDOM_SEED,\n",
    "                                     initial_strategy='mean', n_nearest_features=None, min_value=1e-6,\n",
    "                                   imputation_order='ascending'), impute_features),\n",
    "        ('passthrough', 'passthrough', passthrough_features)\n",
    "    ], remainder='drop')),\n",
    "    ('to_df', DataFrameTransformer(impute_features + passthrough_features, dtypes)),\n",
    "    ('process', ColumnTransformer([\n",
    "        ('categorical', FunctionTransformer(lambda x: x.astype('category')), cat_features),\n",
    "        ('log', LogTransformer(), log_features),\n",
    "        ('scaler', StandardScaler(), standard_features),\n",
    "        ('passthrough', 'passthrough', passthrough_features)\n",
    "    ], remainder='drop')),\n",
    "    ('to_df2', DataFrameTransformer(impute_features + passthrough_features, dtypes)),\n",
    "    ('row_filter', RowFilter('date_from_sub_60', lambda x: x <= 1825)),  # Add this step\n",
    "    ('shuffle', DataFrameShuffler())  # Shuffling step\n",
    "])\n",
    "X_load = read_imputed_datasets_hdf5('/mnt/d/pydatascience/g3_regress/data/X/X')\n",
    "X_train = X_load['X_train_main'][0]\n",
    "X_test = X_load['X_test_main'][0]\n",
    "# X_train['gender'] = X_train['gender'].map({'M':1, 'F':0})\n",
    "# X_test['gender'] = X_test['gender'].map({'M':1, 'F':0})\n",
    "\n",
    "X_train_transformed = pipeline.fit_transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the endpoints, prepare pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def prepare_endpoint_data(endpoint_df):\n",
    "#     # Convert date columns to datetime\n",
    "#     date_columns = ['first_sub_60_date', 'first_sub_10_date', 'ot_date', 'death_date', 'first_endpoint_date']\n",
    "#     for col in date_columns:\n",
    "#         endpoint_df[col] = pd.to_datetime(endpoint_df[col], errors='coerce')  # Use coerce to handle any invalid date formats\n",
    "    \n",
    "#     # Drop the 'Unnamed: 0' column if it's just an artifact of reading from a CSV\n",
    "#     if 'Unnamed: 0' in endpoint_df.columns:\n",
    "#         endpoint_df = endpoint_df.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "#     return endpoint_df\n",
    "\n",
    "# def merge_with_endpoint(imputed_datasets, endpoint_df):\n",
    "#     # Prepare the endpoint data\n",
    "#     endpoint_df = prepare_endpoint_data(endpoint_df)\n",
    "#     # Dictionary to hold the merged data\n",
    "#     merged_datasets = {}\n",
    "\n",
    "#     # Loop over each estimator and its datasets\n",
    "#     for estimator_name, datasets in imputed_datasets.items():\n",
    "#         merged_datasets[estimator_name] = []\n",
    "#         for dataset in datasets:\n",
    "#             # Merge using a left join to keep all rows from the imputed dataset\n",
    "#             merged_data = pd.merge(dataset, endpoint_df, on='key', how='left')\n",
    "#             merged_datasets[estimator_name].append(merged_data)\n",
    "\n",
    "#     return merged_datasets\n",
    "\n",
    "# def process_death_key(df):\n",
    "#     def process_group(group):\n",
    "#         max_date = group['date'].max()\n",
    "#         first_endpoint_date = group['first_endpoint_date'].iloc[0]\n",
    "\n",
    "#         if max_date < first_endpoint_date and (group['endpoint'] == 2).any():\n",
    "#             latest_row = group.loc[group['date'].idxmax()]\n",
    "#             new_row = latest_row.copy()\n",
    "#             new_row['date'] = first_endpoint_date\n",
    "#             new_row['date_from_sub_60'] = (new_row['date'] - new_row['first_sub_60_date']).days\n",
    "#             new_row['date_till_endpoint'] = (new_row['first_endpoint_date'] - new_row['date']).days\n",
    "#             new_row['endpoint'] = 2\n",
    "#             group['endpoint'] = 0  # Set all existing rows' endpoint to 0\n",
    "#             return pd.concat([group, new_row.to_frame().T], ignore_index=True)\n",
    "#         else:\n",
    "#             return group\n",
    "\n",
    "#     return df.groupby('key').apply(process_group).reset_index(drop=True)\n",
    "\n",
    "# def prepare_for_training(merged_datasets):\n",
    "#     prepared_datasets = {}\n",
    "\n",
    "#     for estimator_name, datasets in merged_datasets.items():\n",
    "#         prepared_datasets[estimator_name] = []\n",
    "#         for df in datasets:\n",
    "#             df = df.copy()\n",
    "            \n",
    "#             # Handle categorical data\n",
    "#             df['gender'] = LabelEncoder().fit_transform(df['gender'])\n",
    "#             # Filter rows where 'date' is greater than 'first_sub_60_date'\n",
    "#             df = df[df['date'] > df['first_sub_60_date']]\n",
    "#             # Calculate 'date_from_sub_60' in days\n",
    "#             df.loc[:, 'date_from_sub_60'] = (df['date'] - df['first_sub_60_date']).dt.days.astype(float)\n",
    "#             # Calculate 'date_till_endpoint' in days\n",
    "#             df.loc[:, 'date_till_endpoint'] = (df['first_endpoint_date'] - df['date']).dt.days.astype(float)\n",
    "#             # Log scaling for 'UACR_mg_g', handle 0, negative and NaN values appropriately\n",
    "#             df.loc[df['UACR_mg_g'] <= 0, 'UACR_mg_g'] = 0  # Ensure no non-positive values\n",
    "#             df.loc[:, 'UACR_mg_g'] = df['UACR_mg_g'] + 1  # Shift data to avoid log(0)\n",
    "#             df.loc[:, 'UACR_mg_g_log'] = np.log(df['UACR_mg_g'])\n",
    "#             df.loc[:, 'UACR_mg_g_log'] = df['UACR_mg_g_log'].replace(-np.inf, 0)  # Replace -inf with 0 if log(1) results in -inf\n",
    "#             # df = process_death_key(df)\n",
    "\n",
    "#             prepared_datasets[estimator_name].append(df)\n",
    "\n",
    "#     return prepared_datasets\n",
    "\n",
    "# # Load the datasets\n",
    "# base_filename = '/mnt/d/pydatascience/g3_regress/data/X/X_imputed_mice'\n",
    "# X_train_all = load_imputed_datasets_hdf5(base_filename)\n",
    "# del base_filename\n",
    "# endpoint_df = pd.read_csv('/mnt/d/pydatascience/g3_regress/data/pt_endpoint_ls.csv')\n",
    "# X_train_all = merge_with_endpoint(X_train_all, endpoint_df)\n",
    "# X_train_all = prepare_for_training(X_train_all)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 431832 entries, 0 to 431831\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count   Dtype   \n",
      "---  ------            --------------   -----   \n",
      " 0   gender            431832 non-null  category\n",
      " 1   dm                431832 non-null  category\n",
      " 2   ht                431832 non-null  category\n",
      " 3   sprint            431832 non-null  category\n",
      " 4   endpoint          431832 non-null  category\n",
      " 5   a1c               431832 non-null  float64 \n",
      " 6   po4               431832 non-null  float64 \n",
      " 7   UACR_mg_g         431832 non-null  float64 \n",
      " 8   Cr                431832 non-null  float64 \n",
      " 9   age               431832 non-null  float64 \n",
      " 10  alb               431832 non-null  float64 \n",
      " 11  ca                431832 non-null  float64 \n",
      " 12  hb                431832 non-null  float64 \n",
      " 13  hco3              431832 non-null  float64 \n",
      " 14  key               431832 non-null  int64   \n",
      " 15  date_from_sub_60  431832 non-null  float64 \n",
      "dtypes: category(5), float64(10), int64(1)\n",
      "memory usage: 38.3 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed.info(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Find out the best imputation strategy for the cox model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_csv(path, data):\n",
    "#     os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#     data.to_csv(path, mode='a', header=not os.path.isfile(path))\n",
    "\n",
    "# def fit_and_evaluate(model, train_data, test_data, duration_col, event_col, penalizer, l1_ratio, fold_number):\n",
    "#     try:\n",
    "#         # model.set_params(penalizer=penalizer, l1_ratio=l1_ratio)\n",
    "#         print(f\" Fitting and evaluating Fold {fold_number} with penalizer={penalizer}, l1_ratio={l1_ratio}\")\n",
    "#         model.fit(train_data, duration_col=duration_col, event_col=event_col)\n",
    "#         predictions = model.predict_partial_hazard(test_data)\n",
    "#         score = concordance_index(test_data[duration_col], -predictions, test_data[event_col])\n",
    "#         return score\n",
    "#     except Exception as e:\n",
    "#         # Log the error details\n",
    "#         error_message = f\"Fold {fold_number}: Failed to fit model with penalizer={penalizer}, l1_ratio={l1_ratio}: {str(e)}\"\n",
    "#         print(error_message)\n",
    "#         traceback.print_exc()  # Optional: to log the traceback of the exception\n",
    "        \n",
    "#         # You can also append the error message to a file or a list\n",
    "#         with open('model_fit_errors.log', 'a') as f:\n",
    "#             f.write(error_message + '\\n')\n",
    "        \n",
    "#         return None  # Return None or a specific indicator of failure\n",
    "\n",
    "# def perform_cross_validation(data, k, penalizer_values, l1_ratios, duration_col, event_col):\n",
    "#     kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "#     best_score = 0\n",
    "#     best_model = None\n",
    "#     best_config = (None, None)\n",
    "#     cpu_cores = os.cpu_count()\n",
    "\n",
    "#     for fold_number, (train_index, test_index) in enumerate(kf.split(data), start=1):\n",
    "#         train_data = data.iloc[train_index]\n",
    "#         test_data = data.iloc[test_index]\n",
    "#         with ThreadPoolExecutor(max_workers=cpu_cores-1 if cpu_cores > 1 else 1) as executor:\n",
    "#             futures = []\n",
    "#             for penalizer in penalizer_values:\n",
    "#                 for l1_ratio in l1_ratios:\n",
    "#                     # print(f\"Recent penalizer: {penalizer}, Recent L1 ratio: {l1_ratio}\")\n",
    "#                     model = CoxPHFitter(penalizer=penalizer, l1_ratio=l1_ratio)\n",
    "#                     # Submit the task and store the future along with its parameters in a tuple\n",
    "#                     future = executor.submit(fit_and_evaluate, model, train_data, test_data, duration_col, event_col, penalizer, l1_ratio, fold_number)\n",
    "#                     futures.append((future, penalizer, l1_ratio))\n",
    "\n",
    "#             # Iterate over the list of tuples containing the futures and their parameters\n",
    "#             for future, penalizer, l1_ratio in futures:\n",
    "#                 score = future.result()  # Obtain the result from the future\n",
    "#                 if score is not None and score > best_score:\n",
    "#                     best_score = score\n",
    "#                     best_config = (penalizer, l1_ratio)\n",
    "#                     best_model = model  # Note: This model might not reflect the exact state when best was achieved due to threading\n",
    "\n",
    "#     return best_model, best_score, best_config\n",
    "\n",
    "# def fit_models_to_imputed_datasets(imputed_datasets, penalizer_values, l1_ratios, k_folds=10):\n",
    "#     summary_file = '/mnt/d/pydatascience/g3_regress/doc/cox_model_summaries.csv'\n",
    "#     config_file = '/mnt/d/pydatascience/g3_regress/doc/cox_best_configs.csv'\n",
    "\n",
    "#     for estimator_name, datasets in imputed_datasets.items():\n",
    "#         for dataset_index, dataset in enumerate(datasets):\n",
    "#             print(f\"Training of Cox model with {estimator_name} number {dataset_index} started\")\n",
    "#             categorical_columns = ['gender', 'dm', 'ht']\n",
    "#             continuous_columns = ['age', 'Cr', 'UACR_mg_g_log', 'hb', 'a1c', 'ca', 'hco3']\n",
    "#             duration_col = 'date_from_sub_60'\n",
    "#             event_col = 'endpoint'\n",
    "\n",
    "#             dataset = dataset[categorical_columns + continuous_columns + [duration_col, event_col]]\n",
    "#             best_model, best_score, best_config = perform_cross_validation(dataset, k_folds, penalizer_values, l1_ratios, duration_col, event_col)\n",
    "            \n",
    "#             summary_df = best_model.summary\n",
    "#             summary_df['imputer'] = estimator_name\n",
    "#             summary_df['dataset_index'] = dataset_index\n",
    "#             write_csv(summary_file, summary_df)\n",
    "            \n",
    "#             config_df = pd.DataFrame({\n",
    "#                 'imputer': [estimator_name],\n",
    "#                 'dataset_index': [dataset_index],\n",
    "#                 'best_penalizer': [best_config[0]],\n",
    "#                 'best_l1_ratio': [best_config[1]],\n",
    "#                 'best_score': [best_score]\n",
    "#             })\n",
    "#             write_csv(config_file, config_df)\n",
    "\n",
    "#             print(f\"Training of Cox model with {estimator_name} number {dataset_index} completed\")\n",
    "\n",
    "#     return summary_file, config_file\n",
    "\n",
    "# # Example usage\n",
    "# penalizer_values = np.logspace(-1, 2, 3)  # From 0.1 to 100 in logarithmic scale\n",
    "# l1_ratios = np.linspace(0, 1, 5)\n",
    "# summaries, best_penalizers = fit_models_to_imputed_datasets(X_train_all, penalizer_values, l1_ratios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best imputation for cox model:\n",
    "- Best imputer for cox model: BayesianRidge\n",
    "- Best penalizer: 0.1\n",
    "- Best l1_ratio: 0\n",
    "- Best concordance index: 0.7645822002946346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing clustering iteration 1\n",
      "Setting n_neighbors to 788 (size of minority class)\n",
      "0.08902150854450441\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 4.8731,\tval_loss: 7.5142\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingLogger' object has no attribute 'get_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, logs\n\u001b[1;32m     82\u001b[0m model_cluster \u001b[38;5;241m=\u001b[39m model_init(X[feature_col], params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m---> 83\u001b[0m model_cluster, log_cluster \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mduration_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mevent_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# X_cluster, y_event_cluster, X_cluster_remain = define_medoid(X_train, y_train_event, duration_col=duration_col, event_col=event_col)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# X_train_cluster_tensor, y_train_cluster_tensor = prepare_tensor(X_cluster, y_event_cluster, feature_col=feature_col, duration_col=duration_col)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# log_cluster = model_cluster.fit(X_train_cluster_tensor, y_train_cluster_tensor, params['batch_size'], params['epochs'], callbacks, verbose=True, val_data=val)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[228], line 69\u001b[0m, in \u001b[0;36mrecursive_clustering\u001b[0;34m(model, X, y, duration_col, event_col, feature_col, params, val, callbacks, max_repeats)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(lrfinder\u001b[38;5;241m.\u001b[39mget_best_lr())\n\u001b[1;32m     67\u001b[0m log \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_cluster_tensor, y_train_cluster_tensor, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m], callbacks, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, val_data\u001b[38;5;241m=\u001b[39mval)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_info\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(log\u001b[38;5;241m.\u001b[39mget_info(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     72\u001b[0m logs\u001b[38;5;241m.\u001b[39mappend(log)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingLogger' object has no attribute 'get_info'"
     ]
    }
   ],
   "source": [
    "def model_init(df, params):\n",
    "    net = tt.practical.MLPVanilla(df.shape[1], params['num_nodes'], 1, params['batch_norm'], params['dropout'])\n",
    "    model = CoxPH(net, tt.optim.AdamWR(decoupled_weight_decay=1e-6,\n",
    "                            cycle_eta_multiplier=0.8))\n",
    "    model.optimizer.set_lr(params['lr'])\n",
    "    return model\n",
    "\n",
    "def define_medoid(X,y, duration_col, event_col):\n",
    "    \n",
    "    majority_mask = y == 0\n",
    "    majority_indices = np.where(y == 0)[0]\n",
    "    minority_indices = np.where(y == 1)[0]\n",
    "    \n",
    "    features = [col for col in X.columns if col not in duration_col]\n",
    "    X['majority_mask'] = majority_mask\n",
    "    X_majority = X.loc[X['majority_mask'] == True, features]\n",
    "    X_majority_duration = X.loc[X_train['majority_mask'] == True, duration_col]\n",
    "    \n",
    "    n_clusters = min(np.sum(y == 1), len(X_majority))\n",
    "    print(f\"Setting n_neighbors to {n_clusters} (size of minority class)\")\n",
    "    # Initialize and fit NearestNeighbors\n",
    "    nn = NearestNeighbors(n_neighbors=n_clusters + 1, algorithm='auto', n_jobs=-1)  # +1 because the first neighbor is the point itself\n",
    "    nn.fit(X_majority)\n",
    "    \n",
    "    # Compute the distances to the n_neighbors-th nearest neighbor for each point\n",
    "    # Suppose medoid should be the row that have lowest distances with all other point\n",
    "    distances, _ = nn.kneighbors(X_majority)\n",
    "    total_distance = distances.sum(axis=1)\n",
    "    cluster_center = np.argsort(total_distance)[:n_clusters]\n",
    "    X_train_medoid = X_majority.iloc[cluster_center].copy()\n",
    "    X_remain = X.copy()\n",
    "    X_remain.loc[:, event_col] = y\n",
    "    X_remain = X_remain.drop(X_majority.index[cluster_center])\n",
    "    y_remain = X_remain[event_col].values\n",
    "    X_remain = X_remain.drop(columns=event_col)\n",
    "    X_remain = X_remain.drop(columns=['majority_mask'])\n",
    "    \n",
    "    X_train_medoid.loc[:, duration_col] = X_majority_duration.iloc[cluster_center]\n",
    "    y_train_majority = y[majority_indices]\n",
    "    X_train_medoid.loc[:, event_col] = y_train_majority[cluster_center].copy()\n",
    "    \n",
    "    X_minority = X_train.loc[X_train['majority_mask'] == False]\n",
    "    y_train_minority = y_train_event[minority_indices]\n",
    "    X_minority.loc[:,event_col] = y_train_minority\n",
    "   \n",
    "    # Concat\n",
    "    X_total = pd.concat([X_train_medoid, X_minority])\n",
    "    X_total = X_total.sample(frac=1)\n",
    "    X = X_total.drop(columns=event_col)\n",
    "    X = X_total.drop(columns=['majority_mask'])\n",
    "    y = X_total[event_col].values\n",
    "    \n",
    "    return X, y, X_remain, y_remain\n",
    "\n",
    "def recursive_clustering(model, X, y, duration_col, event_col, feature_col, params, val, callbacks, max_repeats):\n",
    "    remaining_data = X.copy()\n",
    "    remaining_y = y.copy()\n",
    "    repeat_count = 0\n",
    "    logs = []\n",
    "    while len(remaining_data) > 0 and repeat_count < max_repeats:\n",
    "        print(f\"Performing clustering iteration {repeat_count}\")\n",
    "        X_cluster, y_cluster, remaining_data, remaining_y = define_medoid(remaining_data, remaining_y, duration_col, event_col)\n",
    "        \n",
    "        X_train_cluster_tensor, y_train_cluster_tensor = prepare_tensor(X_cluster, y_cluster, feature_col=feature_col, duration_col=duration_col)\n",
    "        lrfinder = model.lr_finder(X_train_cluster_tensor, y_train_cluster_tensor, params['batch_size'], tolerance=10)\n",
    "        model.optimizer.set_lr(lrfinder.get_best_lr())\n",
    "        log = model.fit(X_train_cluster_tensor, y_train_cluster_tensor, params['batch_size'], params['epochs'], callbacks, verbose=True, val_data=val)\n",
    "        \n",
    "        logs.append(log)\n",
    "        repeat_count += 1\n",
    "        gc.collect()\n",
    "\n",
    "        # Stop if there are no more majority samples left to cluster\n",
    "        if remaining_data.empty:\n",
    "            break\n",
    "\n",
    "    return model, logs\n",
    "\n",
    "model_cluster = model_init(X[feature_col], params=params)\n",
    "model_cluster, log_cluster = recursive_clustering(model=model_cluster, X=X_train, y=y_train_event, \n",
    "                                                  duration_col=duration_col,\n",
    "                                                  event_col=event_col, feature_col=feature_col, params=params,\n",
    "                                                  val=val, callbacks=callbacks, max_repeats=10)\n",
    "gc.collect()\n",
    "# X_cluster, y_event_cluster, X_cluster_remain = define_medoid(X_train, y_train_event, duration_col=duration_col, event_col=event_col)\n",
    "# X_train_cluster_tensor, y_train_cluster_tensor = prepare_tensor(X_cluster, y_event_cluster, feature_col=feature_col, duration_col=duration_col)\n",
    "# log_cluster = model_cluster.fit(X_train_cluster_tensor, y_train_cluster_tensor, params['batch_size'], params['epochs'], callbacks, verbose=True, val_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35605"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_cluster.compute_baseline_hazards())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[4, 5, 6, 8, 9, 10, 14, 15, 17, 18, 19, 21, 23, 25, 26, 27, 30, 31, 32, 33, 34, 38, 39, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 60, 61, 63, 64, 66, 67, 71, 72, 75, 76, 77, 78, 82, 86, 90, 102, 114, 121, 122, 124, 136, 138, 142, 144, 147, 150, 151, 154, 155, 157, 158, 162, 163, 164, 172, 184, 185, 186, 187, 188, 190, 193, 194, 201, 205, 213, 215, 226, 232, 235, 237, 238, 240, 243, 246, 247, 253, 256, 258, 261, 262, 265, 267, 269, 270, 272, 276, 277, 278, 282, 291, 297, 308, 309, 311, 314, 319, 321, 325, 326, 330, 338, 340, 341, 342, 347, 349, 351, 352, 353, 354, 355, 356, 360, 362, 364, 365, 366, 372, 373, 374, 375, 378, 379, 380, 387, 390, 391, 392, 395, 402, 404, 407, 410, 413, 415, 416, 418, 422, 423, 425, 432, 436, 438, 443, 445, 452, 453, 460, 463, 465, 467, 468, 470, 471, 472, 473, 479, 480, 481, 486, 487, 488, 490, 491, 493, 498, 501, 502, 504, 506, 507, 508, 509, 511, 514, 515, 517, 518, 520, 522, 523, 527, 542, 543, 545, 547, 548, 549, 551, 557, 560, 563, 564, 566, 567, 571, 574, 575, 577, 580, 582, 583, 585, 587, 588, 590, 591, 593, 594, 596, 600, 605, 606, 607, 610, 611, 612, 613, 614, 615, 617, 618, 619, 620, 624, 625, 626, 627, 628, 631, 633, 635, 637, 641, 642, 647, 648, 650, 651, 652, 653, 654, 655, 658, 660, 662, 665, 666, 667, 668, 669, 673, 674, 675, 678, 681, 684, 688, 689, 690, 693, 694, 696, 698, 700, 708, 713, 726, 727, 729, 730, 732, 735, 736, 737, 739, 743, 745, 747, 748, 751, 752, 753, 754, 760, 766, 767, 768, 775, 778, 779, 780, 781, 786, 789, 792, 793, 794, 795, 796, 802, 803, 804, 805, 808, 811, 812, 813, 816, 820, 821, 822, 825, 827, 829, 831, 833, 836, 837, 839, 843, 846, 848, 849, 850, 859, 860, 864, 865, 867, 868, 873, 874, 878, 879, 880, 881, 882, 883, 884, 885, 888, 890, 891, 892, 893, 895, 897, 898, 904, 906, 908, 911, 914, 916, 917, 920, 921, 922, 923, 925, 926, 928, 930, 931, 932, 933, 935, 939, 941, 943, 947, 948, 950, 955, 956, 959, 962, 964, 967, 969, 970, 972, 975, 976, 978, 979, 980, 982, 984, 985, 987, 988, 994, 999, 1003, 1004, 1005, 1009, 1010, 1012, 1013, 1016, 1017, 1018, 1019, 1021, 1025, 1026, 1029, 1032, 1035, 1036, 1037, 1041, 1042, 1045, 1046, 1048, 1049, 1051, 1054, 1056, 1058, 1059, 1062, 1064, 1065, 1066, 1068, 1069, 1070, 1071, 1074, 1078, 1082, 1083, 1086, 1088, 1089, 1090, 1093, 1094, 1095, 1096, 1098, 1101, 1103, 1104, 1106, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1118, 1119, 1121, 1122, 1126, 1129, 1130, 1132, 1134, 1136, 1139, 1140, 1141, 1142, 1143, 1146, 1147, 1148, 1151, 1152, 1156, 1157, 1159, 1160, 1166, 1167, 1169, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1183, 1185, 1186, 1191, 1192, 1194, 1198, 1208, 1209, 1210, 1211, 1213, 1215, 1217, 1219, 1221, 1222, 1225, 1226, 1227, 1229, 1235, 1236, 1237, 1238, 1240, 1241, 1244, 1247, 1248, 1249, 1253, 1255, 1258, 1259, 1263, 1268, 1269, 1270, 1271, 1273, 1274, 1275, 1276, 1277, 1278, 1282, 1283, 1284, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1296, 1299, 1300, 1301, 1302, 1303, 1305, 1307, 1309, 1310, 1312, 1313, 1315, 1316, 1317, 1319, 1320, 1321, 1322, 1323, 1325, 1326, 1327, 1328, 1331, 1332, 1336, 1338, 1339, 1342, 1344, 1347, 1349, 1350, 1353, 1354, 1356, 1358, 1359, 1360, 1361, 1362, 1364, 1365, 1366, 1367, 1369, 1372, 1373, 1374, 1375, 1376, 1379, 1380, 1381, 1382, 1384, 1385, 1387, 1388, 1389, 1391, 1393, 1395, 1396, 1397, 1398, 1399, 1400, 1402, 1406, 1408, 1410, 1411, 1412, 1413, 1417, 1420, 1423, 1425, 1426, 1427, 1428, 1429, 1430, 1432, 1434, 1435, 1437, 1439, 1442, 1444, 1447, 1449, 1451, 1457, 1458, 1459, 1460, 1462, 1463, 1464, 1467, 1472, 1475, 1478, 1479, 1481, 1483, 1484, 1486, 1487, 1490, 1493, 1495, 1496, 1500, 1502, 1506, 1507, 1508, 1509, 1510, 1511, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1523, 1526, 1527, 1530, 1532, 1535, 1536, 1537, 1539, 1541, 1546, 1547, 1550, 1551, 1552, 1553, 1555, 1557, 1558, 1559, 1560, 1563, 1565, 1566, 1569, 1570, 1574, 1576, 1577, 1581, 1584, 1586, 1587, 1588, 1591, 1593, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1605, 1606, 1607, 1611, 1612, 1615, 1619, 1621, 1622, 1623, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1638, 1642, 1644, 1647, 1648, 1649, 1650, 1653, 1654, 1658, 1659, 1662, 1663, 1666, 1668, 1669, 1672, 1673, 1675, 1676, 1678, 1679, 1680, 1682, 1683, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1694, 1695, 1697, 1698, 1700, 1701, 1702, 1703, 1705, 1707, 1709, 1713, 1714, 1716, 1719, 1720, 1724, 1725, 1726, 1728, 1729, 1730, 1732, 1733, 1734, 1735, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1751, 1753, 1754, 1755, 1759, 1762, 1763, 1765, 1766, 1769, 1770, 1773, 1774, 1775, 1776, 1777, 1779, 1780, 1783, 1785, 1786, 1788, 1789, 1791, 1793, 1795, 1796, 1798, 1800, 1801, 1804, 1809, 1810, 1816, 1817, 1819, 1822, 1823, 1824] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 42\u001b[0m\n\u001b[1;32m     36\u001b[0m results_smotetomek \u001b[38;5;241m=\u001b[39m [process_test_batch(X_batch\u001b[38;5;241m=\u001b[39mX_batch, y_batch\u001b[38;5;241m=\u001b[39my_batch, model\u001b[38;5;241m=\u001b[39mmodel_smotetomek,\n\u001b[1;32m     37\u001b[0m                                          max_follow_up_time\u001b[38;5;241m=\u001b[39mmax_follow_up_time,\n\u001b[1;32m     38\u001b[0m                                          time_points\u001b[38;5;241m=\u001b[39mtime_points)\n\u001b[1;32m     39\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_batches, y_batches)] \n\u001b[1;32m     40\u001b[0m final_results_smotetomek \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results_smotetomek)\n\u001b[0;32m---> 42\u001b[0m results_cluster \u001b[38;5;241m=\u001b[39m [process_test_batch(X_batch\u001b[38;5;241m=\u001b[39mX_batch, y_batch\u001b[38;5;241m=\u001b[39my_batch, model\u001b[38;5;241m=\u001b[39mmodel_cluster, \n\u001b[1;32m     43\u001b[0m                                       max_follow_up_time\u001b[38;5;241m=\u001b[39mmax_follow_up_time,\n\u001b[1;32m     44\u001b[0m                                       time_points\u001b[38;5;241m=\u001b[39mtime_points)\n\u001b[1;32m     45\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_batches, y_batches)]\n\u001b[1;32m     46\u001b[0m final_results_cluster \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results_cluster)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Process each batch and collect the results\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# results = [process_test_batch(X_batch=X_batches, model=model_smoteenn) for batch in batches]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[182], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m results_smotetomek \u001b[38;5;241m=\u001b[39m [process_test_batch(X_batch\u001b[38;5;241m=\u001b[39mX_batch, y_batch\u001b[38;5;241m=\u001b[39my_batch, model\u001b[38;5;241m=\u001b[39mmodel_smotetomek,\n\u001b[1;32m     37\u001b[0m                                          max_follow_up_time\u001b[38;5;241m=\u001b[39mmax_follow_up_time,\n\u001b[1;32m     38\u001b[0m                                          time_points\u001b[38;5;241m=\u001b[39mtime_points)\n\u001b[1;32m     39\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_batches, y_batches)] \n\u001b[1;32m     40\u001b[0m final_results_smotetomek \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results_smotetomek)\n\u001b[0;32m---> 42\u001b[0m results_cluster \u001b[38;5;241m=\u001b[39m [\u001b[43mprocess_test_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mmax_follow_up_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_follow_up_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtime_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_batches, y_batches)]\n\u001b[1;32m     46\u001b[0m final_results_cluster \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results_cluster)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Process each batch and collect the results\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# results = [process_test_batch(X_batch=X_batches, model=model_smoteenn) for batch in batches]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[182], line 18\u001b[0m, in \u001b[0;36mprocess_test_batch\u001b[0;34m(X_batch, y_batch, model, max_follow_up_time, time_points, feature_col, duration_col)\u001b[0m\n\u001b[1;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_surv_df(X_test_batch_tensor, max_duration\u001b[38;5;241m=\u001b[39mmax_follow_up_time)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Convert predictions to DataFrame and match time points\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# y_pred_df = pd.DataFrame(y_pred.T, index=np.arange(y_pred.shape[1]))\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtime_points\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     20\u001b[0m y_pred\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m original_indices\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '[4, 5, 6, 8, 9, 10, 14, 15, 17, 18, 19, 21, 23, 25, 26, 27, 30, 31, 32, 33, 34, 38, 39, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 60, 61, 63, 64, 66, 67, 71, 72, 75, 76, 77, 78, 82, 86, 90, 102, 114, 121, 122, 124, 136, 138, 142, 144, 147, 150, 151, 154, 155, 157, 158, 162, 163, 164, 172, 184, 185, 186, 187, 188, 190, 193, 194, 201, 205, 213, 215, 226, 232, 235, 237, 238, 240, 243, 246, 247, 253, 256, 258, 261, 262, 265, 267, 269, 270, 272, 276, 277, 278, 282, 291, 297, 308, 309, 311, 314, 319, 321, 325, 326, 330, 338, 340, 341, 342, 347, 349, 351, 352, 353, 354, 355, 356, 360, 362, 364, 365, 366, 372, 373, 374, 375, 378, 379, 380, 387, 390, 391, 392, 395, 402, 404, 407, 410, 413, 415, 416, 418, 422, 423, 425, 432, 436, 438, 443, 445, 452, 453, 460, 463, 465, 467, 468, 470, 471, 472, 473, 479, 480, 481, 486, 487, 488, 490, 491, 493, 498, 501, 502, 504, 506, 507, 508, 509, 511, 514, 515, 517, 518, 520, 522, 523, 527, 542, 543, 545, 547, 548, 549, 551, 557, 560, 563, 564, 566, 567, 571, 574, 575, 577, 580, 582, 583, 585, 587, 588, 590, 591, 593, 594, 596, 600, 605, 606, 607, 610, 611, 612, 613, 614, 615, 617, 618, 619, 620, 624, 625, 626, 627, 628, 631, 633, 635, 637, 641, 642, 647, 648, 650, 651, 652, 653, 654, 655, 658, 660, 662, 665, 666, 667, 668, 669, 673, 674, 675, 678, 681, 684, 688, 689, 690, 693, 694, 696, 698, 700, 708, 713, 726, 727, 729, 730, 732, 735, 736, 737, 739, 743, 745, 747, 748, 751, 752, 753, 754, 760, 766, 767, 768, 775, 778, 779, 780, 781, 786, 789, 792, 793, 794, 795, 796, 802, 803, 804, 805, 808, 811, 812, 813, 816, 820, 821, 822, 825, 827, 829, 831, 833, 836, 837, 839, 843, 846, 848, 849, 850, 859, 860, 864, 865, 867, 868, 873, 874, 878, 879, 880, 881, 882, 883, 884, 885, 888, 890, 891, 892, 893, 895, 897, 898, 904, 906, 908, 911, 914, 916, 917, 920, 921, 922, 923, 925, 926, 928, 930, 931, 932, 933, 935, 939, 941, 943, 947, 948, 950, 955, 956, 959, 962, 964, 967, 969, 970, 972, 975, 976, 978, 979, 980, 982, 984, 985, 987, 988, 994, 999, 1003, 1004, 1005, 1009, 1010, 1012, 1013, 1016, 1017, 1018, 1019, 1021, 1025, 1026, 1029, 1032, 1035, 1036, 1037, 1041, 1042, 1045, 1046, 1048, 1049, 1051, 1054, 1056, 1058, 1059, 1062, 1064, 1065, 1066, 1068, 1069, 1070, 1071, 1074, 1078, 1082, 1083, 1086, 1088, 1089, 1090, 1093, 1094, 1095, 1096, 1098, 1101, 1103, 1104, 1106, 1107, 1108, 1110, 1111, 1113, 1114, 1116, 1118, 1119, 1121, 1122, 1126, 1129, 1130, 1132, 1134, 1136, 1139, 1140, 1141, 1142, 1143, 1146, 1147, 1148, 1151, 1152, 1156, 1157, 1159, 1160, 1166, 1167, 1169, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1183, 1185, 1186, 1191, 1192, 1194, 1198, 1208, 1209, 1210, 1211, 1213, 1215, 1217, 1219, 1221, 1222, 1225, 1226, 1227, 1229, 1235, 1236, 1237, 1238, 1240, 1241, 1244, 1247, 1248, 1249, 1253, 1255, 1258, 1259, 1263, 1268, 1269, 1270, 1271, 1273, 1274, 1275, 1276, 1277, 1278, 1282, 1283, 1284, 1285, 1286, 1288, 1289, 1290, 1292, 1293, 1296, 1299, 1300, 1301, 1302, 1303, 1305, 1307, 1309, 1310, 1312, 1313, 1315, 1316, 1317, 1319, 1320, 1321, 1322, 1323, 1325, 1326, 1327, 1328, 1331, 1332, 1336, 1338, 1339, 1342, 1344, 1347, 1349, 1350, 1353, 1354, 1356, 1358, 1359, 1360, 1361, 1362, 1364, 1365, 1366, 1367, 1369, 1372, 1373, 1374, 1375, 1376, 1379, 1380, 1381, 1382, 1384, 1385, 1387, 1388, 1389, 1391, 1393, 1395, 1396, 1397, 1398, 1399, 1400, 1402, 1406, 1408, 1410, 1411, 1412, 1413, 1417, 1420, 1423, 1425, 1426, 1427, 1428, 1429, 1430, 1432, 1434, 1435, 1437, 1439, 1442, 1444, 1447, 1449, 1451, 1457, 1458, 1459, 1460, 1462, 1463, 1464, 1467, 1472, 1475, 1478, 1479, 1481, 1483, 1484, 1486, 1487, 1490, 1493, 1495, 1496, 1500, 1502, 1506, 1507, 1508, 1509, 1510, 1511, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1523, 1526, 1527, 1530, 1532, 1535, 1536, 1537, 1539, 1541, 1546, 1547, 1550, 1551, 1552, 1553, 1555, 1557, 1558, 1559, 1560, 1563, 1565, 1566, 1569, 1570, 1574, 1576, 1577, 1581, 1584, 1586, 1587, 1588, 1591, 1593, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1605, 1606, 1607, 1611, 1612, 1615, 1619, 1621, 1622, 1623, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1638, 1642, 1644, 1647, 1648, 1649, 1650, 1653, 1654, 1658, 1659, 1662, 1663, 1666, 1668, 1669, 1672, 1673, 1675, 1676, 1678, 1679, 1680, 1682, 1683, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1694, 1695, 1697, 1698, 1700, 1701, 1702, 1703, 1705, 1707, 1709, 1713, 1714, 1716, 1719, 1720, 1724, 1725, 1726, 1728, 1729, 1730, 1732, 1733, 1734, 1735, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1751, 1753, 1754, 1755, 1759, 1762, 1763, 1765, 1766, 1769, 1770, 1773, 1774, 1775, 1776, 1777, 1779, 1780, 1783, 1785, 1786, 1788, 1789, 1791, 1793, 1795, 1796, 1798, 1800, 1801, 1804, 1809, 1810, 1816, 1817, 1819, 1822, 1823, 1824] not in index'"
     ]
    }
   ],
   "source": [
    "def process_test_batch(X_batch, y_batch, model, max_follow_up_time, time_points, feature_col=feature_col, duration_col=duration_col,):\n",
    "    original_indices = X_batch.index\n",
    "    # Transform and prepare tensors from the batch\n",
    "    X_batch.loc[:, feature_col] = X_batch[feature_col].apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)\n",
    "    X_test_batch = X_batch[feature_col + duration_col].copy()\n",
    "    y_test_event_batch = y_batch\n",
    "    X_test_batch_tensor, y_test_batch_tensor = prepare_tensor(X_test_batch, y_test_event_batch,\n",
    "                                                        feature_col=feature_col, \n",
    "                                                        duration_col=duration_col)\n",
    "    \n",
    "    # max_follow_up_time = int(min(1825, X_batch_prepared[duration_col].values.max()))\n",
    "    # time_points = np.linspace(0, max_follow_up_time, int(max_follow_up_time), dtype='int')\n",
    "    # Predict\n",
    "    y_pred = model.predict_surv_df(X_test_batch_tensor, max_duration=max_follow_up_time)\n",
    "    \n",
    "    # Convert predictions to DataFrame and match time points\n",
    "    # y_pred_df = pd.DataFrame(y_pred.T, index=np.arange(y_pred.shape[1]))\n",
    "    y_pred = y_pred.loc[time_points]\n",
    "    y_pred = y_pred.T\n",
    "    y_pred.index = original_indices\n",
    "    return y_pred\n",
    "\n",
    "n = 200  # Batch size\n",
    "# y_fin_val = (X_fin_val[event_col] == 1).astype(int).values\n",
    "# X_fin_val = X_fin_val[feature_col + duration_col].copy()\n",
    "max_follow_up_time = int(min(1825, X_fin_val[duration_col].values.max()))\n",
    "time_points = [i for i in range(0, max_follow_up_time)]\n",
    "X_batches = [X_fin_val.iloc[i:i + n] for i in range(0, X_fin_val.shape[0], n)]\n",
    "y_batches = [y_fin_val[i:i + n] for i in range(0, y_fin_val.shape[0], n)]\n",
    "results_smoteenn = [process_test_batch(X_batch=X_batch, y_batch=y_batch, model=model_smoteenn,\n",
    "                                       max_follow_up_time=max_follow_up_time,\n",
    "                                       time_points=time_points)\n",
    "                    for X_batch, y_batch in zip(X_batches, y_batches)]\n",
    "final_results_smoteenn = pd.concat(results_smoteenn)\n",
    "\n",
    "results_smotetomek = [process_test_batch(X_batch=X_batch, y_batch=y_batch, model=model_smotetomek,\n",
    "                                         max_follow_up_time=max_follow_up_time,\n",
    "                                         time_points=time_points)\n",
    "                      for X_batch, y_batch in zip(X_batches, y_batches)] \n",
    "final_results_smotetomek = pd.concat(results_smotetomek)\n",
    "\n",
    "results_cluster = [process_test_batch(X_batch=X_batch, y_batch=y_batch, model=model_cluster, \n",
    "                                      max_follow_up_time=max_follow_up_time,\n",
    "                                      time_points=time_points)\n",
    "                   for X_batch, y_batch in zip(X_batches, y_batches)]\n",
    "final_results_cluster = pd.concat(results_cluster)\n",
    "# Process each batch and collect the results\n",
    "# results = [process_test_batch(X_batch=X_batches, model=model_smoteenn) for batch in batches]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfinder = model.lr_finder(X_batch, y_train, batch_size, tolerance=10)\n",
    "_ = lrfinder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Lengths must match to compare', (961,), (1825,))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m y_test_event_batch \u001b[38;5;241m=\u001b[39m y_batch\n\u001b[1;32m      9\u001b[0m X_test_batch_tensor, y_test_batch_tensor \u001b[38;5;241m=\u001b[39m prepare_tensor(X_test_batch, y_test_event_batch,\n\u001b[1;32m     10\u001b[0m                                                     feature_col\u001b[38;5;241m=\u001b[39mfeature_col, \n\u001b[1;32m     11\u001b[0m                                                     duration_col\u001b[38;5;241m=\u001b[39mduration_col)\n\u001b[0;32m---> 12\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_surv_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_batch_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model_cluster.predict_surv\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# results_cluster = [process_test_batch(X_batch=X_batch, y_batch=y_batch, model=model_cluster, \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#                                       max_follow_up_time=max_follow_up_time,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#                                       time_points=time_points)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#                    for X_batch, y_batch in zip(X_batches, y_batches)]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# final_results_cluster = pd.concat(results_cluster)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pycox/models/cox.py:154\u001b[0m, in \u001b[0;36m_CoxBase.predict_surv_df\u001b[0;34m(self, input, max_duration, batch_size, verbose, baseline_hazards_, eval_, num_workers)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_surv_df\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, max_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8224\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, baseline_hazards_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m                     eval_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict survival function for `input`. S(x, t) = exp(-H(x, t))\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    Require computed baseline hazards.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m        pd.DataFrame -- Survival estimates. One columns for each individual.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_cumulative_hazards\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_hazards_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43meval_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pycox/models/cox.py:129\u001b[0m, in \u001b[0;36m_CoxBase.predict_cumulative_hazards\u001b[0;34m(self, input, max_duration, batch_size, verbose, baseline_hazards_, eval_, num_workers)\u001b[0m\n\u001b[1;32m    126\u001b[0m     baseline_hazards_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbaseline_hazards_\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m baseline_hazards_\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_monotonic_increasing,\\\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeed index of baseline_hazards_ to be monotonic increasing, as it represents time.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_cumulative_hazards\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_hazards_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43meval_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pycox/models/cox.py:250\u001b[0m, in \u001b[0;36m_CoxPHBase._predict_cumulative_hazards\u001b[0;34m(self, input, max_duration, batch_size, verbose, baseline_hazards_, eval_, num_workers)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     bch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_baseline_cumulative_hazards(set_hazards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    249\u001b[0m                                                    baseline_hazards_\u001b[38;5;241m=\u001b[39mbaseline_hazards_)\n\u001b[0;32m--> 250\u001b[0m bch \u001b[38;5;241m=\u001b[39m \u001b[43mbch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_duration\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    251\u001b[0m expg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m, batch_size, \u001b[38;5;28;01mTrue\u001b[39;00m, eval_, num_workers\u001b[38;5;241m=\u001b[39mnum_workers))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(bch\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdot(expg), \n\u001b[1;32m    253\u001b[0m                     index\u001b[38;5;241m=\u001b[39mbch\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/indexing.py:1189\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1189\u001b[0m     maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_if_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m     maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/common.py:384\u001b[0m, in \u001b[0;36mapply_if_callable\u001b[0;34m(maybe_callable, obj, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03mEvaluate possibly callable input using obj and kwargs if it is callable,\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03motherwise return as it is.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m**kwargs\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(maybe_callable):\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmaybe_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_callable\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pycox/models/cox.py:250\u001b[0m, in \u001b[0;36m_CoxPHBase._predict_cumulative_hazards.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     bch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_baseline_cumulative_hazards(set_hazards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    249\u001b[0m                                                    baseline_hazards_\u001b[38;5;241m=\u001b[39mbaseline_hazards_)\n\u001b[0;32m--> 250\u001b[0m bch \u001b[38;5;241m=\u001b[39m bch\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_duration\u001b[49m]\n\u001b[1;32m    251\u001b[0m expg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m, batch_size, \u001b[38;5;28;01mTrue\u001b[39;00m, eval_, num_workers\u001b[38;5;241m=\u001b[39mnum_workers))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(bch\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdot(expg), \n\u001b[1;32m    253\u001b[0m                     index\u001b[38;5;241m=\u001b[39mbch\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/arraylike.py:52\u001b[0m, in \u001b[0;36mOpsMixin.__le__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__le__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__le__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/indexes/base.py:7204\u001b[0m, in \u001b[0;36mIndex._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   7201\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomp_method_OBJECT_ARRAY(op, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, other)\n\u001b[1;32m   7203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7204\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:321\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, (np\u001b[38;5;241m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# TODO: make this treatment consistent across ops and classes.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m#  We are not catching all listlikes here (e.g. frozenset, tuple)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m#  The ambiguous case is object-dtype.  See GH#27803\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lvalues) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rvalues):\n\u001b[0;32m--> 321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    322\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    323\u001b[0m         )\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    326\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    328\u001b[0m ):\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[1;32m    330\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m op(lvalues, rvalues)\n",
      "\u001b[0;31mValueError\u001b[0m: ('Lengths must match to compare', (961,), (1825,))"
     ]
    }
   ],
   "source": [
    "X_batch = X_fin_val.iloc[0:200]\n",
    "y_batch = y_fin_val[0:200]\n",
    "max_follow_up_time = int(min(1825, X_fin_val[duration_col].values.max()))\n",
    "time_points = [i for i in range(0, max_follow_up_time)]\n",
    "\n",
    "X_batch.loc[:, feature_col] = X_batch[feature_col].apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)\n",
    "X_test_batch = X_batch[feature_col + duration_col].copy()\n",
    "y_test_event_batch = y_batch\n",
    "X_test_batch_tensor, y_test_batch_tensor = prepare_tensor(X_test_batch, y_test_event_batch,\n",
    "                                                    feature_col=feature_col, \n",
    "                                                    duration_col=duration_col)\n",
    "y_pred = model_cluster.predict_surv_df(X_test_batch_tensor, max_duration=time_points)\n",
    "# model_cluster.predict_surv\n",
    "# results_cluster = [process_test_batch(X_batch=X_batch, y_batch=y_batch, model=model_cluster, \n",
    "#                                       max_follow_up_time=max_follow_up_time,\n",
    "#                                       time_points=time_points)\n",
    "#                    for X_batch, y_batch in zip(X_batches, y_batches)]\n",
    "# final_results_cluster = pd.concat(results_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814.0</th>\n",
       "      <td>0.924087</td>\n",
       "      <td>0.86031</td>\n",
       "      <td>0.957543</td>\n",
       "      <td>0.980553</td>\n",
       "      <td>0.958949</td>\n",
       "      <td>0.951138</td>\n",
       "      <td>0.958681</td>\n",
       "      <td>0.969423</td>\n",
       "      <td>0.957123</td>\n",
       "      <td>0.951428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938489</td>\n",
       "      <td>0.91261</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.949537</td>\n",
       "      <td>0.958695</td>\n",
       "      <td>0.952072</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>0.938854</td>\n",
       "      <td>0.961965</td>\n",
       "      <td>0.942656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815.0</th>\n",
       "      <td>0.924087</td>\n",
       "      <td>0.86031</td>\n",
       "      <td>0.957543</td>\n",
       "      <td>0.980553</td>\n",
       "      <td>0.958949</td>\n",
       "      <td>0.951138</td>\n",
       "      <td>0.958681</td>\n",
       "      <td>0.969423</td>\n",
       "      <td>0.957123</td>\n",
       "      <td>0.951428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938489</td>\n",
       "      <td>0.91261</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.949537</td>\n",
       "      <td>0.958695</td>\n",
       "      <td>0.952072</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>0.938854</td>\n",
       "      <td>0.961965</td>\n",
       "      <td>0.942656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818.0</th>\n",
       "      <td>0.924087</td>\n",
       "      <td>0.86031</td>\n",
       "      <td>0.957543</td>\n",
       "      <td>0.980553</td>\n",
       "      <td>0.958949</td>\n",
       "      <td>0.951138</td>\n",
       "      <td>0.958681</td>\n",
       "      <td>0.969423</td>\n",
       "      <td>0.957123</td>\n",
       "      <td>0.951428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938489</td>\n",
       "      <td>0.91261</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.949537</td>\n",
       "      <td>0.958695</td>\n",
       "      <td>0.952072</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>0.938854</td>\n",
       "      <td>0.961965</td>\n",
       "      <td>0.942656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820.0</th>\n",
       "      <td>0.924087</td>\n",
       "      <td>0.86031</td>\n",
       "      <td>0.957543</td>\n",
       "      <td>0.980553</td>\n",
       "      <td>0.958949</td>\n",
       "      <td>0.951138</td>\n",
       "      <td>0.958681</td>\n",
       "      <td>0.969423</td>\n",
       "      <td>0.957123</td>\n",
       "      <td>0.951428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938489</td>\n",
       "      <td>0.91261</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.949537</td>\n",
       "      <td>0.958695</td>\n",
       "      <td>0.952072</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>0.938854</td>\n",
       "      <td>0.961965</td>\n",
       "      <td>0.942656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821.0</th>\n",
       "      <td>0.924087</td>\n",
       "      <td>0.86031</td>\n",
       "      <td>0.957543</td>\n",
       "      <td>0.980553</td>\n",
       "      <td>0.958949</td>\n",
       "      <td>0.951138</td>\n",
       "      <td>0.958681</td>\n",
       "      <td>0.969423</td>\n",
       "      <td>0.957123</td>\n",
       "      <td>0.951428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938489</td>\n",
       "      <td>0.91261</td>\n",
       "      <td>0.941571</td>\n",
       "      <td>0.949537</td>\n",
       "      <td>0.958695</td>\n",
       "      <td>0.952072</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>0.938854</td>\n",
       "      <td>0.961965</td>\n",
       "      <td>0.942656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>961 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        1         2         3         4         5         6    \\\n",
       "duration                                                                        \n",
       "0.0       1.000000  1.00000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "1.0       1.000000  1.00000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "2.0       1.000000  1.00000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "3.0       1.000000  1.00000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "7.0       1.000000  1.00000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "...            ...      ...       ...       ...       ...       ...       ...   \n",
       "1814.0    0.924087  0.86031  0.957543  0.980553  0.958949  0.951138  0.958681   \n",
       "1815.0    0.924087  0.86031  0.957543  0.980553  0.958949  0.951138  0.958681   \n",
       "1818.0    0.924087  0.86031  0.957543  0.980553  0.958949  0.951138  0.958681   \n",
       "1820.0    0.924087  0.86031  0.957543  0.980553  0.958949  0.951138  0.958681   \n",
       "1821.0    0.924087  0.86031  0.957543  0.980553  0.958949  0.951138  0.958681   \n",
       "\n",
       "               7         8         9    ...       190      191       192  \\\n",
       "duration                                ...                                \n",
       "0.0       1.000000  1.000000  1.000000  ...  1.000000  1.00000  1.000000   \n",
       "1.0       1.000000  1.000000  1.000000  ...  1.000000  1.00000  1.000000   \n",
       "2.0       1.000000  1.000000  1.000000  ...  1.000000  1.00000  1.000000   \n",
       "3.0       1.000000  1.000000  1.000000  ...  1.000000  1.00000  1.000000   \n",
       "7.0       1.000000  1.000000  1.000000  ...  1.000000  1.00000  1.000000   \n",
       "...            ...       ...       ...  ...       ...      ...       ...   \n",
       "1814.0    0.969423  0.957123  0.951428  ...  0.938489  0.91261  0.941571   \n",
       "1815.0    0.969423  0.957123  0.951428  ...  0.938489  0.91261  0.941571   \n",
       "1818.0    0.969423  0.957123  0.951428  ...  0.938489  0.91261  0.941571   \n",
       "1820.0    0.969423  0.957123  0.951428  ...  0.938489  0.91261  0.941571   \n",
       "1821.0    0.969423  0.957123  0.951428  ...  0.938489  0.91261  0.941571   \n",
       "\n",
       "               193       194       195       196       197       198       199  \n",
       "duration                                                                        \n",
       "0.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "1.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "2.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "3.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "7.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "...            ...       ...       ...       ...       ...       ...       ...  \n",
       "1814.0    0.949537  0.958695  0.952072  0.974157  0.938854  0.961965  0.942656  \n",
       "1815.0    0.949537  0.958695  0.952072  0.974157  0.938854  0.961965  0.942656  \n",
       "1818.0    0.949537  0.958695  0.952072  0.974157  0.938854  0.961965  0.942656  \n",
       "1820.0    0.949537  0.958695  0.952072  0.974157  0.938854  0.961965  0.942656  \n",
       "1821.0    0.949537  0.958695  0.952072  0.974157  0.938854  0.961965  0.942656  \n",
       "\n",
       "[961 rows x 200 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "n = 200  # Batch size\n",
    "batches = [X_test_transformed.iloc[i:i + n] for i in range(0, X_test_transformed.shape[0], n)]\n",
    "# Process each batch and collect the results\n",
    "results = [process_batch(batch) for batch in batches]\n",
    "# Concatenate all results into a single DataFrame\n",
    "final_results = pd.concat(results)\n",
    "# X_test_transformed[feature_col] = X_test_transformed[feature_col].apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)\n",
    "# X_test = X_test_transformed[feature_col + duration_col].copy()\n",
    "# y_test_event = (X_test_transformed[event_col] == 1).astype(int).values\n",
    "# X_test_tensor, y_test_event_tensor = prepare_tensor(X_test, y_test_event, feature_col=feature_col, duration_col=duration_col)\n",
    "\n",
    "\n",
    "# y_pred = model.predict_surv(X_test_tensor, max_duration=max_follow_up_time)\n",
    "# # Convert y_pred to DataFrame and match time points\n",
    "# y_pred_df = pd.DataFrame(y_pred.T, index=np.arange(y_pred.shape[1]))\n",
    "# # Ensure y_pred is a DataFrame and time points match\n",
    "# y_pred_df = y_pred_df.loc[time_points]\n",
    "\n",
    "# integrated_brier = integrated_brier_score(y_test_surv, y_test_surv, y_pred[:, :-2], time_points[:-1])\n",
    "# display(integrated_brier)\n",
    "\n",
    "# risk_scores = -np.log(y_pred[:, :int(max_follow_up_time)])\n",
    "# cidx = concordance_index_censored(y_test_event_truncated.astype(bool), y_test_duration_truncated, risk_scores.mean(axis=1))[0]\n",
    "# display(cidx)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024053286495431304"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_duration_truncated = np.minimum(X_test[duration_col].values, max_follow_up_time)\n",
    "y_test_event = (X_test_transformed[event_col] == 1).astype(int).values\n",
    "y_test_event_truncated = np.where(X_test[duration_col].values > max_follow_up_time, 0, y_test_event)\n",
    "y_test_surv = Surv.from_arrays(event=y_test_event_truncated.reshape(-1), time=y_test_duration_truncated.reshape(-1))\n",
    "integrated_brier = integrated_brier_score(y_test_surv, y_test_surv, final_results.iloc[:, :-1], time_points[:-1])\n",
    "display(integrated_brier)\n",
    "\n",
    "# roc_auc = cumulative_dynamic_auc(y_test_surv, y_test_surv, final_results.iloc[:, :-1], time_points[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan, 0.81024565, 0.84366161, 0.8583991 , 0.85975026])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# roc_auc[0][]\n",
    "days_in_yr = 365.25\n",
    "max_predict_yr = 5\n",
    "time_points = [round(days_in_yr * i) for i in range(max_predict_yr)]\n",
    "roc_auc[0][time_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     99\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m--> 100\u001b[0m best_params, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_event_rrt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m random_search(X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    102\u001b[0m               y_event\u001b[38;5;241m=\u001b[39my_event_rrt,\n\u001b[1;32m    103\u001b[0m               y_duration\u001b[38;5;241m=\u001b[39my_duration,\n\u001b[1;32m    104\u001b[0m               param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions,\n\u001b[1;32m    105\u001b[0m               n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[1;32m    106\u001b[0m               n_splits\u001b[38;5;241m=\u001b[39mn_splits)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Train final model using best parameters\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[113], line 45\u001b[0m, in \u001b[0;36mrandom_search\u001b[0;34m(X, y_event, y_duration, param_distributions, n_iter, n_splits, weight_brier, weight_cindex)\u001b[0m\n\u001b[1;32m     42\u001b[0m val \u001b[38;5;241m=\u001b[39m X_val_tensor, y_val_tensor\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Ensure all tensors have the same length\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_train_tensor) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_train_tensor[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_train_tensor[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_val_tensor) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_val_tensor[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_val_tensor[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     48\u001b[0m log \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_tensor, y_train_tensor, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m], callbacks, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, val_data\u001b[38;5;241m=\u001b[39mval)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to preprocess the data\n",
    "def preprocess_data(df, features):\n",
    "    df[features] = df[features].apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)\n",
    "    return df[features].values.astype(float), df['date_from_sub_60'].values.astype(float), df['key'].values\n",
    "\n",
    "# Function to balance the data using SMOTE and random undersampling\n",
    "def balance_data(X, y_event):\n",
    "    enn = EditedNearestNeighbours()\n",
    "    tomek = TomekLinks()\n",
    "    smote_enn = SMOTEENN(enn=enn, sampling_strategy=0.5, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    smote_tomek = SMOTETomek(tomek=tomek, sampling_strategy=0.5, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    \n",
    "    smote = SMOTE(sampling_strategy=0.1, random_state=RANDOM_SEED)\n",
    "    X_smote, y_event_smote = smote.fit_resample(X, y_event)\n",
    "\n",
    "    rus = RandomUnderSampler(sampling_strategy=0.5, random_state=RANDOM_SEED)\n",
    "    X_balanced, y_event_balanced = rus.fit_resample(X_smote, y_event_smote)\n",
    "    \n",
    "    return X_balanced, y_event_balanced\n",
    "\n",
    "# Function to perform random search\n",
    "def random_search(df, feature_col, duration_col, event_col, event_focus, cluster_col, y_event, y_duration, param_distributions, n_iter, n_splits, weight_brier=0.5, weight_cindex=0.5):\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    group_kfold = GroupKFold(n_splits=n_splits)\n",
    "    param_sampler = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=RANDOM_SEED))\n",
    "\n",
    "    X = df[feature_col + duration_col].copy()\n",
    "    y_event = (df[event_col] == event_focus).astype(int).values\n",
    "    \n",
    "    for params in param_sampler:\n",
    "        combined_score = 0\n",
    "        for fold, (train_idx, val_idx) in enumerate(group_kfold.split(X, y_event_rrt, groups)):\n",
    "            print(f'Training fold {fold} of the test dataset:')\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train_event, y_val_event = y_event[train_idx], y_event[val_idx]\n",
    "            print()\n",
    "\n",
    "            X_train, y_train_event, y_train_duration = balance_data(X_train, y_train_event, y_train_duration)\n",
    "\n",
    "            net = tt.practical.MLPVanilla(X_train.shape[1], params['num_nodes'], 1, params['batch_norm'], params['dropout'])\n",
    "            model = CoxPH(net, tt.optim.Adam)\n",
    "            model.optimizer.set_lr(params['lr'])\n",
    "\n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "            y_train_tensor = (torch.tensor(y_train_duration, dtype=torch.float32), torch.tensor(y_train_event, dtype=torch.int64))\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "            y_val_tensor = (torch.tensor(y_val_duration, dtype=torch.float32), torch.tensor(y_val_event, dtype=torch.int64))\n",
    "            val = X_val_tensor, y_val_tensor\n",
    "\n",
    "            # Ensure all tensors have the same length\n",
    "            assert len(X_train_tensor) == len(y_train_tensor[0]) == len(y_train_tensor[1])\n",
    "            assert len(X_val_tensor) == len(y_val_tensor[0]) == len(y_val_tensor[1])\n",
    "\n",
    "            log = model.fit(X_train_tensor, y_train_tensor, params['batch_size'], params['epochs'], callbacks, verbose=False, val_data=val)\n",
    "            model.compute_baseline_hazards()\n",
    "\n",
    "            y_pred = model.predict_surv(X_val_tensor, max_duration=int(max(y_val_duration))).detach().numpy()\n",
    "            y_pred_df = pd.DataFrame(y_pred.T, index=np.arange(y_pred.shape[1]))\n",
    "\n",
    "            y_test_surv = Surv.from_arrays(event=y_val_event.astype(bool), time=y_val_duration)\n",
    "            integrated_brier = integrated_brier_score(y_test_surv, y_test_surv, y_pred_df.values, np.arange(y_pred_df.shape[0]))\n",
    "\n",
    "            risk_scores = -np.log(y_pred[:, :int(max(y_val_duration))])\n",
    "            c_index = concordance_index_censored(y_val_event.astype(bool), y_val_duration, risk_scores.mean(axis=1))[0]\n",
    "\n",
    "            # Combine the two metrics\n",
    "            combined_score += (weight_brier * (1 - integrated_brier)) + (weight_cindex * c_index)\n",
    "\n",
    "        combined_score /= n_splits\n",
    "\n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_score\n",
    "\n",
    "# Define features and target\n",
    "features = ['gender', 'dm', 'ht', 'sprint', 'a1c', 'po4', 'UACR_mg_g', 'Cr', 'age', 'alb', 'ca', 'hb', 'hco3']\n",
    "duration_col = ['date_from_sub_60']\n",
    "event_col = ['endpoint']\n",
    "cluster_col = ['key']\n",
    "target = ['date_from_sub_60', 'endpoint']\n",
    "\n",
    "# Convert categorical features to numerical\n",
    "X_train_transformed[features] = X_train_transformed[features].apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)\n",
    "\n",
    "# Extract input data\n",
    "X = X_train_transformed[features].values\n",
    "y_duration = X_train_transformed[duration_col].values\n",
    "groups = X_train_transformed[cluster_col].values\n",
    "\n",
    "# Convert endpoint to binary for RRT (1) and death (2)\n",
    "y_event_rrt = (X_train_transformed['endpoint'] == 1).astype(int).values\n",
    "y_event_death = (X_train_transformed['endpoint'] == 2).astype(int).values\n",
    "\n",
    "# Define parameter distributions for random search\n",
    "param_distributions = {\n",
    "    'num_nodes': [[64, 32], [128, 64], [128, 64, 32]],\n",
    "    'batch_norm': [True, False],\n",
    "    'dropout': uniform(0.1, 0.5),\n",
    "    'lr': uniform(1e-5, 1e-3),\n",
    "    'batch_size': randint(128, 512),\n",
    "    'epochs': randint(100, 300)\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "n_iter = 20\n",
    "n_splits = 5\n",
    "best_params, best_score = random_search(X, y_event_rrt, y_duration, param_distributions, n_iter, n_splits)\n",
    "\n",
    "# Train final model using best parameters\n",
    "X_train_balanced, y_train_event_balanced, y_train_duration_balanced = balance_data(X, y_event_rrt, y_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate class weights for RRT\n",
    "rrt_class_counts = np.bincount(y_event_rrt)\n",
    "rrt_class_weights = 1.0 / rrt_class_counts\n",
    "rrt_sample_weights = rrt_class_weights[y_event_rrt]\n",
    "death_class_counts = np.bincount(y_event_death)\n",
    "death_class_weights = 1.0 / death_class_counts\n",
    "death_sample_weights = death_class_weights[y_event_death]\n",
    "\n",
    "# Define the number of splits for cross-validation\n",
    "n_splits = 10\n",
    "group_kfold = GroupKFold(n_splits=n_splits)\n",
    "# Prepare the split indices\n",
    "splits_rrt = list(group_kfold.split(X, y_event_rrt, groups))\n",
    "splits_death = list(group_kfold.split(X, y_event_death, groups))\n",
    "\n",
    "# Prepare the split indices\n",
    "splits = list(group_kfold.split(X, y_event_rrt, groups))\n",
    "\n",
    "# Define the DeepSurv model\n",
    "in_features = X_train.shape[1]\n",
    "num_nodes = [128, 64]\n",
    "out_features = 1\n",
    "batch_norm = True\n",
    "dropout = 0.4\n",
    "output_bias = False\n",
    "\n",
    "net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n",
    "model_rrt = CoxPH(net, tt.optim.Adam)\n",
    "model_rrt.optimizer.set_lr(0.0001)\n",
    "\n",
    "# Set training parameters\n",
    "batch_size = 256\n",
    "epochs = 512\n",
    "callbacks = [tt.callbacks.EarlyStopping(patience=10)]\n",
    "\n",
    "# Train the model using cluster cross-validation for RRT\n",
    "for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "    print(f\"Training RRT model fold {fold+1}/{n_splits}\")\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train_duration, y_val_duration = y_duration[train_idx], y_duration[val_idx]\n",
    "    y_train_event_rrt, y_val_event_rrt = y_event_rrt[train_idx], y_event_rrt[val_idx]\n",
    "    train_sample_weights = torch.tensor(rrt_sample_weights[train_idx], dtype=torch.float32)\n",
    "    \n",
    "    # Ensure all inputs are numpy arrays\n",
    "    X_train, X_val = X_train.astype(float), X_val.astype(float)\n",
    "    y_train_duration, y_val_duration = y_train_duration.astype(float), y_val_duration.astype(float)\n",
    "    y_train_event_rrt, y_val_event_rrt = y_train_event_rrt.astype(int), y_val_event_rrt.astype(int)\n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = (torch.tensor(y_train_duration, dtype=torch.float32), torch.tensor(y_train_event_rrt, dtype=torch.int64))\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = (torch.tensor(y_val_duration, dtype=torch.float32), torch.tensor(y_val_event_rrt, dtype=torch.int64))\n",
    "    val = X_val_tensor, y_val_tensor\n",
    "\n",
    "    log = model_rrt.fit(X_train_tensor, y_train_tensor, batch_size, epochs, callbacks, verbose=True, val_data=val)\n",
    "    \n",
    "# Predict the risk for each time point\n",
    "gc.collect()\n",
    "\n",
    "X_test_transformed[features] = X_test_transformed[features].apply(lambda x: x.cat.codes if x.dtype.name == 'category' else x)\n",
    "X_test_tensor = torch.tensor(X_test_transformed[features].values, dtype=torch.float32)\n",
    "\n",
    "y_test_duration = X_test_transformed['date_from_sub_60'].values\n",
    "y_test_event_rrt = (X_test_transformed['endpoint'] == 1).astype(int).values\n",
    "y_test_event_death = (X_test_transformed['endpoint'] == 2).astype(int).values\n",
    "\n",
    "max_follow_up_time = int(min(1825, y_test_duration.max()))\n",
    "time_points = np.linspace(0, max_follow_up_time, int(max_follow_up_time), dtype='int')\n",
    "y_test_duration_truncated = np.minimum(y_test_duration, max_follow_up_time)\n",
    "y_test_event_truncated = np.where(y_test_duration > max_follow_up_time, 0, y_test_event_rrt)\n",
    "y_test_surv = Surv.from_arrays(event=y_test_event_truncated, time=y_test_duration_truncated)\n",
    "\n",
    "model_rrt.compute_baseline_hazards()\n",
    "y_pred = model_rrt.predict_surv(X_test_tensor, max_duration=max_follow_up_time).detach().numpy()\n",
    "# Convert y_pred to DataFrame and match time points\n",
    "y_pred_df = pd.DataFrame(y_pred.T, index=np.arange(y_pred.shape[1]))\n",
    "# Ensure y_pred is a DataFrame and time points match\n",
    "y_pred_df = y_pred_df.loc[time_points]\n",
    "\n",
    "integrated_brier = integrated_brier_score(y_test_surv, y_test_surv, y_pred[:, :-2], time_points[:-1])\n",
    "display(integrated_brier)\n",
    "\n",
    "risk_scores = -np.log(y_pred[:, :int(max_follow_up_time)])\n",
    "concordance_index_censored(y_test_event_truncated.astype(bool), y_test_duration_truncated, risk_scores.mean(axis=1))[0]\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 1D array, got 2D array instead:\narray=[[1.         1.         1.         ... 0.99823599 0.99823599 0.99823599]\n [1.         1.         1.         ... 0.99269117 0.99269117 0.99269117]\n [1.         1.         1.         ... 0.99330113 0.99330113 0.99330113]\n ...\n [1.         1.         1.         ... 0.99365333 0.99365333 0.99365333]\n [1.         1.         1.         ... 0.99668962 0.99668962 0.99668962]\n [1.         1.         1.         ... 0.99255937 0.99255937 0.99255937]].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m concord_index \u001b[38;5;241m=\u001b[39m \u001b[43mconcordance_index_ipcw\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_surv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_surv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:315\u001b[0m, in \u001b[0;36mconcordance_index_ipcw\u001b[0;34m(survival_train, survival_test, estimate, tau, tied_tol)\u001b[0m\n\u001b[1;32m    312\u001b[0m     mask \u001b[38;5;241m=\u001b[39m test_time \u001b[38;5;241m<\u001b[39m tau\n\u001b[1;32m    313\u001b[0m     survival_test \u001b[38;5;241m=\u001b[39m survival_test[mask]\n\u001b[0;32m--> 315\u001b[0m estimate \u001b[38;5;241m=\u001b[39m \u001b[43m_check_estimate_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m cens \u001b[38;5;241m=\u001b[39m CensoringDistributionEstimator()\n\u001b[1;32m    318\u001b[0m cens\u001b[38;5;241m.\u001b[39mfit(survival_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:38\u001b[0m, in \u001b[0;36m_check_estimate_1d\u001b[0;34m(estimate, test_time)\u001b[0m\n\u001b[1;32m     36\u001b[0m estimate \u001b[38;5;241m=\u001b[39m check_array(estimate, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 1D array, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimate\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m check_consistent_length(test_time, estimate)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimate\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 1D array, got 2D array instead:\narray=[[1.         1.         1.         ... 0.99823599 0.99823599 0.99823599]\n [1.         1.         1.         ... 0.99269117 0.99269117 0.99269117]\n [1.         1.         1.         ... 0.99330113 0.99330113 0.99330113]\n ...\n [1.         1.         1.         ... 0.99365333 0.99365333 0.99365333]\n [1.         1.         1.         ... 0.99668962 0.99668962 0.99668962]\n [1.         1.         1.         ... 0.99255937 0.99255937 0.99255937]].\n"
     ]
    }
   ],
   "source": [
    "concord_index = concordance_index_ipcw(y_test_surv, y_test_surv, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_duration = X_test_transformed['date_from_sub_60'].values\n",
    "\n",
    "max_follow_up_time = np.max(y_test_duration_truncated)\n",
    "# time_points = np.array(time_points)\n",
    "# y_pred = y_pred[:,:len(time_points)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47548,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_event_truncated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all times must be within follow-up time of test data: [0.0; 1701.0[",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m y_test_surv \u001b[38;5;241m=\u001b[39m Surv\u001b[38;5;241m.\u001b[39mfrom_arrays(event\u001b[38;5;241m=\u001b[39my_test_event_truncated, time\u001b[38;5;241m=\u001b[39my_test_duration_truncated)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mintegrated_brier_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_surv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_surv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_follow_up_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_duration_truncated\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:735\u001b[0m, in \u001b[0;36mintegrated_brier_score\u001b[0;34m(survival_train, survival_test, estimate, times)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m\"\"\"The Integrated Brier Score (IBS) provides an overall calculation of\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03mthe model performance at all available times :math:`t_1 \\\\leq t \\\\leq t_\\\\text{max}`.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;124;03m       Statistics in Medicine, vol. 18, no. 17-18, pp. 2529–2545, 1999.\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m# Computing the brier scores\u001b[39;00m\n\u001b[0;32m--> 735\u001b[0m times, brier_scores \u001b[38;5;241m=\u001b[39m \u001b[43mbrier_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurvival_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurvival_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m times\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least two time points must be given\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:609\u001b[0m, in \u001b[0;36mbrier_score\u001b[0;34m(survival_train, survival_test, estimate, times)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"Estimate the time-dependent Brier score for right censored data.\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03mThe time-dependent Brier score is the mean squared error at time point :math:`t`:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m       Statistics in Medicine, vol. 18, no. 17-18, pp. 2529–2545, 1999.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    608\u001b[0m test_event, test_time \u001b[38;5;241m=\u001b[39m check_y_survival(survival_test)\n\u001b[0;32m--> 609\u001b[0m estimate, times \u001b[38;5;241m=\u001b[39m \u001b[43m_check_estimate_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrier_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m times\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    611\u001b[0m     estimate \u001b[38;5;241m=\u001b[39m estimate\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:77\u001b[0m, in \u001b[0;36m_check_estimate_2d\u001b[0;34m(estimate, test_time, time_points, estimator)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_estimate_2d\u001b[39m(estimate, test_time, time_points, estimator):\n\u001b[1;32m     76\u001b[0m     estimate \u001b[38;5;241m=\u001b[39m check_array(estimate, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, allow_nd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimate\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m---> 77\u001b[0m     time_points \u001b[38;5;241m=\u001b[39m \u001b[43m_check_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     check_consistent_length(test_time, estimate)\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m time_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:68\u001b[0m, in \u001b[0;36m_check_times\u001b[0;34m(test_time, times)\u001b[0m\n\u001b[1;32m     65\u001b[0m times \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(times)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m times\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m test_time\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;129;01mor\u001b[39;00m times\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m test_time\u001b[38;5;241m.\u001b[39mmin():\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall times must be within follow-up time of test data: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_time\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_time\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m times\n",
      "\u001b[0;31mValueError\u001b[0m: all times must be within follow-up time of test data: [0.0; 1701.0["
     ]
    }
   ],
   "source": [
    "y_test_surv = Surv.from_arrays(event=y_test_event_truncated, time=y_test_duration_truncated)\n",
    "integrated_brier_score(y_test_surv, y_test_surv, y_pred[:,:int(max_follow_up_time)], y_test_duration_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all times must be within follow-up time of test data: [0.0; 1701.0[",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m max_predict_yr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      3\u001b[0m time_points \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mround\u001b[39m(days_in_yr \u001b[38;5;241m*\u001b[39m i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_predict_yr)]\n\u001b[0;32m----> 5\u001b[0m \u001b[43mintegrated_brier_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_surv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_surv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtime_points\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_duration_truncated\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:735\u001b[0m, in \u001b[0;36mintegrated_brier_score\u001b[0;34m(survival_train, survival_test, estimate, times)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m\"\"\"The Integrated Brier Score (IBS) provides an overall calculation of\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03mthe model performance at all available times :math:`t_1 \\\\leq t \\\\leq t_\\\\text{max}`.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;124;03m       Statistics in Medicine, vol. 18, no. 17-18, pp. 2529–2545, 1999.\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m# Computing the brier scores\u001b[39;00m\n\u001b[0;32m--> 735\u001b[0m times, brier_scores \u001b[38;5;241m=\u001b[39m \u001b[43mbrier_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurvival_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurvival_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m times\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least two time points must be given\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:609\u001b[0m, in \u001b[0;36mbrier_score\u001b[0;34m(survival_train, survival_test, estimate, times)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"Estimate the time-dependent Brier score for right censored data.\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03mThe time-dependent Brier score is the mean squared error at time point :math:`t`:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m       Statistics in Medicine, vol. 18, no. 17-18, pp. 2529–2545, 1999.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    608\u001b[0m test_event, test_time \u001b[38;5;241m=\u001b[39m check_y_survival(survival_test)\n\u001b[0;32m--> 609\u001b[0m estimate, times \u001b[38;5;241m=\u001b[39m \u001b[43m_check_estimate_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrier_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m times\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    611\u001b[0m     estimate \u001b[38;5;241m=\u001b[39m estimate\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:77\u001b[0m, in \u001b[0;36m_check_estimate_2d\u001b[0;34m(estimate, test_time, time_points, estimator)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_estimate_2d\u001b[39m(estimate, test_time, time_points, estimator):\n\u001b[1;32m     76\u001b[0m     estimate \u001b[38;5;241m=\u001b[39m check_array(estimate, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, allow_nd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimate\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m---> 77\u001b[0m     time_points \u001b[38;5;241m=\u001b[39m \u001b[43m_check_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     check_consistent_length(test_time, estimate)\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m time_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/sksurv/metrics.py:68\u001b[0m, in \u001b[0;36m_check_times\u001b[0;34m(test_time, times)\u001b[0m\n\u001b[1;32m     65\u001b[0m times \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(times)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m times\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m test_time\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;129;01mor\u001b[39;00m times\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m test_time\u001b[38;5;241m.\u001b[39mmin():\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall times must be within follow-up time of test data: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_time\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_time\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m times\n",
      "\u001b[0;31mValueError\u001b[0m: all times must be within follow-up time of test data: [0.0; 1701.0["
     ]
    }
   ],
   "source": [
    "days_in_yr = 365.25\n",
    "max_predict_yr = 6\n",
    "time_points = [round(days_in_yr * i) for i in range(max_predict_yr)]\n",
    "\n",
    "integrated_brier_score(y_test_surv, y_test_surv, y_pred.T[time_points], y_test_duration_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>duration</th>\n",
       "      <th>0.0</th>\n",
       "      <th>365.0</th>\n",
       "      <th>730.0</th>\n",
       "      <th>1096.0</th>\n",
       "      <th>1461.0</th>\n",
       "      <th>1826.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>0.997425</td>\n",
       "      <td>0.995957</td>\n",
       "      <td>0.994475</td>\n",
       "      <td>0.992077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999130</td>\n",
       "      <td>0.998295</td>\n",
       "      <td>0.997322</td>\n",
       "      <td>0.996340</td>\n",
       "      <td>0.994748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998947</td>\n",
       "      <td>0.997938</td>\n",
       "      <td>0.996762</td>\n",
       "      <td>0.995575</td>\n",
       "      <td>0.993652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999042</td>\n",
       "      <td>0.998124</td>\n",
       "      <td>0.997053</td>\n",
       "      <td>0.995972</td>\n",
       "      <td>0.994222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999122</td>\n",
       "      <td>0.998281</td>\n",
       "      <td>0.997299</td>\n",
       "      <td>0.996309</td>\n",
       "      <td>0.994704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599807</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.998682</td>\n",
       "      <td>0.997930</td>\n",
       "      <td>0.997170</td>\n",
       "      <td>0.995939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599808</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998571</td>\n",
       "      <td>0.997201</td>\n",
       "      <td>0.995606</td>\n",
       "      <td>0.993996</td>\n",
       "      <td>0.991390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599809</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998499</td>\n",
       "      <td>0.997061</td>\n",
       "      <td>0.995386</td>\n",
       "      <td>0.993695</td>\n",
       "      <td>0.990959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599810</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.998240</td>\n",
       "      <td>0.997236</td>\n",
       "      <td>0.996222</td>\n",
       "      <td>0.994580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599811</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999122</td>\n",
       "      <td>0.998280</td>\n",
       "      <td>0.997298</td>\n",
       "      <td>0.996307</td>\n",
       "      <td>0.994701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599812 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "duration  0.0       365.0     730.0     1096.0    1461.0    1826.0\n",
       "0            1.0  0.998685  0.997425  0.995957  0.994475  0.992077\n",
       "1            1.0  0.999130  0.998295  0.997322  0.996340  0.994748\n",
       "2            1.0  0.998947  0.997938  0.996762  0.995575  0.993652\n",
       "3            1.0  0.999042  0.998124  0.997053  0.995972  0.994222\n",
       "4            1.0  0.999122  0.998281  0.997299  0.996309  0.994704\n",
       "...          ...       ...       ...       ...       ...       ...\n",
       "599807       1.0  0.999327  0.998682  0.997930  0.997170  0.995939\n",
       "599808       1.0  0.998571  0.997201  0.995606  0.993996  0.991390\n",
       "599809       1.0  0.998499  0.997061  0.995386  0.993695  0.990959\n",
       "599810       1.0  0.999102  0.998240  0.997236  0.996222  0.994580\n",
       "599811       1.0  0.999122  0.998280  0.997298  0.996307  0.994701\n",
       "\n",
       "[599812 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_in_yr = 365.25\n",
    "max_predict_yr = 6\n",
    "time_points = [round(days_in_yr * i) for i in range(max_predict_yr)]\n",
    "y_pred.T[time_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'is_monotonic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ev \u001b[38;5;241m=\u001b[39m \u001b[43mEvalSurv\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_duration_truncated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_event_truncated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcensor_surv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:33\u001b[0m, in \u001b[0;36mEvalSurv.__init__\u001b[0;34m(self, surv, durations, events, censor_surv, censor_durations, steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdurations \u001b[38;5;241m=\u001b[39m durations\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents \u001b[38;5;241m=\u001b[39m events\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcensor_surv\u001b[49m \u001b[38;5;241m=\u001b[39m censor_surv\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcensor_durations \u001b[38;5;241m=\u001b[39m censor_durations\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m steps\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:51\u001b[0m, in \u001b[0;36mEvalSurv.censor_surv\u001b[0;34m(self, censor_surv)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(censor_surv) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m censor_surv \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_km_censor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcensor_surv cannot be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcensor_surv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Use e.g. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkm\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:107\u001b[0m, in \u001b[0;36mEvalSurv.add_km_censor\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    104\u001b[0m km \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mkaplan_meier(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdurations, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents)\n\u001b[1;32m    105\u001b[0m surv \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mrepeat(km\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdurations), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    106\u001b[0m                     index\u001b[38;5;241m=\u001b[39mkm\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_censor_est\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:95\u001b[0m, in \u001b[0;36mEvalSurv.add_censor_est\u001b[0;34m(self, censor_surv, steps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"Add censoring estimates so one can use inverse censoring weighting.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m`censor_surv` are the survival estimates trained on (durations, 1-events),\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    or lower index 'post'. If `None` use `self.steps` (default: {None})\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(censor_surv, EvalSurv):\n\u001b[0;32m---> 95\u001b[0m     censor_surv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcensor_surv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdurations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcensor_surv \u001b[38;5;241m=\u001b[39m censor_surv\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pycox/evaluation/eval_surv.py:36\u001b[0m, in \u001b[0;36mEvalSurv.__init__\u001b[0;34m(self, surv, durations, events, censor_surv, censor_durations, steps)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcensor_durations \u001b[38;5;241m=\u001b[39m censor_durations\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m steps\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_surv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_monotonic\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/pandas/core/generic.py:6296\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   6290\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   6291\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   6292\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   6293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   6294\u001b[0m ):\n\u001b[1;32m   6295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 6296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'is_monotonic'"
     ]
    }
   ],
   "source": [
    "ev = EvalSurv(y_pred.T, y_test_duration_truncated, y_test_event_truncated, censor_surv='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>599802</th>\n",
       "      <th>599803</th>\n",
       "      <th>599804</th>\n",
       "      <th>599805</th>\n",
       "      <th>599806</th>\n",
       "      <th>599807</th>\n",
       "      <th>599808</th>\n",
       "      <th>599809</th>\n",
       "      <th>599810</th>\n",
       "      <th>599811</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822.0</th>\n",
       "      <td>0.992102</td>\n",
       "      <td>0.994765</td>\n",
       "      <td>0.993672</td>\n",
       "      <td>0.994240</td>\n",
       "      <td>0.994721</td>\n",
       "      <td>0.992306</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.989502</td>\n",
       "      <td>0.992861</td>\n",
       "      <td>0.992284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992283</td>\n",
       "      <td>0.988717</td>\n",
       "      <td>0.991707</td>\n",
       "      <td>0.993422</td>\n",
       "      <td>0.992658</td>\n",
       "      <td>0.995951</td>\n",
       "      <td>0.991417</td>\n",
       "      <td>0.990988</td>\n",
       "      <td>0.994597</td>\n",
       "      <td>0.994718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823.0</th>\n",
       "      <td>0.992102</td>\n",
       "      <td>0.994765</td>\n",
       "      <td>0.993672</td>\n",
       "      <td>0.994240</td>\n",
       "      <td>0.994721</td>\n",
       "      <td>0.992306</td>\n",
       "      <td>0.995439</td>\n",
       "      <td>0.989502</td>\n",
       "      <td>0.992861</td>\n",
       "      <td>0.992284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992283</td>\n",
       "      <td>0.988717</td>\n",
       "      <td>0.991707</td>\n",
       "      <td>0.993422</td>\n",
       "      <td>0.992658</td>\n",
       "      <td>0.995951</td>\n",
       "      <td>0.991417</td>\n",
       "      <td>0.990988</td>\n",
       "      <td>0.994597</td>\n",
       "      <td>0.994718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824.0</th>\n",
       "      <td>0.992094</td>\n",
       "      <td>0.994759</td>\n",
       "      <td>0.993665</td>\n",
       "      <td>0.994234</td>\n",
       "      <td>0.994715</td>\n",
       "      <td>0.992298</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.989491</td>\n",
       "      <td>0.992853</td>\n",
       "      <td>0.992276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992275</td>\n",
       "      <td>0.988706</td>\n",
       "      <td>0.991699</td>\n",
       "      <td>0.993415</td>\n",
       "      <td>0.992650</td>\n",
       "      <td>0.995947</td>\n",
       "      <td>0.991408</td>\n",
       "      <td>0.990978</td>\n",
       "      <td>0.994591</td>\n",
       "      <td>0.994712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825.0</th>\n",
       "      <td>0.992085</td>\n",
       "      <td>0.994754</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>0.994228</td>\n",
       "      <td>0.994710</td>\n",
       "      <td>0.992290</td>\n",
       "      <td>0.995430</td>\n",
       "      <td>0.989480</td>\n",
       "      <td>0.992846</td>\n",
       "      <td>0.992268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992267</td>\n",
       "      <td>0.988694</td>\n",
       "      <td>0.991690</td>\n",
       "      <td>0.993408</td>\n",
       "      <td>0.992642</td>\n",
       "      <td>0.995943</td>\n",
       "      <td>0.991399</td>\n",
       "      <td>0.990969</td>\n",
       "      <td>0.994585</td>\n",
       "      <td>0.994707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826.0</th>\n",
       "      <td>0.992077</td>\n",
       "      <td>0.994748</td>\n",
       "      <td>0.993652</td>\n",
       "      <td>0.994222</td>\n",
       "      <td>0.994704</td>\n",
       "      <td>0.992282</td>\n",
       "      <td>0.995425</td>\n",
       "      <td>0.989469</td>\n",
       "      <td>0.992838</td>\n",
       "      <td>0.992260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>0.988682</td>\n",
       "      <td>0.991681</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.992634</td>\n",
       "      <td>0.995939</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>0.990959</td>\n",
       "      <td>0.994580</td>\n",
       "      <td>0.994701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1827 rows × 599812 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5       \\\n",
       "duration                                                               \n",
       "0.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "1.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "2.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "3.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "4.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "1822.0    0.992102  0.994765  0.993672  0.994240  0.994721  0.992306   \n",
       "1823.0    0.992102  0.994765  0.993672  0.994240  0.994721  0.992306   \n",
       "1824.0    0.992094  0.994759  0.993665  0.994234  0.994715  0.992298   \n",
       "1825.0    0.992085  0.994754  0.993659  0.994228  0.994710  0.992290   \n",
       "1826.0    0.992077  0.994748  0.993652  0.994222  0.994704  0.992282   \n",
       "\n",
       "            6         7         8         9       ...    599802    599803  \\\n",
       "duration                                          ...                       \n",
       "0.0       1.000000  1.000000  1.000000  1.000000  ...  1.000000  1.000000   \n",
       "1.0       1.000000  1.000000  1.000000  1.000000  ...  1.000000  1.000000   \n",
       "2.0       1.000000  1.000000  1.000000  1.000000  ...  1.000000  1.000000   \n",
       "3.0       1.000000  1.000000  1.000000  1.000000  ...  1.000000  1.000000   \n",
       "4.0       1.000000  1.000000  1.000000  1.000000  ...  1.000000  1.000000   \n",
       "...            ...       ...       ...       ...  ...       ...       ...   \n",
       "1822.0    0.995439  0.989502  0.992861  0.992284  ...  0.992283  0.988717   \n",
       "1823.0    0.995439  0.989502  0.992861  0.992284  ...  0.992283  0.988717   \n",
       "1824.0    0.995434  0.989491  0.992853  0.992276  ...  0.992275  0.988706   \n",
       "1825.0    0.995430  0.989480  0.992846  0.992268  ...  0.992267  0.988694   \n",
       "1826.0    0.995425  0.989469  0.992838  0.992260  ...  0.992259  0.988682   \n",
       "\n",
       "            599804    599805    599806    599807    599808    599809  \\\n",
       "duration                                                               \n",
       "0.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "1.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "2.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "3.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "4.0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "1822.0    0.991707  0.993422  0.992658  0.995951  0.991417  0.990988   \n",
       "1823.0    0.991707  0.993422  0.992658  0.995951  0.991417  0.990988   \n",
       "1824.0    0.991699  0.993415  0.992650  0.995947  0.991408  0.990978   \n",
       "1825.0    0.991690  0.993408  0.992642  0.995943  0.991399  0.990969   \n",
       "1826.0    0.991681  0.993401  0.992634  0.995939  0.991390  0.990959   \n",
       "\n",
       "            599810    599811  \n",
       "duration                      \n",
       "0.0       1.000000  1.000000  \n",
       "1.0       1.000000  1.000000  \n",
       "2.0       1.000000  1.000000  \n",
       "3.0       1.000000  1.000000  \n",
       "4.0       1.000000  1.000000  \n",
       "...            ...       ...  \n",
       "1822.0    0.994597  0.994718  \n",
       "1823.0    0.994597  0.994718  \n",
       "1824.0    0.994591  0.994712  \n",
       "1825.0    0.994585  0.994707  \n",
       "1826.0    0.994580  0.994701  \n",
       "\n",
       "[1827 rows x 599812 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(539830, 13)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(539830,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(539830,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.shape)\n",
    "display(y_train_duration.shape)\n",
    "display(y_train_event_rrt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_duration.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use one imputed dataset to train first\n",
    "#### Due to the need of handling competing risk (rrt and death), we use Fine-Gray subdistribution hazard model to remake the event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Must call `fit` first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m penalizer_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# From 0.1 to 100 in logarithmic scale\u001b[39;00m\n\u001b[1;32m     89\u001b[0m l1_ratios \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m summaries, best_penalizers \u001b[38;5;241m=\u001b[39m \u001b[43mfit_models_to_imputed_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalizer_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_ratios\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[113], line 66\u001b[0m, in \u001b[0;36mfit_models_to_imputed_datasets\u001b[0;34m(imputed_datasets, penalizer_values, l1_ratios, k_folds)\u001b[0m\n\u001b[1;32m     63\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset[categorical_columns \u001b[38;5;241m+\u001b[39m continuous_columns \u001b[38;5;241m+\u001b[39m [duration_col, event_col]]\n\u001b[1;32m     65\u001b[0m model \u001b[38;5;241m=\u001b[39m CoxPHFitter()\n\u001b[0;32m---> 66\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_k_fold_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalizer_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_ratios\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     69\u001b[0m     best_model, best_score, best_penalizer, best_l1_ratio \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[113], line 20\u001b[0m, in \u001b[0;36mparallel_k_fold_cv\u001b[0;34m(model, dataset, duration_col, event_col, k_folds, penalizer_values, l1_ratios)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 futures\u001b[38;5;241m.\u001b[39mappend(executor\u001b[38;5;241m.\u001b[39msubmit(fit_and_evaluate, model, train_data, test_data, duration_col, event_col, penalizer, l1_ratio))\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[0;32m---> 20\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[113], line 44\u001b[0m, in \u001b[0;36mfit_and_evaluate\u001b[0;34m(model, train_data, test_data, duration_col, event_col, penalizer, l1_ratio)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_and_evaluate\u001b[39m(model, train_data, test_data, duration_col, event_col, penalizer, l1_ratio):\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m(penalizer\u001b[38;5;241m=\u001b[39mpenalizer, l1_ratio\u001b[38;5;241m=\u001b[39ml1_ratio)\n\u001b[1;32m     45\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(train_data, duration_col\u001b[38;5;241m=\u001b[39mduration_col, event_col\u001b[38;5;241m=\u001b[39mevent_col)\n\u001b[1;32m     46\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_partial_hazard(test_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/lifelines/fitters/coxph_fitter.py:594\u001b[0m, in \u001b[0;36mCoxPHFitter.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust call `fit` first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m, attr):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, attr)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_name, attr))\n",
      "File \u001b[0;32m~/miniconda3/envs/g3_regress/lib/python3.10/site-packages/lifelines/fitters/coxph_fitter.py:592\u001b[0m, in \u001b[0;36mCoxPHFitter.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 592\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust call `fit` first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, attr):\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, attr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Must call `fit` first."
     ]
    }
   ],
   "source": [
    "def write_csv(path, data):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    data.to_csv(path, mode='a', header=not os.path.isfile(path))\n",
    "\n",
    "\n",
    "def parallel_predict(model, data, batch_size):\n",
    "    \"\"\"Distribute data batches across multiple threads for prediction.\"\"\"\n",
    "    def batch_predict(data_batch):\n",
    "        return model.predict_partial_hazard(data_batch)\n",
    "\n",
    "    num_batches = len(data) // batch_size + (1 if len(data) % batch_size else 0)\n",
    "    futures = []\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for i in range(num_batches):\n",
    "            batch_data = data.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "            futures.append(executor.submit(batch_predict, batch_data))\n",
    "        \n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "    \n",
    "    return pd.concat(results)\n",
    "\n",
    "def fit_models_to_imputed_datasets(imputed_datasets, penalizer_values, l1_ratios, k_folds=100):\n",
    "    # Initialize file paths\n",
    "    summary_file = '/mnt/d/pydatascience/g3_regress/doc/cox_model_summaries.csv'\n",
    "    config_file = '/mnt/d/pydatascience/g3_regress/doc/cox_best_configs.csv'\n",
    "    \n",
    "    # Ensure directories exist\n",
    "    # os.makedirs(os.path.dirname(summary_file), exist_ok=True)\n",
    "    # os.makedirs(os.path.dirname(config_file), exist_ok=True)\n",
    "    \n",
    "    # Create or clear the files and write headers\n",
    "    with open(summary_file, 'w') as f, open(config_file, 'w') as g:\n",
    "        pass  # Just to create/clear files\n",
    "    \n",
    "    for estimator_name, datasets in imputed_datasets.items():\n",
    "        for dataset_index, dataset in enumerate(datasets):\n",
    "            # Handling categorical and continuous columns as before\n",
    "            categorical_columns = ['gender', 'dm', 'ht']\n",
    "            continuous_columns = ['age', 'Cr', 'date_from_sub_60', 'UACR_mg_g_log', 'hb', 'a1c', 'ca', 'hco3']\n",
    "            duration_col = 'date_till_endpoint'\n",
    "            event_col = 'endpoint'\n",
    "            # cluster_col = 'key'\n",
    "            \n",
    "            for col in categorical_columns:\n",
    "                dataset[col] = dataset[col].astype('category')\n",
    "            dataset = dataset[categorical_columns + continuous_columns + [duration_col, event_col]]\n",
    "            \n",
    "            # Grid search over penalizer values and L1/L2 ratios\n",
    "            best_score = 0\n",
    "            best_config = (None, None)\n",
    "            for penalizer in penalizer_values:\n",
    "                for l1_ratio in l1_ratios:\n",
    "                    cph = CoxPHFitter(penalizer=penalizer, l1_ratio=l1_ratio)\n",
    "                    scores = k_fold_cross_validation(cph, dataset, duration_col, event_col, k=k_folds, scoring_method=\"concordance_index\")\n",
    "                    average_score = np.mean(scores)\n",
    "                    if average_score > best_score:\n",
    "                        best_score = average_score\n",
    "                        best_config = (penalizer, l1_ratio)\n",
    "            \n",
    "            # Fit the model with the best configuration\n",
    "            best_model = CoxPHFitter(penalizer=best_config[0], l1_ratio=best_config[1])\n",
    "            best_model.fit(dataset, duration_col=duration_col, event_col=event_col)\n",
    "            \n",
    "            # Convert summary to DataFrame and write to CSV\n",
    "            summary_df = best_model.summary\n",
    "            summary_df['imputer'] = estimator_name\n",
    "            summary_df['dataset_index'] = dataset_index\n",
    "            write_csv(summary_file, summary_df)\n",
    "            # summary_df.to_csv(summary_file, mode='a', header=not os.path.isfile(summary_file))\n",
    "            \n",
    "            # Write the best configuration to another CSV\n",
    "            config_df = pd.DataFrame({\n",
    "                'imputer': [estimator_name],\n",
    "                'dataset_index': [dataset_index],\n",
    "                'best_penalizer': [best_config[0]],\n",
    "                'best_l1_ratio': [best_config[1]],\n",
    "                'best_score': [best_score]\n",
    "            })\n",
    "            write_csv(config_file, config_df)\n",
    "            \n",
    "            print(f\"Training of Cos model with {estimator_name} number {dataset_index} completed\")\n",
    "            # config_df.to_csv(config_file, mode='a', header=not os.path.isfile(config_file))\n",
    "    \n",
    "    return summary_file, config_file\n",
    "\n",
    "\n",
    "penalizer_values = np.logspace(-1, 2, 3)  # From 0.1 to 100 in logarithmic scale\n",
    "l1_ratios = np.linspace(0, 1, 5)\n",
    "summaries, best_penalizers = fit_models_to_imputed_datasets(X_train_all, penalizer_values, l1_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Grid search over penalizer values and L1/L2 ratios\n",
    "            # best_score = 0\n",
    "            # best_config = (None, None)\n",
    "            # for penalizer in penalizer_values:\n",
    "            #     for l1_ratio in l1_ratios:\n",
    "            #         cph = CoxPHFitter(penalizer=penalizer, l1_ratio=l1_ratio)\n",
    "            #         scores = k_fold_cross_validation(cph, dataset, duration_col, event_col, k=k_folds, scoring_method=\"concordance_index\")\n",
    "            #         average_score = np.mean(scores)\n",
    "            #         if average_score > best_score:\n",
    "            #             best_score = average_score\n",
    "            #             best_config = (penalizer, l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # # Fit the model with the best configuration\n",
    "            # best_model = CoxPHFitter(penalizer=best_config[0], l1_ratio=best_config[1])\n",
    "            # best_model.fit(dataset, duration_col=duration_col, event_col=event_col)\n",
    "            \n",
    "            # # Convert summary to DataFrame and write to CSV\n",
    "            # summary_df = best_model.summary\n",
    "            # summary_df['imputer'] = estimator_name\n",
    "            # summary_df['dataset_index'] = dataset_index\n",
    "            # write_csv(summary_file, summary_df)\n",
    "            # # summary_df.to_csv(summary_file, mode='a', header=not os.path.isfile(summary_file))\n",
    "            \n",
    "            # # Write the best configuration to another CSV\n",
    "            # config_df = pd.DataFrame({\n",
    "            #     'imputer': [estimator_name],\n",
    "            #     'dataset_index': [dataset_index],\n",
    "            #     'best_penalizer': [best_config[0]],\n",
    "            #     'best_l1_ratio': [best_config[1]],\n",
    "            #     'best_score': [best_score]\n",
    "            # })\n",
    "            # write_csv(config_file, config_df)\n",
    "            \n",
    "            # print(f\"Training of Cos model with {estimator_name} number {dataset_index} completed\")\n",
    "            # config_df.to_csv(config_file, mode='a', header=not os.path.isfile(config_file))\n",
    "    \n",
    "    # return summary_file, config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_predict(model, data, batch_size):\n",
    "    \"\"\"Distribute data batches across multiple threads for prediction.\"\"\"\n",
    "    def batch_predict(data_batch):\n",
    "        return model.predict_partial_hazard(data_batch)\n",
    "\n",
    "    num_batches = len(data) // batch_size + (1 if len(data) % batch_size else 0)\n",
    "    futures = []\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for i in range(num_batches):\n",
    "            batch_data = data.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "            futures.append(executor.submit(batch_predict, batch_data))\n",
    "        \n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "    \n",
    "    return pd.concat(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g3_regress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
